{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d83f5a7",
   "metadata": {},
   "source": [
    "# Combine Results and Score Using Idiolect\n",
    "\n",
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "5d0a42e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "source(\"./utils.R\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "95cfcc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "suppressPackageStartupMessages(\n",
    "  {\n",
    "    library(dplyr)\n",
    "    library(idiolect)\n",
    "    library(readr)\n",
    "    library(readxl)\n",
    "    library(writexl)\n",
    "    library(purrr)\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefcf806",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2b4a93fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_location = '/Volumes/BCross/paraphrase examples slurm'\n",
    "\n",
    "# Token Size Problems\n",
    "# This table contains the problems for each different min_token_size value in the dataset\n",
    "token_size_problems = read_excel(paste0(base_location, '/token_size_problems.xlsx'))\n",
    "\n",
    "# Raw Score Data\n",
    "# This data contains the llr scores aggregated across problems with averaging across phrase occurences done first\n",
    "raw_score_data = read_excel(paste0(base_location, '/score_by_token_size_avg.xlsx'))\n",
    "\n",
    "# LambdaG Results\n",
    "# Load the LambdaG results for the Wiki test dataset\n",
    "lambdag_raw <- read.csv(paste0(base_location, '/lambdaG_results.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53c5a61",
   "metadata": {},
   "source": [
    "### Create Final Dataset\n",
    "\n",
    "Here we join the raw data with teh problem dataset to filter out incorrect token sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "c0e78ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[38;5;246m# A tibble: 6 × 10\u001b[39m\n",
       "  model problem         corpus known_author unknown_author target llr_no_context\n",
       "  \u001b[3m\u001b[38;5;246m<chr>\u001b[39m\u001b[23m \u001b[3m\u001b[38;5;246m<chr>\u001b[39m\u001b[23m           \u001b[3m\u001b[38;5;246m<chr>\u001b[39m\u001b[23m  \u001b[3m\u001b[38;5;246m<chr>\u001b[39m\u001b[23m        \u001b[3m\u001b[38;5;246m<chr>\u001b[39m\u001b[23m          \u001b[3m\u001b[38;5;246m<lgl>\u001b[39m\u001b[23m           \u001b[3m\u001b[38;5;246m<dbl>\u001b[39m\u001b[23m\n",
       "\u001b[38;5;250m1\u001b[39m gemma HOOTmag vs HOO… Wiki   HOOTmag      HOOTmag        TRUE            11.5 \n",
       "\u001b[38;5;250m2\u001b[39m gemma HOOTmag vs Iai… Wiki   HOOTmag      Iain99         FALSE           20.3 \n",
       "\u001b[38;5;250m3\u001b[39m gemma Hodja_Nasreddi… Wiki   Hodja_Nasre… Hodja_Nasredd… TRUE            37.2 \n",
       "\u001b[38;5;250m4\u001b[39m gemma Hodja_Nasreddi… Wiki   Hodja_Nasre… HonestopL      FALSE           20.9 \n",
       "\u001b[38;5;250m5\u001b[39m gemma HonestopL vs H… Wiki   HonestopL    HOOTmag        FALSE           14.2 \n",
       "\u001b[38;5;250m6\u001b[39m gemma HonestopL vs H… Wiki   HonestopL    HonestopL      TRUE             8.05\n",
       "\u001b[38;5;246m# ℹ 3 more variables: llr_known <dbl>, score <dbl>, min_token_size <dbl>\u001b[39m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want to rename the unkown score column to just score to allow it to work with performance\n",
    "score_data = raw_score_data %>%\n",
    "  inner_join(token_size_problems, by = c('problem', 'min_token_size', 'corpus', 'target')) %>%\n",
    "  rename('score'='llr_unknown')\n",
    "\n",
    "score_data %>% head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac1b465",
   "metadata": {},
   "source": [
    "### Calculate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "380c609a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[38;5;246m# A tibble: 6 × 3\u001b[39m\n",
       "  model corpus min_token_size\n",
       "  \u001b[3m\u001b[38;5;246m<chr>\u001b[39m\u001b[23m \u001b[3m\u001b[38;5;246m<chr>\u001b[39m\u001b[23m           \u001b[3m\u001b[38;5;246m<dbl>\u001b[39m\u001b[23m\n",
       "\u001b[38;5;250m1\u001b[39m gemma Wiki                2\n",
       "\u001b[38;5;250m2\u001b[39m gemma Wiki                3\n",
       "\u001b[38;5;250m3\u001b[39m gemma Wiki                4\n",
       "\u001b[38;5;250m4\u001b[39m gemma Wiki                5\n",
       "\u001b[38;5;250m5\u001b[39m gpt2  Wiki                2\n",
       "\u001b[38;5;250m6\u001b[39m gpt2  Wiki                3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_model_sizes <- score_data %>%\n",
    "  select(model, corpus, min_token_size) %>%\n",
    "  distinct() %>%\n",
    "  arrange(model, corpus, min_token_size)\n",
    "\n",
    "distinct_model_sizes %>% head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "ccc118ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_group <- function(model, corpus, min_token_size) {\n",
    "\n",
    "  distinct_problems <- token_size_problems %>%\n",
    "    filter(min_token_size == !!min_token_size)\n",
    "\n",
    "  # Filter score_data by the combination\n",
    "  filtered <- score_data %>%\n",
    "    filter(model == !!model,\n",
    "           corpus == !!corpus,\n",
    "           min_token_size == !!min_token_size) %>%\n",
    "    inner_join(distinct_problems, by=c('corpus', 'problem', 'min_token_size', 'target'))\n",
    "  \n",
    "  # Run your performance function (assume it returns a 1-row data frame)\n",
    "  perf <- performance(filtered)\n",
    "  perf <- perf$evaluation\n",
    "\n",
    "  # Add the identifying columns\n",
    "  cbind(\n",
    "    data.frame(model = model,\n",
    "               corpus = corpus,\n",
    "               min_token_size = min_token_size),\n",
    "    perf\n",
    "  )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "52405eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |======================================================================| 100%\n",
      "Setting levels: control = FALSE, case = TRUE\n",
      "Setting direction: controls < cases\n",
      "  |======================================================================| 100%\n",
      "Setting levels: control = FALSE, case = TRUE\n",
      "Setting direction: controls < cases\n",
      "  |======================================================================| 100%\n",
      "Setting levels: control = FALSE, case = TRUE\n",
      "Setting direction: controls < cases\n",
      "  |======================================================================| 100%\n",
      "Setting levels: control = FALSE, case = TRUE\n",
      "Setting direction: controls < cases\n",
      "  |======================================================================| 100%\n",
      "Setting levels: control = FALSE, case = TRUE\n",
      "Setting direction: controls < cases\n",
      "  |======================================================================| 100%\n",
      "Setting levels: control = FALSE, case = TRUE\n",
      "Setting direction: controls < cases\n",
      "  |======================================================================| 100%\n",
      "Setting levels: control = FALSE, case = TRUE\n",
      "Setting direction: controls < cases\n",
      "  |======================================================================| 100%\n",
      "Setting levels: control = FALSE, case = TRUE\n",
      "Setting direction: controls < cases\n",
      "  |======================================================================| 100%\n",
      "Setting levels: control = FALSE, case = TRUE\n",
      "Setting direction: controls < cases\n",
      "  |======================================================================| 100%\n",
      "Setting levels: control = FALSE, case = TRUE\n",
      "Setting direction: controls < cases\n",
      "  |======================================================================| 100%\n",
      "Setting levels: control = FALSE, case = TRUE\n",
      "Setting direction: controls < cases\n",
      "  |======================================================================| 100%\n",
      "Setting levels: control = FALSE, case = TRUE\n",
      "Setting direction: controls < cases\n",
      "  |======================================================================| 100%\n",
      "Setting levels: control = FALSE, case = TRUE\n",
      "Setting direction: controls < cases\n",
      "  |======================================================================| 100%\n",
      "Setting levels: control = FALSE, case = TRUE\n",
      "Setting direction: controls < cases\n",
      "  |======================================================================| 100%\n",
      "Setting levels: control = FALSE, case = TRUE\n",
      "Setting direction: controls < cases\n",
      "  |======================================================================| 100%\n",
      "Setting levels: control = FALSE, case = TRUE\n",
      "Setting direction: controls < cases\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  model corpus min_token_size      Cllr  Cllr_min      EER Mean TRUE LLR\n",
       "1 gemma   Wiki              2 0.8619175 0.7444600 30.11696     0.3143858\n",
       "2  gpt2   Wiki              2 0.8618176 0.7502645 29.64912     0.3041526\n",
       "3 llama   Wiki              2 0.8770738 0.7874005 31.14035     0.2745068\n",
       "4  qwen   Wiki              2 0.8776719 0.7682051 30.73684     0.2710594\n",
       "5 gemma   Wiki              3 0.7607292 0.6629074 23.76564     1.0192055\n",
       "6  gpt2   Wiki              3 0.7704295 0.6742033 25.77778     0.8408998\n",
       "  Mean FALSE LLR TRUE trials FALSE trials       AUC Balanced Accuracy Precision\n",
       "1     -0.2345160         114          114 0.7686543         0.6919643 0.7009346\n",
       "2     -0.2375316         114          114 0.7683355         0.6919643 0.7009346\n",
       "3     -0.2087909         114          114 0.7539063         0.6785714 0.6886792\n",
       "4     -0.2081527         114          114 0.7507972         0.6741071 0.6788991\n",
       "5     -0.3642962         113          112 0.8384111         0.7469697 0.7956989\n",
       "6     -0.3445611         113          112 0.8330057         0.7334562 0.7888889\n",
       "     Recall        F1 TP FN FP TN\n",
       "1 0.6696429 0.6849315 75 37 32 80\n",
       "2 0.6696429 0.6849315 75 37 32 80\n",
       "3 0.6517857 0.6697248 73 39 33 79\n",
       "4 0.6607143 0.6696833 74 38 35 77\n",
       "5 0.6666667 0.7254902 74 37 19 91\n",
       "6 0.6396396 0.7064677 71 40 19 91"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results <- distinct_model_sizes %>%\n",
    "  pmap_dfr(process_group)\n",
    "\n",
    "results %>% arrange(corpus, min_token_size, model) %>% head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f611a73",
   "metadata": {},
   "source": [
    "### LambdaG Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "416344d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the distinct corpus and min_token_size adding LambdaG as model at front\n",
    "distinct_corpus_sizes <- distinct_model_sizes %>%\n",
    "  select(corpus, min_token_size) %>%\n",
    "  distinct() %>%\n",
    "  arrange(corpus, min_token_size) %>%\n",
    "  mutate(model = \"LambdaG\") %>%\n",
    "  relocate(model, .before = everything())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d6d1f37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[38;5;246m# A tibble: 4 × 3\u001b[39m\n",
       "  model   corpus min_token_size\n",
       "  \u001b[3m\u001b[38;5;246m<chr>\u001b[39m\u001b[23m   \u001b[3m\u001b[38;5;246m<chr>\u001b[39m\u001b[23m           \u001b[3m\u001b[38;5;246m<dbl>\u001b[39m\u001b[23m\n",
       "\u001b[38;5;250m1\u001b[39m LambdaG Wiki                2\n",
       "\u001b[38;5;250m2\u001b[39m LambdaG Wiki                3\n",
       "\u001b[38;5;250m3\u001b[39m LambdaG Wiki                4\n",
       "\u001b[38;5;250m4\u001b[39m LambdaG Wiki                5"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_corpus_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b610ecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_group_lambdag <- function(model, corpus, min_token_size) {\n",
    "  \"Function to process the lambdaG results\"\n",
    "\n",
    "  # Filter score_data by the combination\n",
    "  problems <- token_size_problems %>%\n",
    "    filter(corpus == !!corpus,\n",
    "           min_token_size == !!min_token_size)\n",
    "  \n",
    "  filtered_lambdag <- lambdag_results %>%\n",
    "    inner_join(problems, by=c('problem', 'target'))\n",
    "\n",
    "  # Run your performance function (assume it returns a 1-row data frame)\n",
    "  perf <- performance(filtered_lambdag)\n",
    "  perf <- perf$evaluation\n",
    "\n",
    "  # Add the identifying columns\n",
    "  cbind(\n",
    "    data.frame(model = model,\n",
    "               corpus = corpus,\n",
    "               min_token_size = min_token_size),\n",
    "    perf\n",
    "  )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "96cc453b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |======================================================================| 100%\n",
      "Setting levels: control = FALSE, case = TRUE\n",
      "Setting direction: controls < cases\n",
      "  |======================================================================| 100%\n",
      "Setting levels: control = FALSE, case = TRUE\n",
      "Setting direction: controls < cases\n",
      "  |======================================================================| 100%\n",
      "Setting levels: control = FALSE, case = TRUE\n",
      "Setting direction: controls < cases\n",
      "  |======================================================================| 100%\n",
      "Setting levels: control = FALSE, case = TRUE\n",
      "Setting direction: controls < cases\n"
     ]
    }
   ],
   "source": [
    "results_lambdag <- distinct_corpus_sizes %>%\n",
    "  pmap_dfr(process_group_lambdag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b899cced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    model corpus min_token_size      Cllr  Cllr_min      EER Mean TRUE LLR\n",
       "1 LambdaG   Wiki              2 0.6476871 0.5243020 16.66667      1.092086\n",
       "2 LambdaG   Wiki              3 0.6374915 0.5171254 16.44444      1.139849\n",
       "3 LambdaG   Wiki              4 0.6061458 0.4756306 14.59695      1.331042\n",
       "4 LambdaG   Wiki              5 0.6872042 0.4197365 15.33742      1.709290\n",
       "  Mean FALSE LLR TRUE trials FALSE trials       AUC Balanced Accuracy Precision\n",
       "1     -0.7585601         114          114 0.9011480         0.8303571 0.8303571\n",
       "2     -0.7887780         113          112 0.9049140         0.8325962 0.8363636\n",
       "3     -0.8316408          92           55 0.9171908         0.8545073 0.9250000\n",
       "4     -0.6700987          50           12 0.8895833         0.8666667 0.9756098\n",
       "     Recall        F1 TP FN FP TN\n",
       "1 0.8303571 0.8303571 93 19 19 93\n",
       "2 0.8288288 0.8325792 92 19 18 92\n",
       "3 0.8222222 0.8705882 74 16  6 47\n",
       "4 0.8333333 0.8988764 40  8  1  9"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_lambdag %>% head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b3bddf",
   "metadata": {},
   "source": [
    "### Combine Results with LambdaG Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "891ce0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_combined <- rbind(results, results_lambdag) %>%\n",
    "  arrange(corpus, min_token_size, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ff98706b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     model corpus min_token_size      Cllr  Cllr_min      EER Mean TRUE LLR\n",
       "1  LambdaG   Wiki              2 0.6476871 0.5243020 16.66667     1.0920856\n",
       "2    gemma   Wiki              2 0.8619175 0.7444600 30.11696     0.3143858\n",
       "3     gpt2   Wiki              2 0.8618176 0.7502645 29.64912     0.3041526\n",
       "4    llama   Wiki              2 0.8770738 0.7874005 31.14035     0.2745068\n",
       "5     qwen   Wiki              2 0.8776719 0.7682051 30.73684     0.2710594\n",
       "6  LambdaG   Wiki              3 0.6374915 0.5171254 16.44444     1.1398488\n",
       "7    gemma   Wiki              3 0.7607292 0.6629074 23.76564     1.0192055\n",
       "8     gpt2   Wiki              3 0.7704295 0.6742033 25.77778     0.8408998\n",
       "9    llama   Wiki              3 0.7753341 0.6722298 25.00000     0.8893752\n",
       "10    qwen   Wiki              3 0.7943285 0.7049314 26.18090     0.7520333\n",
       "   Mean FALSE LLR TRUE trials FALSE trials       AUC Balanced Accuracy\n",
       "1      -0.7585601         114          114 0.9011480         0.8303571\n",
       "2      -0.2345160         114          114 0.7686543         0.6919643\n",
       "3      -0.2375316         114          114 0.7683355         0.6919643\n",
       "4      -0.2087909         114          114 0.7539063         0.6785714\n",
       "5      -0.2081527         114          114 0.7507972         0.6741071\n",
       "6      -0.7887780         113          112 0.9049140         0.8325962\n",
       "7      -0.3642962         113          112 0.8384111         0.7469697\n",
       "8      -0.3445611         113          112 0.8330057         0.7334562\n",
       "9      -0.3401422         113          112 0.8289926         0.7288288\n",
       "10     -0.3050944         113          112 0.8135954         0.7199017\n",
       "   Precision    Recall        F1 TP FN FP TN\n",
       "1  0.8303571 0.8303571 0.8303571 93 19 19 93\n",
       "2  0.7009346 0.6696429 0.6849315 75 37 32 80\n",
       "3  0.7009346 0.6696429 0.6849315 75 37 32 80\n",
       "4  0.6886792 0.6517857 0.6697248 73 39 33 79\n",
       "5  0.6788991 0.6607143 0.6696833 74 38 35 77\n",
       "6  0.8363636 0.8288288 0.8325792 92 19 18 92\n",
       "7  0.7956989 0.6666667 0.7254902 74 37 19 91\n",
       "8  0.7888889 0.6396396 0.7064677 71 40 19 91\n",
       "9  0.7684211 0.6576577 0.7087379 73 38 22 88\n",
       "10 0.7752809 0.6216216 0.6900000 69 42 20 90"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_combined %>% head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5b48bf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_combined %>%\n",
    "  write_xlsx(paste0(base_location, \"/idiolect_token_results_summary_avg.xlsx\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "r"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
