{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1e9339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Any, Dict, List, Sequence, Set, Tuple\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b298043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('../../src'))\n",
    "\n",
    "from read_and_write_docs import read_jsonl, read_rds\n",
    "from tokenize_and_score import load_model\n",
    "from utils import apply_temp_doc_id, build_metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c67faf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_ngrams(\n",
    "    text1: str,\n",
    "    text2: str,\n",
    "    n: int,\n",
    "    model: Any = None,\n",
    "    tokenizer: Any = None,\n",
    "    include_subgrams: bool = False,\n",
    ") -> Dict[int, Set[Tuple[Any, ...]]]:\n",
    "    \"\"\"\n",
    "    Return shared n-grams of length >= n between two texts.\n",
    "\n",
    "    If include_subgrams is False (default), remove any shared n-gram that is a\n",
    "    contiguous subspan of a longer shared n-gram. (So a 5-gram that’s part of a\n",
    "    shared 6-gram is excluded; unrelated 5-grams remain.)\n",
    "    \"\"\"\n",
    "    if n < 1:\n",
    "        raise ValueError(\"n must be >= 1\")\n",
    "\n",
    "    def _word_tokens(s: str) -> List[str]:\n",
    "        return re.findall(r\"\\w+\", s.casefold())\n",
    "\n",
    "    def _hf_tokens(txt: str) -> List[Any]:\n",
    "        if hasattr(tokenizer, \"tokenize\"):\n",
    "            return list(tokenizer.tokenize(txt))\n",
    "        enc = tokenizer(\n",
    "            txt,\n",
    "            add_special_tokens=False,\n",
    "            return_attention_mask=False,\n",
    "            return_token_type_ids=False,\n",
    "        )\n",
    "        input_ids = enc.get(\"input_ids\", [])\n",
    "        if input_ids and isinstance(input_ids[0], (list, tuple)):\n",
    "            input_ids = input_ids[0]\n",
    "        if hasattr(tokenizer, \"convert_ids_to_tokens\"):\n",
    "            return tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        return input_ids\n",
    "\n",
    "    def _ngrams_by_len(seq: Sequence[Any], min_n: int) -> Dict[int, Set[Tuple[Any, ...]]]:\n",
    "        out: Dict[int, Set[Tuple[Any, ...]]] = {}\n",
    "        L = len(seq)\n",
    "        for k in range(min_n, L + 1):\n",
    "            s: Set[Tuple[Any, ...]] = set()\n",
    "            for i in range(0, L - k + 1):\n",
    "                s.add(tuple(seq[i : i + k]))\n",
    "            if s:\n",
    "                out[k] = s\n",
    "        return out\n",
    "\n",
    "    token_mode = (model is not None) and (tokenizer is not None)\n",
    "    seq1 = _hf_tokens(text1) if token_mode else _word_tokens(text1)\n",
    "    seq2 = _hf_tokens(text2) if token_mode else _word_tokens(text2)\n",
    "\n",
    "    ngrams1 = _ngrams_by_len(seq1, n)\n",
    "    ngrams2 = _ngrams_by_len(seq2, n)\n",
    "\n",
    "    common: Dict[int, Set[Tuple[Any, ...]]] = {}\n",
    "    for k in set(ngrams1.keys()).intersection(ngrams2.keys()):\n",
    "        inter = ngrams1[k] & ngrams2[k]\n",
    "        if inter:\n",
    "            common[k] = inter\n",
    "\n",
    "    if include_subgrams or not common:\n",
    "        return common\n",
    "\n",
    "    # Remove n-grams that are contiguous subspans of any longer shared n-gram\n",
    "    to_remove: Dict[int, Set[Tuple[Any, ...]]] = defaultdict(set)\n",
    "    lengths = sorted(common.keys())\n",
    "    for k in lengths:\n",
    "        # For each longer length, generate all contiguous subspans down to n\n",
    "        for longer_k in [L for L in lengths if L > k]:\n",
    "            for g in common[longer_k]:\n",
    "                # produce all subspans of length k from g\n",
    "                for i in range(0, longer_k - k + 1):\n",
    "                    to_remove[k].add(g[i : i + k])\n",
    "\n",
    "    # Apply removals\n",
    "    for k, rem in to_remove.items():\n",
    "        if k in common:\n",
    "            common[k] = {g for g in common[k] if g not in rem}\n",
    "            if not common[k]:\n",
    "                del common[k]\n",
    "\n",
    "    return common\n",
    "\n",
    "\n",
    "def highest_common(common: Dict[int, Set[Tuple[Any, ...]]]) -> Tuple[int, Set[Tuple[Any, ...]]]:\n",
    "    \"\"\"\n",
    "    Given the dict returned by `common_ngrams`, return (max_n, ngrams_at_max).\n",
    "    If there are none, returns (0, empty set).\n",
    "    \"\"\"\n",
    "    if not common:\n",
    "        return 0, set()\n",
    "    max_k = max(common.keys())\n",
    "    return max_k, common[max_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50004f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def largest_common_ngram_problems(\n",
    "    metadata: pd.DataFrame,\n",
    "    known: pd.DataFrame,\n",
    "    unknown: pd.DataFrame,\n",
    "    n: int,  # kept for compatibility in case your helpers use it\n",
    "    model: Any = None,\n",
    "    tokenizer: Any = None,\n",
    "    print_progress: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each metadata row (keeps, problem, known_author, unknown_author, known_doc_id, unknown_doc_id),\n",
    "    filter `known`/`unknown` by doc_id, take text via .reset_index().loc[0, 'text'],\n",
    "    compute common_ngrams(...) then highest_common(common), and extract:\n",
    "      - highes_common_count: the number (first element)\n",
    "      - highes_common_ngram: the n-gram as a space-joined string\n",
    "\n",
    "    Returns columns:\n",
    "      ['keeps','problem','known_author','unknown_author','known_doc_id','unknown_doc_id',\n",
    "       'highes_common_count','highes_common_ngram']\n",
    "    \"\"\"\n",
    "    required_meta_cols = [\n",
    "        \"problem\", \"known_author\", \"unknown_author\",\n",
    "        \"known_doc_id\", \"unknown_doc_id\",\n",
    "    ]\n",
    "    missing_meta = [c for c in required_meta_cols if c not in metadata.columns]\n",
    "    if missing_meta:\n",
    "        raise ValueError(f\"metadata missing columns: {missing_meta}\")\n",
    "\n",
    "    for df_name, df in [(\"known\", known), (\"unknown\", unknown)]:\n",
    "        if \"doc_id\" not in df.columns:\n",
    "            raise ValueError(f\"'{df_name}' is missing required column 'doc_id'\")\n",
    "        if \"text\" not in df.columns:\n",
    "            raise ValueError(f\"'{df_name}' is missing required column 'text'\")\n",
    "\n",
    "    def _pick_ngram_string(ngrams: Any) -> str:\n",
    "        \"\"\"\n",
    "        Accepts one of:\n",
    "          - a tuple/list of tokens (single n-gram),\n",
    "          - a set/list of n-gram tuples (choose deterministic first),\n",
    "          - an already-joined string.\n",
    "        Returns a single space-joined n-gram string.\n",
    "        \"\"\"\n",
    "        # If we received a collection of n-grams, pick a deterministic one\n",
    "        if isinstance(ngrams, (set, list, tuple)) and ngrams and isinstance(next(iter(ngrams)), (tuple, list, str)):\n",
    "            # If it's a set/list of tuples/lists/strings, sort deterministically\n",
    "            if isinstance(ngrams, (set, list)) and ngrams and not isinstance(ngrams, str):\n",
    "                try:\n",
    "                    candidate = sorted(ngrams)[0]\n",
    "                except Exception:\n",
    "                    candidate = next(iter(ngrams))\n",
    "            else:\n",
    "                candidate = ngrams  # already a single n-gram tuple/list/str\n",
    "        else:\n",
    "            candidate = ngrams\n",
    "\n",
    "        # If candidate is a sequence of tokens, join with spaces; otherwise cast to str\n",
    "        if isinstance(candidate, (tuple, list)):\n",
    "            return \" \".join(map(str, candidate))\n",
    "        return str(candidate)\n",
    "\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    total = len(metadata)\n",
    "    it = metadata[required_meta_cols].itertuples(index=False, name=\"MetaRow\")\n",
    "\n",
    "    for i, row in enumerate(it, 1):\n",
    "        problem, known_author, unknown_author, known_doc_id, unknown_doc_id = row\n",
    "\n",
    "        kdf = known.loc[known[\"doc_id\"] == known_doc_id].reset_index(drop=True)\n",
    "        udf = unknown.loc[unknown[\"doc_id\"] == unknown_doc_id].reset_index(drop=True)\n",
    "\n",
    "        if kdf.empty or udf.empty:\n",
    "            count_val = None\n",
    "            ngram_str = None\n",
    "        else:\n",
    "            text_known = kdf.loc[0, \"text\"]\n",
    "            text_unknown = udf.loc[0, \"text\"]\n",
    "\n",
    "            # If your common_ngrams expects n, switch to: common_ngrams(text_known, text_unknown, n)\n",
    "            common = common_ngrams(text_known, text_unknown, n=n)\n",
    "            hc = highest_common(common)\n",
    "\n",
    "            if hc is None:\n",
    "                count_val = None\n",
    "                ngram_str = None\n",
    "            else:\n",
    "                # Expecting (number, ngrams)\n",
    "                try:\n",
    "                    count_val, ngrams_obj = hc\n",
    "                except Exception:\n",
    "                    # Fallback: treat whole object as the ngram payload and set count None\n",
    "                    count_val = None\n",
    "                    ngrams_obj = hc\n",
    "                ngram_str = _pick_ngram_string(ngrams_obj)\n",
    "\n",
    "        rows.append({\n",
    "            \"problem\": problem,\n",
    "            \"known_author\": known_author,\n",
    "            \"unknown_author\": unknown_author,\n",
    "            \"known_doc_id\": known_doc_id,\n",
    "            \"unknown_doc_id\": unknown_doc_id,\n",
    "            \"highest_common_count\": count_val,      # extracted number\n",
    "            \"highest_common_ngram\": ngram_str,      # tokens joined by ' '\n",
    "        })\n",
    "\n",
    "        if print_progress and total:\n",
    "            if (i % max(1, total // 50) == 0) or (i == total):\n",
    "                pct = int(i * 100 / total)\n",
    "                print(f\"\\rProcessed {i}/{total} ({pct}%)\", end=\"\")\n",
    "\n",
    "    if print_progress:\n",
    "        print()\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e487a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_loc = \"/Volumes/BCross/datasets/author_verification/training/Wiki/known_raw.jsonl\"\n",
    "known = read_jsonl(known_loc)\n",
    "known = apply_temp_doc_id(known)\n",
    "\n",
    "known_loc = \"/Volumes/BCross/datasets/author_verification/training/Wiki/known_corpus_split/hipocrite_text_3.jsonl\"\n",
    "known_text = read_jsonl(known_loc).loc[0, 'text']\n",
    "\n",
    "unknown_loc = \"/Volumes/BCross/datasets/author_verification/training/Wiki/unknown_raw.jsonl\"\n",
    "unknown = read_jsonl(unknown_loc)\n",
    "unknown_df = apply_temp_doc_id(unknown)\n",
    "\n",
    "unknown_text = unknown_df[unknown_df['doc_id']=='hipocrite_text_4'].reset_index().loc[0, 'text']\n",
    "\n",
    "metadata_loc = \"/Volumes/BCross/datasets/author_verification/training/metadata.rds\"\n",
    "metadata = read_rds(metadata_loc)\n",
    "filtered_metadata = metadata[metadata['corpus'] == 'Wiki']\n",
    "agg_metadata = build_metadata_df(filtered_metadata, known, unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "147187ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 450/450 (100%)\n"
     ]
    }
   ],
   "source": [
    "wiki_problems = largest_common_ngram_problems(agg_metadata, known, unknown, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57832365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-10>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>known_author</th>\n",
       "      <th>unknown_author</th>\n",
       "      <th>known_doc_id</th>\n",
       "      <th>unknown_doc_id</th>\n",
       "      <th>highest_common_count</th>\n",
       "      <th>highest_common_ngram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>Greg_L vs Greg_L</td>\n",
       "      <td>Greg_L</td>\n",
       "      <td>Greg_L</td>\n",
       "      <td>greg_l_text_11</td>\n",
       "      <td>greg_l_text_10</td>\n",
       "      <td>9</td>\n",
       "      <td>allege harbor views that are contrary to the c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>Haymaker vs Haymaker</td>\n",
       "      <td>Haymaker</td>\n",
       "      <td>Haymaker</td>\n",
       "      <td>haymaker_text_3</td>\n",
       "      <td>haymaker_text_2</td>\n",
       "      <td>8</td>\n",
       "      <td>at the end of the day we re</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>EdJohnston vs EdJohnston</td>\n",
       "      <td>EdJohnston</td>\n",
       "      <td>EdJohnston</td>\n",
       "      <td>edjohnston_text_2</td>\n",
       "      <td>edjohnston_text_5</td>\n",
       "      <td>7</td>\n",
       "      <td>i don t see any reason to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Atama vs Atama</td>\n",
       "      <td>Atama</td>\n",
       "      <td>Atama</td>\n",
       "      <td>atama_text_1</td>\n",
       "      <td>atama_text_5</td>\n",
       "      <td>7</td>\n",
       "      <td>the article there shouldn t be any</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>Fipplet vs Fipplet</td>\n",
       "      <td>Fipplet</td>\n",
       "      <td>Fipplet</td>\n",
       "      <td>fipplet_text_2</td>\n",
       "      <td>fipplet_text_5</td>\n",
       "      <td>6</td>\n",
       "      <td>doesn t mean we should state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>Fragments_of_Jade vs Fragments_of_Jade</td>\n",
       "      <td>Fragments_of_Jade</td>\n",
       "      <td>Fragments_of_Jade</td>\n",
       "      <td>fragments_of_jade_text_4</td>\n",
       "      <td>fragments_of_jade_text_10</td>\n",
       "      <td>4</td>\n",
       "      <td>on my talk page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>Hardyplants vs Hardyplants</td>\n",
       "      <td>Hardyplants</td>\n",
       "      <td>Hardyplants</td>\n",
       "      <td>hardyplants_text_2</td>\n",
       "      <td>hardyplants_text_4</td>\n",
       "      <td>4</td>\n",
       "      <td>that it is a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>Garda40 vs Garda40</td>\n",
       "      <td>Garda40</td>\n",
       "      <td>Garda40</td>\n",
       "      <td>garda40_text_3</td>\n",
       "      <td>garda40_text_1</td>\n",
       "      <td>4</td>\n",
       "      <td>is not an issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>George vs George</td>\n",
       "      <td>George</td>\n",
       "      <td>George</td>\n",
       "      <td>george_text_10</td>\n",
       "      <td>george_text_13</td>\n",
       "      <td>4</td>\n",
       "      <td>i don t have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>BrotherDarksoul vs BrotherDarksoul</td>\n",
       "      <td>BrotherDarksoul</td>\n",
       "      <td>BrotherDarksoul</td>\n",
       "      <td>brotherdarksoul_text_1</td>\n",
       "      <td>brotherdarksoul_text_4</td>\n",
       "      <td>4</td>\n",
       "      <td>being that of a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    problem       known_author  \\\n",
       "379                        Greg_L vs Greg_L             Greg_L   \n",
       "432                    Haymaker vs Haymaker           Haymaker   \n",
       "270                EdJohnston vs EdJohnston         EdJohnston   \n",
       "72                           Atama vs Atama              Atama   \n",
       "331                      Fipplet vs Fipplet            Fipplet   \n",
       "..                                      ...                ...   \n",
       "356  Fragments_of_Jade vs Fragments_of_Jade  Fragments_of_Jade   \n",
       "421              Hardyplants vs Hardyplants        Hardyplants   \n",
       "367                      Garda40 vs Garda40            Garda40   \n",
       "373                        George vs George             George   \n",
       "138      BrotherDarksoul vs BrotherDarksoul    BrotherDarksoul   \n",
       "\n",
       "        unknown_author              known_doc_id             unknown_doc_id  \\\n",
       "379             Greg_L            greg_l_text_11             greg_l_text_10   \n",
       "432           Haymaker           haymaker_text_3            haymaker_text_2   \n",
       "270         EdJohnston         edjohnston_text_2          edjohnston_text_5   \n",
       "72               Atama              atama_text_1               atama_text_5   \n",
       "331            Fipplet            fipplet_text_2             fipplet_text_5   \n",
       "..                 ...                       ...                        ...   \n",
       "356  Fragments_of_Jade  fragments_of_jade_text_4  fragments_of_jade_text_10   \n",
       "421        Hardyplants        hardyplants_text_2         hardyplants_text_4   \n",
       "367            Garda40            garda40_text_3             garda40_text_1   \n",
       "373             George            george_text_10             george_text_13   \n",
       "138    BrotherDarksoul    brotherdarksoul_text_1     brotherdarksoul_text_4   \n",
       "\n",
       "     highest_common_count                               highest_common_ngram  \n",
       "379                     9  allege harbor views that are contrary to the c...  \n",
       "432                     8                        at the end of the day we re  \n",
       "270                     7                          i don t see any reason to  \n",
       "72                      7                 the article there shouldn t be any  \n",
       "331                     6                       doesn t mean we should state  \n",
       "..                    ...                                                ...  \n",
       "356                     4                                    on my talk page  \n",
       "421                     4                                       that it is a  \n",
       "367                     4                                    is not an issue  \n",
       "373                     4                                       i don t have  \n",
       "138                     4                                    being that of a  \n",
       "\n",
       "[89 rows x 7 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_author_problems = wiki_problems[wiki_problems['known_author'] == wiki_problems['unknown_author']]\n",
    "same_author_problems.sort_values([\"highest_common_count\"], ascending=[False], inplace=True)\n",
    "same_author_problems[(same_author_problems['highest_common_count'] > 3) & (same_author_problems['highest_common_count'] <= 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94029a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-11>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>known_author</th>\n",
       "      <th>unknown_author</th>\n",
       "      <th>known_doc_id</th>\n",
       "      <th>unknown_doc_id</th>\n",
       "      <th>highest_common_count</th>\n",
       "      <th>highest_common_ngram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Britmax vs BrotherDarksoul</td>\n",
       "      <td>Britmax</td>\n",
       "      <td>BrotherDarksoul</td>\n",
       "      <td>britmax_text_1</td>\n",
       "      <td>brotherdarksoul_text_4</td>\n",
       "      <td>5</td>\n",
       "      <td>there is no need to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>Falcon9x5 vs Fipplet</td>\n",
       "      <td>Falcon9x5</td>\n",
       "      <td>Fipplet</td>\n",
       "      <td>falcon9x5_text_3</td>\n",
       "      <td>fipplet_text_5</td>\n",
       "      <td>5</td>\n",
       "      <td>i don t know if</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>Enemesis vs Equanimous1</td>\n",
       "      <td>Enemesis</td>\n",
       "      <td>Equanimous1</td>\n",
       "      <td>enemesis_text_3</td>\n",
       "      <td>equanimous1_text_5</td>\n",
       "      <td>5</td>\n",
       "      <td>the current state of the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>Enemesis vs Equanimous1</td>\n",
       "      <td>Enemesis</td>\n",
       "      <td>Equanimous1</td>\n",
       "      <td>enemesis_text_2</td>\n",
       "      <td>equanimous1_text_5</td>\n",
       "      <td>5</td>\n",
       "      <td>the current state of the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>Fyunck(click) vs Garda40</td>\n",
       "      <td>Fyunck(click)</td>\n",
       "      <td>Garda40</td>\n",
       "      <td>fyunck_click_text_12</td>\n",
       "      <td>garda40_text_1</td>\n",
       "      <td>4</td>\n",
       "      <td>i m not sure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>AlanBarnet vs Alanyst</td>\n",
       "      <td>AlanBarnet</td>\n",
       "      <td>Alanyst</td>\n",
       "      <td>alanbarnet_text_10</td>\n",
       "      <td>alanyst_text_10</td>\n",
       "      <td>3</td>\n",
       "      <td>out of the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Atama vs Athenean</td>\n",
       "      <td>Atama</td>\n",
       "      <td>Athenean</td>\n",
       "      <td>atama_text_1</td>\n",
       "      <td>athenean_text_5</td>\n",
       "      <td>3</td>\n",
       "      <td>a lot of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>Chanakyathegreat vs Cla68</td>\n",
       "      <td>Chanakyathegreat</td>\n",
       "      <td>Cla68</td>\n",
       "      <td>chanakyathegreat_text_11</td>\n",
       "      <td>cla68_text_1</td>\n",
       "      <td>3</td>\n",
       "      <td>added to the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>AlanBarnet vs Alanyst</td>\n",
       "      <td>AlanBarnet</td>\n",
       "      <td>Alanyst</td>\n",
       "      <td>alanbarnet_text_4</td>\n",
       "      <td>alanyst_text_10</td>\n",
       "      <td>3</td>\n",
       "      <td>in their own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Caboga vs Chanakyathegreat</td>\n",
       "      <td>Caboga</td>\n",
       "      <td>Chanakyathegreat</td>\n",
       "      <td>caboga_text_5</td>\n",
       "      <td>chanakyathegreat_text_1</td>\n",
       "      <td>3</td>\n",
       "      <td>in the past</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>145 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        problem      known_author    unknown_author  \\\n",
       "135  Britmax vs BrotherDarksoul           Britmax   BrotherDarksoul   \n",
       "328        Falcon9x5 vs Fipplet         Falcon9x5           Fipplet   \n",
       "287     Enemesis vs Equanimous1          Enemesis       Equanimous1   \n",
       "286     Enemesis vs Equanimous1          Enemesis       Equanimous1   \n",
       "364    Fyunck(click) vs Garda40     Fyunck(click)           Garda40   \n",
       "..                          ...               ...               ...   \n",
       "27        AlanBarnet vs Alanyst        AlanBarnet           Alanyst   \n",
       "75            Atama vs Athenean             Atama          Athenean   \n",
       "166   Chanakyathegreat vs Cla68  Chanakyathegreat             Cla68   \n",
       "29        AlanBarnet vs Alanyst        AlanBarnet           Alanyst   \n",
       "161  Caboga vs Chanakyathegreat            Caboga  Chanakyathegreat   \n",
       "\n",
       "                 known_doc_id           unknown_doc_id  highest_common_count  \\\n",
       "135            britmax_text_1   brotherdarksoul_text_4                     5   \n",
       "328          falcon9x5_text_3           fipplet_text_5                     5   \n",
       "287           enemesis_text_3       equanimous1_text_5                     5   \n",
       "286           enemesis_text_2       equanimous1_text_5                     5   \n",
       "364      fyunck_click_text_12           garda40_text_1                     4   \n",
       "..                        ...                      ...                   ...   \n",
       "27         alanbarnet_text_10          alanyst_text_10                     3   \n",
       "75               atama_text_1          athenean_text_5                     3   \n",
       "166  chanakyathegreat_text_11             cla68_text_1                     3   \n",
       "29          alanbarnet_text_4          alanyst_text_10                     3   \n",
       "161             caboga_text_5  chanakyathegreat_text_1                     3   \n",
       "\n",
       "         highest_common_ngram  \n",
       "135       there is no need to  \n",
       "328           i don t know if  \n",
       "287  the current state of the  \n",
       "286  the current state of the  \n",
       "364              i m not sure  \n",
       "..                        ...  \n",
       "27                 out of the  \n",
       "75                   a lot of  \n",
       "166              added to the  \n",
       "29               in their own  \n",
       "161               in the past  \n",
       "\n",
       "[145 rows x 7 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_author_problems = wiki_problems[wiki_problems['known_author'] != wiki_problems['unknown_author']]\n",
    "diff_author_problems.sort_values([\"highest_common_count\"], ascending=[False], inplace=True)\n",
    "diff_author_problems[(diff_author_problems['highest_common_count'] > 2) & (diff_author_problems['highest_common_count'] <= 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48874d8",
   "metadata": {},
   "source": [
    "## Pick a test\n",
    "\n",
    "Chose a same author and different author test containing at least one of the same authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7e65ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_author_known = known[known['doc_id'] == 'fipplet_text_2'].reset_index().loc[0, 'text']\n",
    "same_author_unknown = unknown[unknown['doc_id'] == 'fipplet_text_5'].reset_index().loc[0, 'text']\n",
    "\n",
    "diff_author_known = known[known['doc_id'] == 'falcon9x5_text_3'].reset_index().loc[0, 'text']\n",
    "diff_author_unknown = unknown[unknown['doc_id'] == 'fipplet_text_5'].reset_index().loc[0, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f76b9ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_author_common = common_ngrams(same_author_known, same_author_unknown, n=2)\n",
    "diff_author_common = common_ngrams(diff_author_known, diff_author_unknown, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f26efab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: {('about', 'the'),\n",
       "  ('at', 'least'),\n",
       "  ('been', 'a'),\n",
       "  ('calling', 'it'),\n",
       "  ('definition', 'of'),\n",
       "  ('if', 'you'),\n",
       "  ('is', 'the'),\n",
       "  ('isn', 't'),\n",
       "  ('it', 's'),\n",
       "  ('that', 'the'),\n",
       "  ('that', 'was'),\n",
       "  ('this', 'is'),\n",
       "  ('this', 'to'),\n",
       "  ('to', 'be'),\n",
       "  ('want', 'to'),\n",
       "  ('was', 'the'),\n",
       "  ('with', 'the')},\n",
       " 3: {('part', 'of', 'the')},\n",
       " 4: {('i', 'don', 't', 'think')},\n",
       " 5: {('i', 'don', 't', 'know', 'if')}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_author_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e4964ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, {('i', 'don', 't', 'know', 'if')})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_common(diff_author_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e739d0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def largest_common_ngram_df(\n",
    "    directory: str,\n",
    "    n: int,\n",
    "    model: Any = None,\n",
    "    tokenizer: Any = None,\n",
    "    ordered_pairs: bool = False,\n",
    "    print_progress: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scan .jsonl files in `directory` and, for each file pair, compute:\n",
    "      - largest shared n-gram length (>= n)\n",
    "      - all n-grams at that largest length (as strings joined by spaces)\n",
    "\n",
    "    Returns a DataFrame with columns:\n",
    "      ['text_1', 'text_2', 'largest_n', 'largest_ngrams']\n",
    "\n",
    "    Notes:\n",
    "      - Uses `read_jsonl(path).loc[0, 'text']` to load each file (assumes single row with 'text').\n",
    "      - If BOTH `model` and `tokenizer` are provided, tokenizes first and works over tokens.\n",
    "        Otherwise uses a simple word tokenizer (casefolded, punctuation stripped).\n",
    "      - `ordered_pairs=False` yields unordered pairs (A,B) once. Set True to include (B,A).\n",
    "      - Set `print_progress=False` to silence progress prints.\n",
    "    \"\"\"\n",
    "    if n < 1:\n",
    "        raise ValueError(\"n must be >= 1\")\n",
    "\n",
    "    # Gather just .jsonl files\n",
    "    paths = [\n",
    "        os.path.join(directory, fn)\n",
    "        for fn in os.listdir(directory)\n",
    "        if fn.lower().endswith(\".jsonl\")\n",
    "    ]\n",
    "    paths.sort()\n",
    "    if len(paths) < 2:\n",
    "        raise ValueError(\"Need at least two .jsonl files in the directory.\")\n",
    "\n",
    "    if print_progress:\n",
    "        print(f\"Found {len(paths)} .jsonl files in: {directory}\")\n",
    "\n",
    "    # --- Load texts using your read_jsonl ---\n",
    "    texts: Dict[str, str] = {}\n",
    "    for p in paths:\n",
    "        df = read_jsonl(p)\n",
    "        if \"text\" not in df.columns:\n",
    "            raise ValueError(f\"'text' column not found in {p}\")\n",
    "        if df.shape[0] < 1:\n",
    "            raise ValueError(f\"No rows found in {p}\")\n",
    "        texts[p] = str(df.loc[0, \"text\"])\n",
    "\n",
    "    # --- Tokenization helpers ---\n",
    "    def _word_tokens(s: str) -> List[str]:\n",
    "        return re.findall(r\"\\w+\", s.casefold())\n",
    "\n",
    "    def _hf_tokens(txt: str) -> List[Any]:\n",
    "        if hasattr(tokenizer, \"tokenize\"):\n",
    "            return list(tokenizer.tokenize(txt))\n",
    "        enc = tokenizer(\n",
    "            txt,\n",
    "            add_special_tokens=False,\n",
    "            return_attention_mask=False,\n",
    "            return_token_type_ids=False,\n",
    "        )\n",
    "        input_ids = enc.get(\"input_ids\", [])\n",
    "        if input_ids and isinstance(input_ids[0], (list, tuple)):\n",
    "            input_ids = input_ids[0]\n",
    "        if hasattr(tokenizer, \"convert_ids_to_tokens\"):\n",
    "            return tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        return input_ids\n",
    "\n",
    "    token_mode = (model is not None) and (tokenizer is not None)\n",
    "\n",
    "    # Pre-tokenize all docs once, with progress\n",
    "    docs: Dict[str, List[Any]] = {}\n",
    "    if print_progress:\n",
    "        print(\"Tokenizing documents...\")\n",
    "    for i, p in enumerate(paths, 1):\n",
    "        docs[p] = _hf_tokens(texts[p]) if token_mode else _word_tokens(texts[p])\n",
    "        if print_progress:\n",
    "            interval = max(1, len(paths) // 20)\n",
    "            if (i % interval == 0) or (i == len(paths)):\n",
    "                pct = int(i * 100 / len(paths))\n",
    "                sys.stdout.write(f\"\\r  Tokenized {i}/{len(paths)} ({pct}%)\")\n",
    "                sys.stdout.flush()\n",
    "    if print_progress:\n",
    "        print()\n",
    "\n",
    "    # Cache n-gram sets per doc per k to reuse across many pairs\n",
    "    cache: Dict[str, Dict[int, Set[Tuple[Any, ...]]]] = {p: {} for p in paths}\n",
    "\n",
    "    def ngram_set(seq: Sequence[Any], k: int, key: str) -> Set[Tuple[Any, ...]]:\n",
    "        cdoc = cache[key]\n",
    "        if k in cdoc:\n",
    "            return cdoc[k]\n",
    "        L = len(seq)\n",
    "        s: Set[Tuple[Any, ...]] = set()\n",
    "        if k <= L:\n",
    "            for i in range(L - k + 1):\n",
    "                s.add(tuple(seq[i : i + k]))\n",
    "        cdoc[k] = s\n",
    "        return s\n",
    "\n",
    "    # Pair generator\n",
    "    pair_iter = (\n",
    "        itertools.permutations(paths, 2) if ordered_pairs\n",
    "        else itertools.combinations(paths, 2)\n",
    "    )\n",
    "    total_pairs = len(paths) * (len(paths) - 1) if ordered_pairs else (len(paths) * (len(paths) - 1)) // 2\n",
    "    if print_progress:\n",
    "        print(f\"Comparing {total_pairs} {'ordered' if ordered_pairs else 'unordered'} pairs...\")\n",
    "\n",
    "    rows = []\n",
    "    for idx, (p1, p2) in enumerate(pair_iter, 1):\n",
    "        seq1, seq2 = docs[p1], docs[p2]\n",
    "        largest = 0\n",
    "        largest_set: Set[Tuple[Any, ...]] = set()\n",
    "\n",
    "        if len(seq1) >= n and len(seq2) >= n:\n",
    "            upper = min(len(seq1), len(seq2))\n",
    "            k = n\n",
    "            while k <= upper:\n",
    "                s1 = ngram_set(seq1, k, p1)\n",
    "                s2 = ngram_set(seq2, k, p2)\n",
    "                if not s1 or not s2:\n",
    "                    break\n",
    "                inter = s1 & s2\n",
    "                if not inter:\n",
    "                    break\n",
    "                largest = k\n",
    "                largest_set = inter\n",
    "                k += 1\n",
    "\n",
    "        # Convert the winning n-grams (tuples) into space-joined strings\n",
    "        largest_ngrams = [\" \".join(map(str, tup)) for tup in sorted(largest_set)]\n",
    "\n",
    "        rows.append({\n",
    "            \"text_1\": os.path.basename(p1),\n",
    "            \"text_2\": os.path.basename(p2),\n",
    "            \"largest_n\": int(largest),\n",
    "            \"largest_ngrams\": largest_ngrams,   # list[str]\n",
    "        })\n",
    "\n",
    "        if print_progress:\n",
    "            interval = max(1, total_pairs // 100)\n",
    "            if (idx % interval == 0) or (idx == total_pairs):\n",
    "                pct = int(idx * 100 / total_pairs)\n",
    "                sys.stdout.write(f\"\\r  Progress {idx}/{total_pairs} ({pct}%)\")\n",
    "                sys.stdout.flush()\n",
    "\n",
    "    if print_progress:\n",
    "        print(\"\\nDone.\")\n",
    "\n",
    "    df = (\n",
    "        pd.DataFrame(rows)\n",
    "        .sort_values([\"largest_n\", \"text_1\", \"text_2\"], ascending=[False, True, True])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a5985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Any, Dict, Set, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "# assumes `read_jsonl` and `common_ngrams` are already defined/imported\n",
    "\n",
    "def largest_common_ngram_df(\n",
    "    directory: str,\n",
    "    n: int,\n",
    "    model: Any = None,\n",
    "    tokenizer: Any = None,\n",
    "    ordered_pairs: bool = False,\n",
    "    print_progress: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scan .jsonl files in `directory` and, for each file pair, compute:\n",
    "      - largest shared n-gram length (>= n)\n",
    "      - all n-grams at that largest length (as strings joined by spaces)\n",
    "\n",
    "    Returns a DataFrame with columns:\n",
    "      ['text_1', 'text_2', 'largest_n', 'largest_ngrams']\n",
    "\n",
    "    Notes:\n",
    "      - Uses `read_jsonl(path).loc[0, 'text']` to load each file (assumes single row with 'text').\n",
    "      - Delegates n-gram logic to `common_ngrams(..., include_subgrams=False)`.\n",
    "      - If BOTH `model` and `tokenizer` are provided, `common_ngrams` will use token mode; otherwise word mode.\n",
    "      - `ordered_pairs=False` yields unordered pairs (A,B) once. Set True to include (B,A).\n",
    "      - Set `print_progress=False` to silence progress prints.\n",
    "    \"\"\"\n",
    "    if n < 1:\n",
    "        raise ValueError(\"n must be >= 1\")\n",
    "\n",
    "    # Gather just .jsonl files\n",
    "    paths = [\n",
    "        os.path.join(directory, fn)\n",
    "        for fn in os.listdir(directory)\n",
    "        if fn.lower().endswith(\".jsonl\")\n",
    "    ]\n",
    "    paths.sort()\n",
    "    if len(paths) < 2:\n",
    "        raise ValueError(\"Need at least two .jsonl files in the directory.\")\n",
    "\n",
    "    if print_progress:\n",
    "        print(f\"Found {len(paths)} .jsonl files in: {directory}\")\n",
    "\n",
    "    # --- Load texts using your read_jsonl ---\n",
    "    texts: Dict[str, str] = {}\n",
    "    for p in paths:\n",
    "        df = read_jsonl(p)\n",
    "        if \"text\" not in df.columns:\n",
    "            raise ValueError(f\"'text' column not found in {p}\")\n",
    "        if df.shape[0] < 1:\n",
    "            raise ValueError(f\"No rows found in {p}\")\n",
    "        texts[p] = str(df.loc[0, \"text\"])\n",
    "\n",
    "    # Pair generator\n",
    "    pair_iter = (\n",
    "        itertools.permutations(paths, 2) if ordered_pairs\n",
    "        else itertools.combinations(paths, 2)\n",
    "    )\n",
    "    total_pairs = len(paths) * (len(paths) - 1) if ordered_pairs else (len(paths) * (len(paths) - 1)) // 2\n",
    "    if print_progress:\n",
    "        print(f\"Comparing {total_pairs} {'ordered' if ordered_pairs else 'unordered'} pairs...\")\n",
    "\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    for idx, (p1, p2) in enumerate(pair_iter, 1):\n",
    "        # Use common_ngrams to get only the longest shared n-grams\n",
    "        cmn = common_ngrams(\n",
    "            texts[p1],\n",
    "            texts[p2],\n",
    "            n,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            include_subgrams=False,  # keep only the globally-longest shared n-grams\n",
    "        )\n",
    "\n",
    "        if cmn:\n",
    "            largest_k = max(cmn.keys())\n",
    "            ng_set: Set[Tuple[Any, ...]] = cmn[largest_k]\n",
    "        else:\n",
    "            largest_k = 0\n",
    "            ng_set = set()\n",
    "\n",
    "        # Convert the winning n-grams (tuples) into space-joined strings\n",
    "        largest_ngrams = [\" \".join(map(str, tup)) for tup in sorted(ng_set)]\n",
    "\n",
    "        rows.append({\n",
    "            \"text_1\": os.path.basename(p1),\n",
    "            \"text_2\": os.path.basename(p2),\n",
    "            \"largest_n\": int(largest_k),\n",
    "            \"largest_ngrams\": largest_ngrams,   # list[str]\n",
    "        })\n",
    "\n",
    "        if print_progress:\n",
    "            interval = max(1, total_pairs // 100)\n",
    "            if (idx % interval == 0) or (idx == total_pairs):\n",
    "                pct = int(idx * 100 / total_pairs)\n",
    "                sys.stdout.write(f\"\\r  Progress {idx}/{total_pairs} ({pct}%)\")\n",
    "                sys.stdout.flush()\n",
    "\n",
    "    if print_progress:\n",
    "        print(\"\\nDone.\")\n",
    "\n",
    "    df = (\n",
    "        pd.DataFrame(rows)\n",
    "        .sort_values([\"largest_n\", \"text_1\", \"text_2\"], ascending=[False, True, True])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "61117d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 225 .jsonl files in: /Volumes/BCross/datasets/author_verification/training/Wiki/known_corpus_split/\n",
      "Tokenizing documents...\n",
      "  Tokenized 225/225 (100%)\n",
      "Comparing 25200 unordered pairs...\n",
      "  Progress 25200/25200 (100%)\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "results = largest_common_ngram_df(\n",
    "    directory='/Volumes/BCross/datasets/author_verification/training/Wiki/known_corpus_split/',\n",
    "    n=2,\n",
    "    model=None,\n",
    "    tokenizer=None,\n",
    "    ordered_pairs=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2fad53c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>largest_n</th>\n",
       "      <th>largest_ngrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bfigura_text_4.jsonl</td>\n",
       "      <td>falcon9x5_text_2.jsonl</td>\n",
       "      <td>7</td>\n",
       "      <td>[as far as i m aware it]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>caboga_text_3.jsonl</td>\n",
       "      <td>caboga_text_5.jsonl</td>\n",
       "      <td>7</td>\n",
       "      <td>[a reliable source for the name kabudzic]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>classicjupiter2_text_4.jsonl</td>\n",
       "      <td>david_shankbone_text_5.jsonl</td>\n",
       "      <td>7</td>\n",
       "      <td>[just wanted to drop you a note]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>classicjupiter2_text_4.jsonl</td>\n",
       "      <td>habap_text_3.jsonl</td>\n",
       "      <td>7</td>\n",
       "      <td>[just wanted to drop you a note]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>david_shankbone_text_5.jsonl</td>\n",
       "      <td>habap_text_3.jsonl</td>\n",
       "      <td>7</td>\n",
       "      <td>[just wanted to drop you a note]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2318</th>\n",
       "      <td>hardyplants_text_2.jsonl</td>\n",
       "      <td>hardyplants_text_3.jsonl</td>\n",
       "      <td>4</td>\n",
       "      <td>[that it is a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2319</th>\n",
       "      <td>hardyplants_text_3.jsonl</td>\n",
       "      <td>headleydown_text_2.jsonl</td>\n",
       "      <td>4</td>\n",
       "      <td>[on the talk page]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2320</th>\n",
       "      <td>hardyplants_text_3.jsonl</td>\n",
       "      <td>headleydown_text_3.jsonl</td>\n",
       "      <td>4</td>\n",
       "      <td>[on the talk page]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2321</th>\n",
       "      <td>haymaker_text_3.jsonl</td>\n",
       "      <td>hipocrite_text_3.jsonl</td>\n",
       "      <td>4</td>\n",
       "      <td>[you choose not to]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2322</th>\n",
       "      <td>haymaker_text_5.jsonl</td>\n",
       "      <td>headleydown_text_2.jsonl</td>\n",
       "      <td>4</td>\n",
       "      <td>[would be far more]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2304 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text_1                        text_2  largest_n  \\\n",
       "19            bfigura_text_4.jsonl        falcon9x5_text_2.jsonl          7   \n",
       "20             caboga_text_3.jsonl           caboga_text_5.jsonl          7   \n",
       "21    classicjupiter2_text_4.jsonl  david_shankbone_text_5.jsonl          7   \n",
       "22    classicjupiter2_text_4.jsonl            habap_text_3.jsonl          7   \n",
       "23    david_shankbone_text_5.jsonl            habap_text_3.jsonl          7   \n",
       "...                            ...                           ...        ...   \n",
       "2318      hardyplants_text_2.jsonl      hardyplants_text_3.jsonl          4   \n",
       "2319      hardyplants_text_3.jsonl      headleydown_text_2.jsonl          4   \n",
       "2320      hardyplants_text_3.jsonl      headleydown_text_3.jsonl          4   \n",
       "2321         haymaker_text_3.jsonl        hipocrite_text_3.jsonl          4   \n",
       "2322         haymaker_text_5.jsonl      headleydown_text_2.jsonl          4   \n",
       "\n",
       "                                 largest_ngrams  \n",
       "19                     [as far as i m aware it]  \n",
       "20    [a reliable source for the name kabudzic]  \n",
       "21             [just wanted to drop you a note]  \n",
       "22             [just wanted to drop you a note]  \n",
       "23             [just wanted to drop you a note]  \n",
       "...                                         ...  \n",
       "2318                             [that it is a]  \n",
       "2319                         [on the talk page]  \n",
       "2320                         [on the talk page]  \n",
       "2321                        [you choose not to]  \n",
       "2322                        [would be far more]  \n",
       "\n",
       "[2304 rows x 4 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[(results['largest_n']>3) & (results['largest_n']<8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65888c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 225 .jsonl files in: /Volumes/BCross/datasets/author_verification/training/Wiki/known_corpus_split/\n",
      "Comparing 25200 unordered pairs...\n",
      "  Progress 252/25200 (1%)"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mlargest_common_ngram_df\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Volumes/BCross/datasets/author_verification/training/Wiki/known_corpus_split/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mordered_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n",
      "\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "\n",
      "Cell \u001b[0;32mIn[30], line 70\u001b[0m, in \u001b[0;36mlargest_common_ngram_df\u001b[0;34m(directory, n, model, tokenizer, ordered_pairs, print_progress)\u001b[0m\n",
      "\u001b[1;32m     67\u001b[0m rows: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (p1, p2) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pair_iter, \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# Use common_ngrams to get only the longest shared n-grams\u001b[39;00m\n",
      "\u001b[0;32m---> 70\u001b[0m     cmn \u001b[38;5;241m=\u001b[39m \u001b[43mcommon_ngrams\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_subgrams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# keep only the globally-longest shared n-grams\u001b[39;49;00m\n",
      "\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cmn:\n",
      "\u001b[1;32m     80\u001b[0m         largest_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(cmn\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\n",
      "Cell \u001b[0;32mIn[3], line 53\u001b[0m, in \u001b[0;36mcommon_ngrams\u001b[0;34m(text1, text2, n, model, tokenizer, include_subgrams)\u001b[0m\n",
      "\u001b[1;32m     50\u001b[0m seq1 \u001b[38;5;241m=\u001b[39m _hf_tokens(text1) \u001b[38;5;28;01mif\u001b[39;00m token_mode \u001b[38;5;28;01melse\u001b[39;00m _word_tokens(text1)\n",
      "\u001b[1;32m     51\u001b[0m seq2 \u001b[38;5;241m=\u001b[39m _hf_tokens(text2) \u001b[38;5;28;01mif\u001b[39;00m token_mode \u001b[38;5;28;01melse\u001b[39;00m _word_tokens(text2)\n",
      "\u001b[0;32m---> 53\u001b[0m ngrams1 \u001b[38;5;241m=\u001b[39m \u001b[43m_ngrams_by_len\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     54\u001b[0m ngrams2 \u001b[38;5;241m=\u001b[39m _ngrams_by_len(seq2, n)\n",
      "\u001b[1;32m     56\u001b[0m common: Dict[\u001b[38;5;28mint\u001b[39m, Set[Tuple[Any, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]] \u001b[38;5;241m=\u001b[39m {}\n",
      "\n",
      "Cell \u001b[0;32mIn[3], line 44\u001b[0m, in \u001b[0;36mcommon_ngrams.<locals>._ngrams_by_len\u001b[0;34m(seq, min_n)\u001b[0m\n",
      "\u001b[1;32m     42\u001b[0m s: Set[Tuple[Any, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, L \u001b[38;5;241m-\u001b[39m k \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;32m---> 44\u001b[0m     s\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mtuple\u001b[39m(seq[i : i \u001b[38;5;241m+\u001b[39m k]))\n",
      "\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m s:\n",
      "\u001b[1;32m     46\u001b[0m     out[k] \u001b[38;5;241m=\u001b[39m s\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = largest_common_ngram_df(\n",
    "    directory='/Volumes/BCross/datasets/author_verification/training/Wiki/known_corpus_split/',\n",
    "    n=2,\n",
    "    model=None,\n",
    "    tokenizer=None,\n",
    "    ordered_pairs=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44df5f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[(results['largest_n']>3) & (results['largest_n']<8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "773fc11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = load_model(\"/Volumes/BCross/models/Qwen 2.5/Qwen2.5-0.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2cabc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/Volumes/BCross/datasets/author_verification/training/Wiki/known_corpus_split/'\n",
    "\n",
    "text_1 = read_jsonl(f\"{base_dir}caboga_text_1.jsonl\").loc[0, 'text']\n",
    "text_2 = read_jsonl(f\"{base_dir}hardyplants_text_2.jsonl\").loc[0, 'text']\n",
    "\n",
    "common = common_ngrams(text_1, text_2, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ec8eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_tokens = common_ngrams(text_1, text_2, n=2, tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb2d91a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: {('.Ċ', 'The'),\n",
       "  ('Ġ', '1'),\n",
       "  ('Ġand', 'Ġthe'),\n",
       "  ('Ġduring', 'Ġthe'),\n",
       "  ('Ġin', 'Ġthe'),\n",
       "  ('Ġof', 'Ġa'),\n",
       "  ('Ġof', 'Ġnon'),\n",
       "  ('Ġof', 'Ġthe'),\n",
       "  ('Ġon', 'Ġthe'),\n",
       "  ('Ġthat', 'Ġis'),\n",
       "  ('Ġthat', 'Ġwas'),\n",
       "  ('Ġwithin', 'Ġthe'),\n",
       "  ('Ġyou', 'Ġare')},\n",
       " 3: {('Ġa', 'Ġnumber', 'Ġof')},\n",
       " 6: {('Ġfrom', 'Ġone', 'Ġgeneration', 'Ġto', 'Ġthe', 'Ġnext')}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e59121fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: {('and', 'the'),\n",
       "  ('during', 'the'),\n",
       "  ('if', 'you'),\n",
       "  ('in', 'the'),\n",
       "  ('of', 'a'),\n",
       "  ('of', 'non'),\n",
       "  ('of', 'the'),\n",
       "  ('on', 'the'),\n",
       "  ('that', 'is'),\n",
       "  ('that', 'was'),\n",
       "  ('within', 'the'),\n",
       "  ('you', 'are')},\n",
       " 3: {('a', 'number', 'of')},\n",
       " 6: {('from', 'one', 'generation', 'to', 'the', 'next')}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ee4c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Largest shared n = 6 (1 n-grams)\n",
      "from one generation to the next\n"
     ]
    }
   ],
   "source": [
    "def highest_common(common: Dict[int, Set[Tuple[Any, ...]]]) -> Tuple[int, Set[Tuple[Any, ...]]]:\n",
    "    \"\"\"\n",
    "    Given the dict returned by `common_ngrams`, return (max_n, ngrams_at_max).\n",
    "    If there are none, returns (0, empty set).\n",
    "    \"\"\"\n",
    "    if not common:\n",
    "        return 0, set()\n",
    "    max_k = max(common.keys())\n",
    "    return max_k, common[max_k]\n",
    "\n",
    "# Usage\n",
    "max_k, ngrams_at_max = highest_common(common)\n",
    "if max_k == 0:\n",
    "    print(\"No shared n-grams ≥ n.\")\n",
    "else:\n",
    "    print(f\"Largest shared n = {max_k} ({len(ngrams_at_max)} n-grams)\")\n",
    "    # Print them all as readable strings (optional)\n",
    "    for tup in sorted(ngrams_at_max):\n",
    "        print(\" \".join(map(str, tup)))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
