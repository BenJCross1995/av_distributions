{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29afe9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import ast\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from from_root import from_root\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure we can import from src/\n",
    "sys.path.insert(0, str(from_root(\"src\")))\n",
    "\n",
    "from read_and_write_docs import read_jsonl, read_rds, read_txt\n",
    "from model_loading import load_model, distinct_special_chars\n",
    "from utils import apply_temp_doc_id, build_metadata_df\n",
    "from n_gram_functions import (\n",
    "    common_ngrams,\n",
    "    filter_ngrams,\n",
    "    pretty_print_common_ngrams,\n",
    "    get_scored_df,\n",
    "    get_scored_df_no_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4b7eab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "base_read_loc = \"/Volumes/BCross/datasets/author_verification\"\n",
    "data_type = \"test\"\n",
    "corpus = \"Wiki\"\n",
    "\n",
    "known_loc = f\"{base_read_loc}/{data_type}/{corpus}/known_raw.jsonl\"\n",
    "unknown_loc = f\"{base_read_loc}/{data_type}/{corpus}/unknown_raw.jsonl\"\n",
    "\n",
    "metadata_loc = f\"{base_read_loc}/{data_type}/metadata.rds\"\n",
    "\n",
    "print(\"Loading data\")\n",
    "known = read_jsonl(known_loc)\n",
    "known = apply_temp_doc_id(known)\n",
    "    \n",
    "unknown = read_jsonl(unknown_loc)\n",
    "unknown = apply_temp_doc_id(unknown)\n",
    "\n",
    "# NOTE - Is this used?\n",
    "metadata = read_rds(metadata_loc)\n",
    "filtered_metadata = metadata[metadata['corpus'] == corpus]\n",
    "agg_metadata = build_metadata_df(filtered_metadata, known, unknown)\n",
    "    \n",
    "print(\"Data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224d59c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n"
     ]
    }
   ],
   "source": [
    "model_loc = \"/Volumes/BCross/models/ModernBERT/ModernBERT-base\"\n",
    "\n",
    "print(\"Loading model\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_loc)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_loc)\n",
    "special_tokens = distinct_special_chars(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bdf623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_doc_list = (\n",
    "    read_txt(\"/Volumes/BCross/datasets/author_verification/test/Wiki/known_doc_list.txt\")\n",
    "    .strip()\n",
    "    .split(\"\\n\")\n",
    ")\n",
    "unknown_doc_list = (\n",
    "    read_txt(\"/Volumes/BCross/datasets/author_verification/test/Wiki/unknown_doc_list.txt\")\n",
    "    .strip()\n",
    "    .split(\"\\n\")\n",
    ")\n",
    "\n",
    "known_doc = known_doc_list[0]\n",
    "unknown_doc = unknown_doc_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96fd6f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----\n",
    "# Get the chosen text & metadata\n",
    "# -----\n",
    "\n",
    "known_text = known[known['doc_id'] == known_doc].reset_index().loc[0, 'text'].lower()\n",
    "unknown_text = unknown[unknown['doc_id'] == unknown_doc].reset_index().loc[0, 'text'].lower()\n",
    "    \n",
    "problem_metadata = agg_metadata[(agg_metadata['known_doc_id'] == known_doc)\n",
    "                                & (agg_metadata['unknown_doc_id'] == unknown_doc)].reset_index()\n",
    "problem_metadata['target'] = problem_metadata['known_author'] == problem_metadata['unknown_author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4936437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----\n",
    "# Create document dataframe\n",
    "# -----\n",
    "    \n",
    "# This is used to display the text\n",
    "docs_df = pd.DataFrame(\n",
    "{\n",
    "    \"known\":   [corpus, data_type, known_doc, known_text],\n",
    "    \"unknown\": [corpus, data_type, unknown_doc, unknown_text],\n",
    "},\n",
    "index=[\"corpus\", \"data type\", \"doc\", \"text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92c6c944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting common n-grams\n",
      "There are 12 n-grams in common!\n"
     ]
    }
   ],
   "source": [
    "# -----\n",
    "# Get common n-grams\n",
    "# -----\n",
    "    \n",
    "print(\"Getting common n-grams\")\n",
    "common = common_ngrams(known_text, unknown_text, n=2, model=model, tokenizer=tokenizer, lowercase=True)\n",
    "    \n",
    "# Filter to remove smaller n-grams which don't satisfy the rules\n",
    "common = filter_ngrams(common, special_tokens=special_tokens)\n",
    "n_gram_list = pretty_print_common_ngrams(common, tokenizer=tokenizer, return_format='flat', show_raw=True)\n",
    "print(f\"There are {len(n_gram_list)} n-grams in common!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8e71d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: {('Ġabout', 'Ġthis'),\n",
       "  ('Ġarticles', 'Ġon'),\n",
       "  ('Ġbecause', 'Ġthe'),\n",
       "  ('Ġthe', 'Ġsubject'),\n",
       "  ('Ġthis', 'Ġarticle')},\n",
       " 3: {(',', 'Ġbut', 'Ġthis'),\n",
       "  (',', 'Ġyou', 'Ġare'),\n",
       "  ('Ġdo', 'Ġnot', 'Ġhave'),\n",
       "  ('Ġone', 'Ġof', 'Ġthe'),\n",
       "  ('Ġwelcome', 'Ġto', 'Ġimprove'),\n",
       "  ('Ġyou', 'Ġdo', 'Ġnot')},\n",
       " 4: {(',', 'Ġthis', 'Ġis', 'Ġnot')}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6f11d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(', but this', \"(',', 'Ġbut', 'Ġthis')\"),\n",
       " (', you are', \"(',', 'Ġyou', 'Ġare')\"),\n",
       " (' do not have', \"('Ġdo', 'Ġnot', 'Ġhave')\"),\n",
       " (' one of the', \"('Ġone', 'Ġof', 'Ġthe')\"),\n",
       " (' welcome to improve', \"('Ġwelcome', 'Ġto', 'Ġimprove')\"),\n",
       " (' you do not', \"('Ġyou', 'Ġdo', 'Ġnot')\"),\n",
       " (' about this', \"('Ġabout', 'Ġthis')\"),\n",
       " (' articles on', \"('Ġarticles', 'Ġon')\"),\n",
       " (' because the', \"('Ġbecause', 'Ġthe')\"),\n",
       " (' the subject', \"('Ġthe', 'Ġsubject')\"),\n",
       " (' this article', \"('Ġthis', 'Ġarticle')\"),\n",
       " (', this is not', \"(',', 'Ġthis', 'Ġis', 'Ġnot')\")]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6c265a",
   "metadata": {},
   "source": [
    "## Phrase Masking Funcitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7a09143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "def mask_phrase(\n",
    "    text: str,\n",
    "    phrase: str,\n",
    "    mask_token: str = \"[MASK]\"\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    For each occurrence of `phrase` in `text`, create a version of\n",
    "    the text where ONLY that occurrence is replaced with `mask_token`.\n",
    "\n",
    "    Returns:\n",
    "        List of masked texts.\n",
    "        - If the phrase appears N times, the list has length N.\n",
    "        - If it appears 0 times, the list is empty.\n",
    "    \"\"\"\n",
    "    if not phrase:\n",
    "        raise ValueError(\"phrase must be a non-empty string\")\n",
    "\n",
    "    pattern = re.escape(phrase)\n",
    "    matches = list(re.finditer(pattern, text))\n",
    "\n",
    "    masked_variants: List[str] = []\n",
    "\n",
    "    for m in matches:\n",
    "        start, end = m.span()\n",
    "        masked_text = text[:start] + mask_token + text[end:]\n",
    "        masked_variants.append(masked_text)\n",
    "\n",
    "    return masked_variants\n",
    "\n",
    "def mask_phrase_by_tokens(\n",
    "    text: str,\n",
    "    phrase_tokens: List[str],\n",
    "    tokenizer: tokenizer,\n",
    "    mask_token: str | None = '[MASK]',\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    For each occurrence of `phrase_tokens` (a token sequence) in the tokenized\n",
    "    version of `text`, create a version of the text where ONLY that occurrence\n",
    "    is replaced by mask tokens (one [MASK] per token in the phrase).\n",
    "\n",
    "    Args:\n",
    "        text: Original text.\n",
    "        phrase_tokens: List of token strings for the phrase (e.g., tokenizer.tokenize(phrase)).\n",
    "        tokenizer: HF tokenizer used to tokenize and detokenize.\n",
    "        mask_token: Mask token string to use. Defaults to tokenizer.mask_token.\n",
    "\n",
    "    Returns:\n",
    "        List of masked texts:\n",
    "        - If the phrase appears N times (as a token subsequence), the list has length N.\n",
    "        - If it appears 0 times, the list is empty.\n",
    "    \"\"\"\n",
    "    if not phrase_tokens:\n",
    "        raise ValueError(\"phrase_tokens must be a non-empty list of tokens\")\n",
    "\n",
    "    if mask_token is None:\n",
    "        if tokenizer.mask_token is None:\n",
    "            raise ValueError(\"mask_token not provided and tokenizer.mask_token is None\")\n",
    "        mask_token = tokenizer.mask_token\n",
    "\n",
    "    # Tokenize text without special tokens\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    n = len(phrase_tokens)\n",
    "\n",
    "    masked_variants: List[str] = []\n",
    "\n",
    "    # Find all subsequence matches of phrase_tokens in tokens\n",
    "    for start_idx in range(len(tokens) - n + 1):\n",
    "        if tokens[start_idx:start_idx + n] == phrase_tokens:\n",
    "            # Create a masked copy for this particular occurrence\n",
    "            masked_tokens = tokens.copy()\n",
    "            for j in range(n):\n",
    "                masked_tokens[start_idx + j] = mask_token\n",
    "\n",
    "            masked_text = tokenizer.convert_tokens_to_string(masked_tokens)\n",
    "            masked_variants.append(masked_text)\n",
    "\n",
    "    return masked_variants\n",
    "\n",
    "from typing import List, Dict, Optional\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "\n",
    "def mask_phrase(\n",
    "    text: str,\n",
    "    phrase_tokens: List[str],\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    mask_token: Optional[str] = \"[MASK]\",\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    For each occurrence of `phrase_tokens` (a token sequence) in the tokenized\n",
    "    version of `text`, create two versions of the text:\n",
    "\n",
    "    1. single_mask: the whole phrase replaced by a **single** mask_token.\n",
    "    2. multi_mask: each token in the phrase replaced by its own mask_token.\n",
    "\n",
    "    Returns a dict:\n",
    "        {\n",
    "            \"single_mask\": [ ... ],\n",
    "            \"multi_mask\":  [ ... ],\n",
    "        }\n",
    "    where each list has length == number of occurrences (can be 0).\n",
    "    \"\"\"\n",
    "    if not phrase_tokens:\n",
    "        raise ValueError(\"phrase_tokens must be a non-empty list of tokens\")\n",
    "\n",
    "    if mask_token is None:\n",
    "        if tokenizer.mask_token is None:\n",
    "            raise ValueError(\"mask_token not provided and tokenizer.mask_token is None\")\n",
    "        mask_token = tokenizer.mask_token\n",
    "\n",
    "    # Tokenize the text (no special tokens)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    n = len(phrase_tokens)\n",
    "\n",
    "    single_mask_variants: List[str] = []\n",
    "    multi_mask_variants: List[str] = []\n",
    "\n",
    "    # Find all subsequence matches of phrase_tokens in tokens\n",
    "    for start_idx in range(len(tokens) - n + 1):\n",
    "        if tokens[start_idx:start_idx + n] == phrase_tokens:\n",
    "            # --- multi-mask: replace each token in the phrase by mask_token ---\n",
    "            multi_tokens = tokens.copy()\n",
    "            for j in range(n):\n",
    "                multi_tokens[start_idx + j] = mask_token\n",
    "            multi_text = tokenizer.convert_tokens_to_string(multi_tokens)\n",
    "            multi_mask_variants.append(multi_text)\n",
    "\n",
    "            # --- single-mask: collapse the whole phrase into a single mask_token ---\n",
    "            single_tokens = tokens[:start_idx] + [mask_token] + tokens[start_idx + n:]\n",
    "            single_text = tokenizer.convert_tokens_to_string(single_tokens)\n",
    "            single_mask_variants.append(single_text)\n",
    "\n",
    "    return {\n",
    "        \"single_mask\": single_mask_variants,\n",
    "        \"multi_mask\": multi_mask_variants,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaf307d",
   "metadata": {},
   "source": [
    "### Paraphrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "24dde3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1 — Joint multi-mask fill with beam search (ModernBERT or any MLM)\n",
    "import math\n",
    "from typing import List, Dict, Tuple, Optional, Iterable\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerBase, PreTrainedModel, AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "@torch.no_grad()\n",
    "def fill_masks_beam(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    masked_text: str,\n",
    "    top_k_per_mask: int = 25,   # widen/limit local options at each mask\n",
    "    beam_size: int = 100,       # how many partial hypotheses to keep\n",
    "    max_candidates: int = 50,   # truncate final list\n",
    "    banned_token_ids: Optional[Iterable[int]] = None,\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Jointly fill one or more [MASK] tokens using a simple beam search.\n",
    "\n",
    "    Returns a list of dicts: {text, score, tokens, token_ids, mask_positions}\n",
    "    where 'score' is the sum of log-probs across all masks (higher is better).\n",
    "    \"\"\"\n",
    "    if tokenizer.mask_token_id is None:\n",
    "        raise ValueError(\"Tokenizer must define mask_token_id (MLM required).\")\n",
    "\n",
    "    enc = tokenizer(masked_text, return_tensors=\"pt\", add_special_tokens=True, truncation=True)\n",
    "    input_ids = enc[\"input_ids\"]\n",
    "    mask_id = tokenizer.mask_token_id\n",
    "    mask_positions = (input_ids[0] == mask_id).nonzero(as_tuple=False).flatten().tolist()\n",
    "    if not mask_positions:\n",
    "        return []\n",
    "\n",
    "    outputs = model(**enc)\n",
    "    log_probs = outputs.logits.log_softmax(-1)  # [1, seq_len, vocab]\n",
    "\n",
    "    specials = set(getattr(tokenizer, \"all_special_ids\", []) or [])\n",
    "    banned = set(banned_token_ids or []) | specials\n",
    "\n",
    "    # Precompute top-k candidates at each mask position\n",
    "    per_mask_cands = []\n",
    "    for pos in mask_positions:\n",
    "        lp = log_probs[0, pos].clone()\n",
    "        if banned:\n",
    "            idx = torch.tensor(sorted(banned), dtype=torch.long)\n",
    "            lp.index_fill_(0, idx, float(\"-inf\"))\n",
    "        topk = torch.topk(lp, k=min(top_k_per_mask, lp.numel()))\n",
    "        per_mask_cands.append((topk.indices.tolist(), topk.values.tolist()))\n",
    "\n",
    "    # Beam over masks (left-to-right in token order)\n",
    "    beam = [(0.0, [])]  # (cum_logprob, chosen_token_ids_so_far)\n",
    "    for cand_ids, cand_lps in per_mask_cands:\n",
    "        new_beam = []\n",
    "        for cum_lp, chosen in beam:\n",
    "            for tid, lp in zip(cand_ids, cand_lps):\n",
    "                new_beam.append((cum_lp + float(lp), chosen + [tid]))\n",
    "        new_beam.sort(key=lambda x: x[0], reverse=True)\n",
    "        beam = new_beam[:beam_size]\n",
    "\n",
    "    # Materialize and deduplicate by decoded text\n",
    "    best = {}\n",
    "    for cum_lp, choice_ids in beam:\n",
    "        filled = input_ids.clone()\n",
    "        for pos, tid in zip(mask_positions, choice_ids):\n",
    "            filled[0, pos] = tid\n",
    "        text_out = tokenizer.decode(filled[0], skip_special_tokens=True)\n",
    "        prev = best.get(text_out)\n",
    "        if (prev is None) or (cum_lp > prev[\"score\"]):\n",
    "            best[text_out] = {\n",
    "                \"text\": text_out,\n",
    "                \"score\": cum_lp,\n",
    "                \"tokens\": tokenizer.convert_ids_to_tokens(choice_ids),\n",
    "                \"token_ids\": choice_ids,\n",
    "                \"mask_positions\": mask_positions,\n",
    "            }\n",
    "    return sorted(best.values(), key=lambda r: r[\"score\"], reverse=True)[:max_candidates]\n",
    "\n",
    "# C1 — Try multiple lengths at one masked span and rank globally\n",
    "from typing import Iterable, List, Dict\n",
    "\n",
    "def variable_length_infill(\n",
    "    model, tokenizer,\n",
    "    masked_template: str,              # contains ONE [MASK] span\n",
    "    length_options: Iterable[int] = (1, 2, 3, 4),\n",
    "    per_length_topk: int = 10,\n",
    "    normalize_by_masks: bool = True,   # de-bias longer spans\n",
    "    **beam_kwargs\n",
    ") -> List[Dict]:\n",
    "    mask_tok = tokenizer.mask_token\n",
    "    assert mask_tok in masked_template, \"Template must contain one [MASK] span.\"\n",
    "    rows = []\n",
    "    for L in length_options:\n",
    "        expanded = masked_template.replace(mask_tok, \" \".join([mask_tok]*L), 1)\n",
    "        outs = fill_masks_beam(model, tokenizer, expanded, **beam_kwargs)\n",
    "        for o in outs[:per_length_topk]:\n",
    "            score = o[\"score\"] / L if normalize_by_masks else o[\"score\"]\n",
    "            rows.append({\"text\": o[\"text\"], \"length\": L, \"score\": score, \"raw_score\": o[\"score\"], \"tokens\": o[\"tokens\"]})\n",
    "    return sorted(rows, key=lambda r: r[\"score\"], reverse=True)\n",
    "\n",
    "from typing import List, Dict, Optional\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "\n",
    "def beam_outputs_to_phrases(\n",
    "    outputs: List[Dict],\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    original_phrase: Optional[str] = None,\n",
    "    lowercase: bool = True,\n",
    "    unique: bool = True,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Convert MLM beam outputs (from fill_masks_beam / variable_length_infill)\n",
    "    into a list of phrases, similar to parse_paraphrases.\n",
    "\n",
    "    Assumes that `o[\"tokens\"]` are the tokens for the masked span\n",
    "    (which is true for variable_length_infill, and for single-span masks).\n",
    "    \"\"\"\n",
    "    phrases: List[str] = []\n",
    "\n",
    "    # Prepare comparison baseline (like parse_paraphrases)\n",
    "    if original_phrase is not None and lowercase:\n",
    "        original_cmp = original_phrase.lower()\n",
    "    else:\n",
    "        original_cmp = original_phrase\n",
    "\n",
    "    for o in outputs:\n",
    "        # Decode just the predicted tokens for the masked span\n",
    "        candidate = tokenizer.convert_tokens_to_string(o[\"tokens\"])\n",
    "\n",
    "        # Compare and optionally lowercase\n",
    "        cand_cmp = candidate.lower() if lowercase else candidate\n",
    "\n",
    "        # Drop suggestions that are the same as the original phrase\n",
    "        if original_cmp is not None and cand_cmp == original_cmp:\n",
    "            continue\n",
    "\n",
    "        phrases.append(cand_cmp if lowercase else candidate)\n",
    "\n",
    "    # Deduplicate (like your set(), but preserve order)\n",
    "    if unique:\n",
    "        seen = set()\n",
    "        deduped = []\n",
    "        for p in phrases:\n",
    "            if p not in seen:\n",
    "                seen.add(p)\n",
    "                deduped.append(p)\n",
    "        return deduped\n",
    "\n",
    "    return phrases\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def fill_masks_beam_phrases(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    masked_text: str,\n",
    "    original_phrase: Optional[str] = None,\n",
    "    lowercase: bool = True,\n",
    "    top_k_per_mask: int = 25,\n",
    "    beam_size: int = 100,\n",
    "    max_candidates: int = 50,\n",
    "    banned_token_ids: Optional[Iterable[int]] = None,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Convenience wrapper around fill_masks_beam that returns only the\n",
    "    predicted phrases (like parse_paraphrases).\n",
    "    \"\"\"\n",
    "    beam_outputs = fill_masks_beam(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        masked_text=masked_text,\n",
    "        top_k_per_mask=top_k_per_mask,\n",
    "        beam_size=beam_size,\n",
    "        max_candidates=max_candidates,\n",
    "        banned_token_ids=banned_token_ids,\n",
    "    )\n",
    "\n",
    "    return beam_outputs_to_phrases(\n",
    "        outputs=beam_outputs,\n",
    "        tokenizer=tokenizer,\n",
    "        original_phrase=original_phrase,\n",
    "        lowercase=lowercase,\n",
    "        unique=True,\n",
    "    )\n",
    "\n",
    "def variable_length_infill_phrases(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    masked_template: str,               # contains ONE [MASK] span\n",
    "    original_phrase: Optional[str] = None,\n",
    "    lowercase: bool = True,\n",
    "    length_options: Iterable[int] = (1, 2, 3, 4),\n",
    "    per_length_topk: int = 10,\n",
    "    normalize_by_masks: bool = True,\n",
    "    **beam_kwargs,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Run variable_length_infill and return only the span phrases as a list,\n",
    "    similar to parse_paraphrases.\n",
    "    \"\"\"\n",
    "\n",
    "    rows = variable_length_infill(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        masked_template=masked_template,\n",
    "        length_options=length_options,\n",
    "        per_length_topk=per_length_topk,\n",
    "        normalize_by_masks=normalize_by_masks,\n",
    "        **beam_kwargs,\n",
    "    )\n",
    "    # rows are dicts with at least: {\"text\", \"length\", \"score\", \"raw_score\", \"tokens\"}\n",
    "\n",
    "    return beam_outputs_to_phrases(\n",
    "        outputs=rows,        # compatible: each row has \"tokens\"\n",
    "        tokenizer=tokenizer,\n",
    "        original_phrase=original_phrase,\n",
    "        lowercase=lowercase,\n",
    "        unique=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d4970afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_dict = {}\n",
    "width = len(str(len(n_gram_list)))  # e.g., 10 -> 2, 100 -> 3\n",
    "\n",
    "for idx, (phrase_pretty, phrase_raw) in enumerate(n_gram_list, start=1):\n",
    "    phrase_list = list(ast.literal_eval(phrase_raw))\n",
    "    \n",
    "    # Mask the phrase in the text\n",
    "    masked_data = mask_phrase(known_text, phrase_list, tokenizer, \"[MASK]\")\n",
    "    string_based_masked_list = masked_data['single_mask']\n",
    "    token_based_masked_list = masked_data['multi_mask']\n",
    "    \n",
    "    paraphrases = []\n",
    "    for i in range(0, len(string_based_masked_list)):\n",
    "        \n",
    "        token_paraphrases = fill_masks_beam_phrases(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            masked_text=token_based_masked_list[i],\n",
    "            original_phrase=phrase_pretty,\n",
    "            lowercase=True,\n",
    "            top_k_per_mask=25,\n",
    "            beam_size=100,\n",
    "            max_candidates=50\n",
    "        )\n",
    "        \n",
    "        paraphrases.extend(token_paraphrases)\n",
    "        \n",
    "        string_paraphrases = variable_length_infill_phrases(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            masked_template=string_based_masked_list[i],\n",
    "            original_phrase = phrase_pretty,\n",
    "            lowercase= True,\n",
    "            length_options=tuple(range(2, len(phrase_list) + 1)),\n",
    "            per_length_topk=10,\n",
    "            normalize_by_masks=True\n",
    "        )\n",
    "        \n",
    "        paraphrases.extend(string_paraphrases)\n",
    "        \n",
    "    paraphrases = list(set(paraphrases))\n",
    "    key = f\"phrase_{idx:0{width}d}\"  # -> phrase_01, phrase_002, etc.\n",
    "    n_gram_dict[key] = {\"phrase\": phrase_pretty, \"paraphrases\": paraphrases}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d7d26720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phrase_01': {'phrase': ', but this',\n",
       "  'paraphrases': [' together it',\n",
       "   ',\\nthe',\n",
       "   ', it',\n",
       "   ' together\\n it',\n",
       "   '. it',\n",
       "   ' together the it',\n",
       "   ', thethis',\n",
       "   ' in but it',\n",
       "   ' because\\nthis',\n",
       "   ',\\nhis',\n",
       "   ', the the',\n",
       "   ' with\\n it',\n",
       "   ' together but it',\n",
       "   ' into but it',\n",
       "   '.\\n this',\n",
       "   ' with this it',\n",
       "   ' into it',\n",
       "   '. the this',\n",
       "   ' together this it',\n",
       "   ' but it',\n",
       "   ' because\\n this',\n",
       "   '.\\n the',\n",
       "   ', butthis',\n",
       "   ', becausethe',\n",
       "   ',\\n this',\n",
       "   ', the',\n",
       "   ' and this',\n",
       "   ' but this',\n",
       "   ' book\\n this',\n",
       "   ' because this this',\n",
       "   ' into the it',\n",
       "   '. because this',\n",
       "   ',\\nthis',\n",
       "   ', this this',\n",
       "   '. this this',\n",
       "   '. but that',\n",
       "   ', this',\n",
       "   ', thatthis',\n",
       "   ', this it',\n",
       "   ' in the it',\n",
       "   ' into that it',\n",
       "   ' in it',\n",
       "   ', the this',\n",
       "   ' with this that',\n",
       "   ' with it it',\n",
       "   ' with but that',\n",
       "   ', but it',\n",
       "   ' with something it',\n",
       "   ' into because it',\n",
       "   ' with that it',\n",
       "   '.\\n it',\n",
       "   ' because the',\n",
       "   ', thisthis',\n",
       "   ',, it',\n",
       "   ' into something it',\n",
       "   ' because the the',\n",
       "   ', that the',\n",
       "   '.\\nthe',\n",
       "   ' with the that',\n",
       "   ', this the',\n",
       "   ' and it',\n",
       "   ' with it',\n",
       "   ' into but that',\n",
       "   ', thethe',\n",
       "   ' because it',\n",
       "   ', because the',\n",
       "   ',\\n that',\n",
       "   ' with\\n that',\n",
       "   ' because because this',\n",
       "   ', the it',\n",
       "   ' into this it',\n",
       "   ' into this that',\n",
       "   ', but that',\n",
       "   ', the that',\n",
       "   ', and this',\n",
       "   ' because this',\n",
       "   '. but it',\n",
       "   ',\\n other',\n",
       "   '. thethis',\n",
       "   ' because thethis',\n",
       "   ', that',\n",
       "   '. because the',\n",
       "   ' because\\n the',\n",
       "   ', that this',\n",
       "   '. the it',\n",
       "   '.\\nthis',\n",
       "   ', becausethis',\n",
       "   ' into\\n that',\n",
       "   ' and the',\n",
       "   '. that it',\n",
       "   ' with but it',\n",
       "   ' and\\n this',\n",
       "   ', this that',\n",
       "   ' because\\nthe',\n",
       "   ' into, it',\n",
       "   ', but the',\n",
       "   ' that the',\n",
       "   ' into it it',\n",
       "   ', that it',\n",
       "   ' because becausethis',\n",
       "   ', it it',\n",
       "   ' with because it',\n",
       "   ' with, it',\n",
       "   ' into\\n it',\n",
       "   ' that this',\n",
       "   '. this it',\n",
       "   ' with the it',\n",
       "   '\\n\\n this',\n",
       "   ',\\n it',\n",
       "   ' into the that',\n",
       "   ' into but this',\n",
       "   '. this',\n",
       "   ' because because the',\n",
       "   ' because the this',\n",
       "   '. the the',\n",
       "   ', because this',\n",
       "   '. becausethis',\n",
       "   ',\\n the']},\n",
       " 'phrase_02': {'phrase': ', you are',\n",
       "  'paraphrases': [' anybody would be',\n",
       "   ' you you is',\n",
       "   ' you would be',\n",
       "   ' you you are',\n",
       "   ', will are',\n",
       "   ' that would be',\n",
       "   \",'d be\",\n",
       "   ' it you be',\n",
       "   ' you be',\n",
       "   ' you you,',\n",
       "   ', would are',\n",
       "   ' you are be',\n",
       "   ' people would be',\n",
       "   ' you,',\n",
       "   ', are be',\n",
       "   ' we are',\n",
       "   ', will be',\n",
       "   ', you be',\n",
       "   \" you'd are\",\n",
       "   ', will,',\n",
       "   ' you would are',\n",
       "   ' they would be',\n",
       "   ' anyone would be',\n",
       "   ' anyone would are',\n",
       "   ', are',\n",
       "   ' you would is',\n",
       "   ' i would be',\n",
       "   ' i you be',\n",
       "   ' the would be',\n",
       "   ' you you be',\n",
       "   ' it would be',\n",
       "   \" you would're\",\n",
       "   ' you would,',\n",
       "   ' you will are',\n",
       "   ' you you very',\n",
       "   ', you,',\n",
       "   ' you would very',\n",
       "   \" you'd be\",\n",
       "   ' it would are',\n",
       "   ', would is',\n",
       "   ' you people be',\n",
       "   ' we would be',\n",
       "   ' you is',\n",
       "   ' you would',\n",
       "   ', would be',\n",
       "   ' you will be',\n",
       "   ' you would all',\n",
       "   ' i would are',\n",
       "   ' you are are',\n",
       "   ' you will,',\n",
       "   \", would're\",\n",
       "   \" you're\",\n",
       "   ', you is',\n",
       "   ' it are',\n",
       "   ', would very',\n",
       "   ' you are',\n",
       "   ', you very',\n",
       "   ', would,',\n",
       "   ' i are']},\n",
       " 'phrase_03': {'phrase': ' do not have',\n",
       "  'paraphrases': [\" dont't have\",\n",
       "   \" don't of\",\n",
       "   ' haven not have',\n",
       "   \" don't little\",\n",
       "   \" don't much\",\n",
       "   \" have't much\",\n",
       "   \" just't have\",\n",
       "   ' have not have',\n",
       "   ' don a have',\n",
       "   \" do't of\",\n",
       "   ' have the',\n",
       "   ' cannot no',\n",
       "   ' have not much',\n",
       "   \" don't enough\",\n",
       "   \" can't have\",\n",
       "   \" don't limited\",\n",
       "   \" have't\",\n",
       "   ' have some',\n",
       "   ' have limited',\n",
       "   ' have have have',\n",
       "   ' do have have',\n",
       "   \" don't the\",\n",
       "   \" do't enough\",\n",
       "   \" don't no\",\n",
       "   ' don have much',\n",
       "   \" am't have\",\n",
       "   ' have not',\n",
       "   ' don not the',\n",
       "   ' don not enough',\n",
       "   ' don out have',\n",
       "   \" don't had\",\n",
       "   ' have have',\n",
       "   \" don't get\",\n",
       "   ' don got have',\n",
       "   ' don no have',\n",
       "   \" have't the\",\n",
       "   \" have't of\",\n",
       "   \" have't have\",\n",
       "   ' have little',\n",
       "   ' don not of',\n",
       "   \" haven't have\",\n",
       "   ' am not have',\n",
       "   \" do't much\",\n",
       "   \" have't enough\",\n",
       "   ' don very have',\n",
       "   \"'m't have\",\n",
       "   ' don plenty have',\n",
       "   ' don have have',\n",
       "   \" cannot't have\",\n",
       "   ' dont not have',\n",
       "   \"'m not have\",\n",
       "   ' dont no',\n",
       "   ' don not no',\n",
       "   ' do not much',\n",
       "   ' have no',\n",
       "   \" don't have\",\n",
       "   ' don not much',\n",
       "   ' don not have',\n",
       "   \" do't have\"]},\n",
       " 'phrase_04': {'phrase': ' one of the',\n",
       "  'paraphrases': [' among the',\n",
       "   ' one of three',\n",
       "   ' one of five',\n",
       "   ' one and the',\n",
       "   ' one of primary',\n",
       "   ' also of the',\n",
       "   ' one of 2',\n",
       "   ' of the',\n",
       "   ' one of those',\n",
       "   ' one of important',\n",
       "   ' one of his',\n",
       "   ' one of some',\n",
       "   ' among of the',\n",
       "   ' one of my',\n",
       "   ' a of the',\n",
       "   ' one of 3',\n",
       "   ' among of',\n",
       "   ' 1 of the',\n",
       "   ' one of two',\n",
       "   ' one of major',\n",
       "   ' one of four',\n",
       "   ' it of the',\n",
       "   ' one of few',\n",
       "   ' one, the',\n",
       "   ' the of the',\n",
       "   ' one the',\n",
       "   ' part of the',\n",
       "   ' the of',\n",
       "   ' exactly of the',\n",
       "   ' one of of',\n",
       "   ' one of other',\n",
       "   ' certainly of the',\n",
       "   ' one number the',\n",
       "   ' not of the',\n",
       "   ' an of the',\n",
       "   ' one of her',\n",
       "   ' one of more',\n",
       "   ' her of the',\n",
       "   ' one of biggest',\n",
       "   ' one very the',\n",
       "   ' the the',\n",
       "   ' just of the',\n",
       "   ' one of only',\n",
       "   ' one of many',\n",
       "   ' one of several',\n",
       "   ' one the the',\n",
       "   ' one one the',\n",
       "   ' one of big',\n",
       "   ' one most the',\n",
       "   ' one main',\n",
       "   ' probably of the',\n",
       "   ' one of main',\n",
       "   ' another of the',\n",
       "   ' she of the',\n",
       "   ' on of the',\n",
       "   ' the main',\n",
       "   ' a the',\n",
       "   ' one among the',\n",
       "   ' one of']},\n",
       " 'phrase_05': {'phrase': ' welcome to improve',\n",
       "  'paraphrases': [' welcome to review',\n",
       "   ' welcome to do',\n",
       "   ' much to read',\n",
       "   ' welcome to of',\n",
       "   ' interested to find',\n",
       "   ' important to read',\n",
       "   ' welcome in read',\n",
       "   ' likely to reading',\n",
       "   ' likely to read',\n",
       "   ' interested to',\n",
       "   ' welcome to study',\n",
       "   ' good to read',\n",
       "   ' much to',\n",
       "   ' free to read',\n",
       "   ' welcome to understand',\n",
       "   ' interested to see',\n",
       "   ' well to read',\n",
       "   ' well to reading',\n",
       "   ' welcome to see',\n",
       "   ' welcome to discuss',\n",
       "   ' welcome to in',\n",
       "   ' welcome to follow',\n",
       "   ' welcome to cite',\n",
       "   ' interested with',\n",
       "   ' welcome to',\n",
       "   ' welcome to check',\n",
       "   ' capable to read',\n",
       "   ' welcome interested read',\n",
       "   ' interested to read',\n",
       "   ' happy to read',\n",
       "   ' welcome to find',\n",
       "   ' welcome to write',\n",
       "   ' welcome to get',\n",
       "   ' able to reading',\n",
       "   ' welcome in',\n",
       "   ' interested in',\n",
       "   ' able to read',\n",
       "   ' welcome to publish',\n",
       "   ' welcome to reference',\n",
       "   ' interested about',\n",
       "   ' encouraged to read',\n",
       "   ' welcome to use',\n",
       "   ' interested to reading',\n",
       "   ' welcome in reading',\n",
       "   ' welcome to access',\n",
       "   ' welcome to with',\n",
       "   ' welcome to reading',\n",
       "   ' well to see',\n",
       "   ' welcome to to',\n",
       "   ' welcome to view',\n",
       "   ' welcome to research',\n",
       "   ' welcome to read',\n",
       "   ' well to find',\n",
       "   ' good to reading',\n",
       "   ' very to read',\n",
       "   ' welcome read',\n",
       "   ' useful to read',\n",
       "   ' interested reading',\n",
       "   ' interesting to read',\n",
       "   ' interested read']},\n",
       " 'phrase_06': {'phrase': ' you do not',\n",
       "  'paraphrases': [', can someone',\n",
       "   ' that did you',\n",
       "   ', can i',\n",
       "   ' that can someone',\n",
       "   ', could you',\n",
       "   ', can please',\n",
       "   ' you do you',\n",
       "   ', you i',\n",
       "   ', can not',\n",
       "   ', will you',\n",
       "   ' you can',\n",
       "   ' you can to',\n",
       "   ' that can i',\n",
       "   ' that you you',\n",
       "   ' that can can',\n",
       "   ' you could',\n",
       "   ' that can you',\n",
       "   ', i you',\n",
       "   ' that can to',\n",
       "   ' you could you',\n",
       "   ' you can can',\n",
       "   ', would you',\n",
       "   ' that can would',\n",
       "   ', can to',\n",
       "   ', can can',\n",
       "   ' that can please',\n",
       "   ' you can not',\n",
       "   ' you i',\n",
       "   ', can would',\n",
       "   ' you you you',\n",
       "   ' that i you',\n",
       "   ' that can',\n",
       "   ' you will',\n",
       "   ' that will you',\n",
       "   ' that you i',\n",
       "   ' it can you',\n",
       "   ' you can you',\n",
       "   ' you would you',\n",
       "   ' you will you',\n",
       "   ', do you',\n",
       "   ', can you',\n",
       "   ' that could you',\n",
       "   ' that can not',\n",
       "   \" that can't\",\n",
       "   ' you can please',\n",
       "   ' i can you',\n",
       "   ' you can would',\n",
       "   ' you would',\n",
       "   \" you can't\",\n",
       "   ' that you not',\n",
       "   ' that you',\n",
       "   ' that do you',\n",
       "   ' i you',\n",
       "   ' you you',\n",
       "   ' you can i',\n",
       "   ', you you',\n",
       "   \", can't\",\n",
       "   ' that would you',\n",
       "   ' that you to',\n",
       "   ' i can']},\n",
       " 'phrase_07': {'phrase': ' about this',\n",
       "  'paraphrases': [' about sad',\n",
       "   ' of the',\n",
       "   ' about their',\n",
       "   ' this the',\n",
       "   ' that the',\n",
       "   ' about first',\n",
       "   ' about the',\n",
       "   ' about particular',\n",
       "   ' in the',\n",
       "   ' the one',\n",
       "   ' on this',\n",
       "   ' about a',\n",
       "   ' about interesting',\n",
       "   ' about court',\n",
       "   ' that this',\n",
       "   ' of this',\n",
       "   ' the this',\n",
       "   ' on the',\n",
       "   ' this this',\n",
       "   ' about important',\n",
       "   ' about last',\n",
       "   ' about actual',\n",
       "   ' about my',\n",
       "   ' one the',\n",
       "   ' about another',\n",
       "   ' about one',\n",
       "   ' what this',\n",
       "   ' the another',\n",
       "   ' about your',\n",
       "   ', this',\n",
       "   ' one this',\n",
       "   ' a one',\n",
       "   ' about his',\n",
       "   ' about our',\n",
       "   ' about second',\n",
       "   ' in this',\n",
       "   ' the the',\n",
       "   ' about other',\n",
       "   ' about real',\n",
       "   ' about latest',\n",
       "   ' about famous',\n",
       "   ' a another',\n",
       "   ' about that',\n",
       "   ' what the',\n",
       "   ' an this',\n",
       "   ' a this',\n",
       "   ' a the',\n",
       "   ' about specific',\n",
       "   ' a a']},\n",
       " 'phrase_08': {'phrase': ' articles on',\n",
       "  'paraphrases': [' articles of',\n",
       "   ' parts of',\n",
       "   ' editions on',\n",
       "   ' books on',\n",
       "   ' pages on',\n",
       "   ' pages about',\n",
       "   ' content on',\n",
       "   ' information on',\n",
       "   ' versions on',\n",
       "   ' questions on',\n",
       "   ' page on',\n",
       "   ' books of',\n",
       "   ' sources on',\n",
       "   ' version about',\n",
       "   ' book about',\n",
       "   ' to of',\n",
       "   ' sources about',\n",
       "   ' version on',\n",
       "   ' articles about',\n",
       "   ' sections of',\n",
       "   ' aspects about',\n",
       "   ' sources of',\n",
       "   ' page of',\n",
       "   ' version of',\n",
       "   ' aspects on',\n",
       "   ' versions of',\n",
       "   ' parts about',\n",
       "   ' book on',\n",
       "   ' information about',\n",
       "   ' section of',\n",
       "   ' article of',\n",
       "   ' part on',\n",
       "   ' parts on',\n",
       "   ' section on',\n",
       "   ' information of',\n",
       "   ' editions of',\n",
       "   ' sections on',\n",
       "   ' versions about',\n",
       "   ' questions of',\n",
       "   ' article on',\n",
       "   ' part of',\n",
       "   ' and of',\n",
       "   ' pages of',\n",
       "   ' book of',\n",
       "   ' and on',\n",
       "   ' content of',\n",
       "   ' books about',\n",
       "   ' aspects of',\n",
       "   ' to on']},\n",
       " 'phrase_09': {'phrase': ' because the',\n",
       "  'paraphrases': [' for this',\n",
       "   '. that',\n",
       "   ' because that',\n",
       "   '. his',\n",
       "   ' saying her',\n",
       "   ' and that',\n",
       "   ' that the',\n",
       "   \"'s the\",\n",
       "   ' because its',\n",
       "   ', the',\n",
       "   ' that a',\n",
       "   ' and their',\n",
       "   ' and this',\n",
       "   ' saying every',\n",
       "   ' because her',\n",
       "   ' saying this',\n",
       "   ' that this',\n",
       "   ' because a',\n",
       "   ' that their',\n",
       "   ' for the',\n",
       "   ' claiming the',\n",
       "   ', this',\n",
       "   ' claiming that',\n",
       "   ' saying their',\n",
       "   ' because this',\n",
       "   ' claiming this',\n",
       "   ', that',\n",
       "   ' for his',\n",
       "   ' saying a',\n",
       "   ' as his',\n",
       "   ' saying his',\n",
       "   ' because his',\n",
       "   ' that her',\n",
       "   \"'s his\",\n",
       "   ' for that',\n",
       "   ' claiming his',\n",
       "   ' that that',\n",
       "   ' that every',\n",
       "   ' and her',\n",
       "   ', his',\n",
       "   '. the',\n",
       "   '. this',\n",
       "   ' saying the',\n",
       "   ' and the',\n",
       "   ' because every',\n",
       "   ' saying that',\n",
       "   ' and his',\n",
       "   ' because their',\n",
       "   ' that his']},\n",
       " 'phrase_10': {'phrase': ' the subject',\n",
       "  'paraphrases': [' this pages',\n",
       "   ' the book',\n",
       "   ' this article',\n",
       "   ' this subject',\n",
       "   ' the internet',\n",
       "   ' this by',\n",
       "   ' this site',\n",
       "   ' this topics',\n",
       "   ' the topic',\n",
       "   ' thisikipedia',\n",
       "   ' this and',\n",
       "   ' the own',\n",
       "   ' the web',\n",
       "   ' our site',\n",
       "   ' the website',\n",
       "   ' this web',\n",
       "   ' that by',\n",
       "   ' your page',\n",
       "   ' your site',\n",
       "   ' the page',\n",
       "   ' this,',\n",
       "   ' this page',\n",
       "   ' theikipedia',\n",
       "   ' the of',\n",
       "   ' our page',\n",
       "   ' this sources',\n",
       "   ' this book',\n",
       "   ' the site',\n",
       "   ' your by',\n",
       "   ' the and',\n",
       "   ' this own',\n",
       "   ' the,',\n",
       "   ' the by',\n",
       "   ' this of',\n",
       "   ' this website',\n",
       "   ' his by',\n",
       "   ' your website',\n",
       "   ' his site',\n",
       "   ' this internet',\n",
       "   ' this or',\n",
       "   ' our subject',\n",
       "   ' your subject',\n",
       "   ' this topic',\n",
       "   ' our website',\n",
       "   ' this blog',\n",
       "   ' his website',\n",
       "   ' this media',\n",
       "   ' our by',\n",
       "   ' this net']},\n",
       " 'phrase_11': {'phrase': ' this article',\n",
       "  'paraphrases': [' this it',\n",
       "   ', text',\n",
       "   ' the they',\n",
       "   ' the things',\n",
       "   ' the these',\n",
       "   ' the versions',\n",
       "   ', it',\n",
       "   ' the we',\n",
       "   ' the information',\n",
       "   ' the documents',\n",
       "   ', documents',\n",
       "   ' these it',\n",
       "   ', articles',\n",
       "   ' the version',\n",
       "   ' the this',\n",
       "   ' the articles',\n",
       "   ' that it',\n",
       "   ' the sections',\n",
       "   ' the sources',\n",
       "   ' those they',\n",
       "   ' these they',\n",
       "   ' the text',\n",
       "   ' these two',\n",
       "   ' these sources',\n",
       "   ', this',\n",
       "   ', two',\n",
       "   ' those it',\n",
       "   ', sources',\n",
       "   ' this sources',\n",
       "   ', information',\n",
       "   ' this information',\n",
       "   ' the section',\n",
       "   ' the that',\n",
       "   ' that they',\n",
       "   ', that',\n",
       "   ' these information',\n",
       "   ' the,',\n",
       "   ' the it',\n",
       "   ' this two',\n",
       "   ' the parts',\n",
       "   ', sections',\n",
       "   ', versions',\n",
       "   ' the books',\n",
       "   ' the article',\n",
       "   ' this they',\n",
       "   ', they',\n",
       "   ', article',\n",
       "   ' the two',\n",
       "   ', we',\n",
       "   ', these']},\n",
       " 'phrase_12': {'phrase': ', this is not',\n",
       "  'paraphrases': [', not not know',\n",
       "   ' i did not know',\n",
       "   ', not of',\n",
       "   \" i do't know\",\n",
       "   ' i do not know',\n",
       "   ', is not know',\n",
       "   ', do not matter',\n",
       "   ' it don not know',\n",
       "   \", do't know\",\n",
       "   ', we not know',\n",
       "   \" we don't know\",\n",
       "   ', don not is',\n",
       "   ', don tell know',\n",
       "   \",'s\",\n",
       "   \" we do't know\",\n",
       "   ', on',\n",
       "   ', not know',\n",
       "   ', can not know',\n",
       "   ', does not know',\n",
       "   ', don not believe',\n",
       "   ' they do not know',\n",
       "   ' you don not know',\n",
       "   ', is is',\n",
       "   ' we do not know',\n",
       "   \" i don't know\",\n",
       "   ' not of',\n",
       "   ', not not',\n",
       "   ', don not tell',\n",
       "   ', know',\n",
       "   ', will not know',\n",
       "   ' i we not know',\n",
       "   ', do not tell',\n",
       "   ' we can not know',\n",
       "   ', are not know',\n",
       "   ', do not of',\n",
       "   ', not on',\n",
       "   ', don not on',\n",
       "   \", don't know\",\n",
       "   ', we is',\n",
       "   ' they don not know',\n",
       "   ', did not know',\n",
       "   ' it do not know',\n",
       "   ' we we not know',\n",
       "   ', not is',\n",
       "   ' you do not know',\n",
       "   ', of',\n",
       "   ' we don not know',\n",
       "   ' that of',\n",
       "   ' not is',\n",
       "   ', matter',\n",
       "   ' we don not of',\n",
       "   ', do not see',\n",
       "   ' we did not know',\n",
       "   ', don not know',\n",
       "   ', don not of',\n",
       "   ' that is',\n",
       "   ', do not is',\n",
       "   ', we on',\n",
       "   ', is on',\n",
       "   ', do not know',\n",
       "   ' i don not know',\n",
       "   ', don not you',\n",
       "   ', don not see',\n",
       "   ', you is',\n",
       "   ' i can not know',\n",
       "   ', do not on',\n",
       "   ', don not matter',\n",
       "   ', have not know',\n",
       "   ', is',\n",
       "   ', you not know']}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8f3eca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Tuple, Dict\n",
    "from transformers import PreTrainedTokenizerBase, PreTrainedModel\n",
    "\n",
    "@torch.no_grad()\n",
    "def score_phrase_in_context(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    tokens: List[str],\n",
    "    span: Tuple[int, int],\n",
    "    candidate_tokens: List[str],\n",
    ") -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    Approximate log P(candidate_tokens | context) using pseudo log-likelihood:\n",
    "    for each candidate token position, mask it, get log P(true token | context),\n",
    "    and accumulate.\n",
    "\n",
    "    Returns a dict with:\n",
    "        {\n",
    "          \"token_logprobs\": [float, ...],  # one per token in candidate_tokens\n",
    "          \"sum_logprob\": float,\n",
    "          \"avg_logprob\": float,\n",
    "        }\n",
    "    \"\"\"\n",
    "    start_idx, end_idx = span\n",
    "    assert (end_idx - start_idx + 1) == len(candidate_tokens), \"Span length mismatch.\"\n",
    "\n",
    "    # Insert candidate tokens into a copy of the tokens\n",
    "    base_tokens = tokens.copy()\n",
    "    base_tokens[start_idx:end_idx + 1] = candidate_tokens\n",
    "\n",
    "    # Encode once with specials\n",
    "    enc = tokenizer(\n",
    "        base_tokens,\n",
    "        is_split_into_words=False,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    input_ids = enc[\"input_ids\"]  # [1, seq_len]\n",
    "    mask_id = tokenizer.mask_token_id\n",
    "    if mask_id is None:\n",
    "        raise ValueError(\"Tokenizer must have a mask_token_id.\")\n",
    "\n",
    "    token_logprobs: List[float] = []\n",
    "\n",
    "    # Assume [CLS] at position 0 and tokens aligned 1:1 after that\n",
    "    seq_start = 1  # offset of first \"real\" token in input_ids\n",
    "\n",
    "    for pos_offset, tok_str in enumerate(candidate_tokens):\n",
    "        token_id = tokenizer.convert_tokens_to_ids(tok_str)\n",
    "\n",
    "        pos = seq_start + start_idx + pos_offset\n",
    "\n",
    "        masked_ids = input_ids.clone()\n",
    "        masked_ids[0, pos] = mask_id\n",
    "\n",
    "        outputs = model(input_ids=masked_ids)\n",
    "        logits = outputs.logits  # [1, seq_len, vocab]\n",
    "        log_probs = torch.log_softmax(logits[0, pos], dim=-1)\n",
    "\n",
    "        tok_log_prob = float(log_probs[token_id].item())\n",
    "        token_logprobs.append(tok_log_prob)\n",
    "\n",
    "    sum_logprob = float(sum(token_logprobs))\n",
    "    avg_logprob = sum_logprob / len(token_logprobs)\n",
    "\n",
    "    return {\n",
    "        \"token_logprobs\": token_logprobs,\n",
    "        \"sum_logprob\": sum_logprob,\n",
    "        \"avg_logprob\": avg_logprob,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "13043522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "from transformers import PreTrainedTokenizerBase, PreTrainedModel\n",
    "\n",
    "def find_phrase_token_spans(\n",
    "    text: str,\n",
    "    phrase_tokens: List[str],\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    ") -> Tuple[List[Tuple[int, int]], List[str]]:\n",
    "    \"\"\"\n",
    "    Find all contiguous occurrences of phrase_tokens in tokenized text.\n",
    "\n",
    "    Returns:\n",
    "        spans: list of (start_idx, end_idx_inclusive) in token indices.\n",
    "        tokens: the tokenized text.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    spans: List[Tuple[int, int]] = []\n",
    "    n = len(phrase_tokens)\n",
    "\n",
    "    for start_idx in range(len(tokens) - n + 1):\n",
    "        if tokens[start_idx:start_idx + n] == phrase_tokens:\n",
    "            spans.append((start_idx, start_idx + n - 1))\n",
    "\n",
    "    return spans, tokens\n",
    "\n",
    "\n",
    "def score_candidates_in_unknown(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    unknown_text: str,\n",
    "    phrase_tokens: List[str],\n",
    "    candidates: List[str],\n",
    ") -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    For each occurrence of phrase_tokens in unknown_text, compute MLM scores for\n",
    "    each candidate phrase.\n",
    "\n",
    "    Returns a dict:\n",
    "\n",
    "        {\n",
    "          \"occurrences\": [\n",
    "             {\n",
    "               \"span\": (start_idx, end_idx),\n",
    "               \"scores\": [\n",
    "                  {\n",
    "                    \"candidate\": str,\n",
    "                    \"token_logprobs\": [float, ...],\n",
    "                    \"sum_logprob\": float,\n",
    "                    \"avg_logprob\": float,\n",
    "                  },\n",
    "                  ...\n",
    "               ]\n",
    "             },\n",
    "             ...\n",
    "          ]\n",
    "        }\n",
    "    \"\"\"\n",
    "    spans, base_tokens = find_phrase_token_spans(unknown_text, phrase_tokens, tokenizer)\n",
    "\n",
    "    results: Dict[str, object] = {\"occurrences\": []}\n",
    "\n",
    "    # Pre-tokenize candidates\n",
    "    cand_token_map: Dict[str, List[str]] = {\n",
    "        cand: tokenizer.tokenize(cand) for cand in candidates\n",
    "    }\n",
    "\n",
    "    for span in spans:\n",
    "        start_idx, end_idx = span\n",
    "        span_len = end_idx - start_idx + 1\n",
    "\n",
    "        scores_for_occurrence = []\n",
    "\n",
    "        for cand, cand_tokens in cand_token_map.items():\n",
    "            # Easiest: only compare candidates of same token length as the span\n",
    "            if len(cand_tokens) != span_len:\n",
    "                continue\n",
    "\n",
    "            score_dict = score_phrase_in_context(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                tokens=base_tokens,\n",
    "                span=span,\n",
    "                candidate_tokens=cand_tokens,\n",
    "            )\n",
    "\n",
    "            scores_for_occurrence.append({\n",
    "                \"candidate\": cand,\n",
    "                \"token_logprobs\": score_dict[\"token_logprobs\"],\n",
    "                \"sum_logprob\": score_dict[\"sum_logprob\"],\n",
    "                \"avg_logprob\": score_dict[\"avg_logprob\"],\n",
    "            })\n",
    "\n",
    "        # Sort candidates by avg_logprob (higher = more likely)\n",
    "        scores_for_occurrence.sort(\n",
    "            key=lambda x: x[\"avg_logprob\"],\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        results[\"occurrences\"].append({\n",
    "            \"span\": span,\n",
    "            \"scores\": scores_for_occurrence,\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d2eabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', 'Ġbut', 'Ġthis']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_tokens = n_gram_list[0][1]\n",
    "ref_tokens = list(ast.literal_eval(ref_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "69eebfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_tokens = n_gram_list[0][1]\n",
    "phrase_tokens = list(ast.literal_eval(ref_tokens))\n",
    "phrases = n_gram_dict['phrase_01']\n",
    "candidates = [phrases['phrase']] + phrases['paraphrases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5a781de0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "File \u001b[0;32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:767\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n",
      "\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n",
      "\u001b[0;32m--> 767\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    769\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n",
      "\u001b[1;32m    770\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n",
      "\u001b[1;32m    771\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n",
      "\u001b[1;32m    772\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n",
      "\u001b[1;32m    773\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n",
      "\u001b[1;32m    774\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:729\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n",
      "\u001b[1;32m    728\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(value))\n",
      "\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 3 at dim 1 (got 5)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[128], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mscore_candidates_in_unknown\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43munknown_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munknown_text\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mphrase_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mphrase_tokens\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcandidates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcandidates\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Example: inspect first occurrence\u001b[39;00m\n",
      "\u001b[1;32m     10\u001b[0m occ \u001b[38;5;241m=\u001b[39m scores[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moccurrences\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "\n",
      "Cell \u001b[0;32mIn[115], line 78\u001b[0m, in \u001b[0;36mscore_candidates_in_unknown\u001b[0;34m(model, tokenizer, unknown_text, phrase_tokens, candidates)\u001b[0m\n",
      "\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cand_tokens) \u001b[38;5;241m!=\u001b[39m span_len:\n",
      "\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;32m---> 78\u001b[0m     score_dict \u001b[38;5;241m=\u001b[39m \u001b[43mscore_phrase_in_context\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_tokens\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspan\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcand_tokens\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     86\u001b[0m     scores_for_occurrence\u001b[38;5;241m.\u001b[39mappend({\n",
      "\u001b[1;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcandidate\u001b[39m\u001b[38;5;124m\"\u001b[39m: cand,\n",
      "\u001b[1;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: score_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n",
      "\u001b[1;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum_logprob\u001b[39m\u001b[38;5;124m\"\u001b[39m: score_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum_logprob\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n",
      "\u001b[1;32m     90\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_logprob\u001b[39m\u001b[38;5;124m\"\u001b[39m: score_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_logprob\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n",
      "\u001b[1;32m     91\u001b[0m     })\n",
      "\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Sort candidates by avg_logprob (higher = more likely)\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n",
      "\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n",
      "\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "Cell \u001b[0;32mIn[114], line 33\u001b[0m, in \u001b[0;36mscore_phrase_in_context\u001b[0;34m(model, tokenizer, tokens, span, candidate_tokens)\u001b[0m\n",
      "\u001b[1;32m     30\u001b[0m base_tokens[start_idx:end_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m candidate_tokens\n",
      "\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Encode once with specials\u001b[39;00m\n",
      "\u001b[0;32m---> 33\u001b[0m enc \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_tokens\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     38\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     39\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m enc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# [1, seq_len]\u001b[39;00m\n",
      "\u001b[1;32m     40\u001b[0m mask_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mmask_token_id\n",
      "\n",
      "File \u001b[0;32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2855\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   2853\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n",
      "\u001b[1;32m   2854\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n",
      "\u001b[0;32m-> 2855\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   2856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m   2857\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "\n",
      "File \u001b[0;32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2943\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   2938\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n",
      "\u001b[1;32m   2939\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m   2940\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m   2941\u001b[0m         )\n",
      "\u001b[1;32m   2942\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n",
      "\u001b[0;32m-> 2943\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   2944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2945\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2961\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2962\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2963\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   2964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m   2965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n",
      "\u001b[1;32m   2966\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n",
      "\u001b[1;32m   2967\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   2985\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n",
      "\u001b[1;32m   2986\u001b[0m     )\n",
      "\n",
      "File \u001b[0;32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3144\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   3134\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n",
      "\u001b[1;32m   3135\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n",
      "\u001b[1;32m   3136\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n",
      "\u001b[1;32m   3137\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   3141\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n",
      "\u001b[1;32m   3142\u001b[0m )\n",
      "\u001b[0;32m-> 3144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   3145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3146\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3162\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3163\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3164\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:601\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n",
      "\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_ids \u001b[38;5;129;01min\u001b[39;00m sanitized_tokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "\u001b[1;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eventual_warn_about_too_long_sequence(input_ids, max_length, verbose)\n",
      "\u001b[0;32m--> 601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msanitized_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitized_encodings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:240\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n",
      "\u001b[1;32m    236\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n",
      "\u001b[1;32m    238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n",
      "\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:783\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n",
      "\u001b[1;32m    778\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;32m    779\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n",
      "\u001b[1;32m    780\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    781\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    782\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;32m--> 783\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n",
      "\u001b[1;32m    784\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    785\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    786\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    787\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    788\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "scores = score_candidates_in_unknown(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    unknown_text=unknown_text,\n",
    "    phrase_tokens=phrase_tokens,\n",
    "    candidates=candidates,\n",
    ")\n",
    "\n",
    "# Example: inspect first occurrence\n",
    "occ = scores[\"occurrences\"][0]\n",
    "print(\"Span:\", occ[\"span\"])\n",
    "for s in occ[\"scores\"][:5]:\n",
    "    print(\n",
    "        f\"Candidate: {s['candidate']!r}\\n\"\n",
    "        f\"  token_logprobs: {s['token_logprobs']}\\n\"\n",
    "        f\"  sum_logprob: {s['sum_logprob']:.3f}\\n\"\n",
    "        f\"  avg_logprob: {s['avg_logprob']:.3f}\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e4cc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from from_root import from_root\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure we can import from src/\n",
    "sys.path.insert(0, str(from_root(\"src\")))\n",
    "\n",
    "from read_and_write_docs import read_jsonl, read_rds\n",
    "from model_loading import load_model, distinct_special_chars\n",
    "from utils import apply_temp_doc_id, build_metadata_df\n",
    "from n_gram_functions import (\n",
    "    common_ngrams,\n",
    "    filter_ngrams,\n",
    "    pretty_print_common_ngrams,\n",
    "    get_scored_df,\n",
    "    get_scored_df_no_context\n",
    ")\n",
    "from open_ai import initialise_client, llm\n",
    "from excel_functions import create_excel_template\n",
    "\n",
    "# --------------------\n",
    "# Helpers\n",
    "# --------------------\n",
    "\n",
    "# remove illegal control chars (keep \\t, \\n, \\r)\n",
    "_ILLEGAL_RE = re.compile(r\"[\\x00-\\x08\\x0B-\\x0C\\x0E-\\x1F]\")\n",
    "\n",
    "def _clean_cell(x):\n",
    "    if isinstance(x, str):\n",
    "        return _ILLEGAL_RE.sub(\"\", x)\n",
    "    return x\n",
    "\n",
    "def clean_for_excel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    obj_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    df[obj_cols] = df[obj_cols].applymap(_clean_cell)\n",
    "    return df\n",
    "\n",
    "def create_system_prompt(prompt_loc):\n",
    "    \"\"\"Reads the prompt as a .txt file for better versioning\"\"\"\n",
    "    with open(prompt_loc, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "    \n",
    "def create_user_prompt(known_text, phrase, raw_phrase):\n",
    "    \"\"\"The method of input to the LLM as described in the system prompt\"\"\"\n",
    "    user_prompt = f\"\"\"\n",
    "<DOC>\n",
    "{known_text}\n",
    "</DOC>\n",
    "<RAW NGRAM>\n",
    "\"{raw_phrase}\"\n",
    "</RAW NGRAM>\n",
    "<NGRAM>\n",
    "\"{phrase}\"\n",
    "</NGRAM>\n",
    "\"\"\"\n",
    "    \n",
    "    return user_prompt\n",
    "\n",
    "def parse_paraphrases(response, phrase, lowercase=True):\n",
    "    \"\"\"Extract paraphrases from OpenAI response (JSON mode).\"\"\"\n",
    "    paraphrase_list = []\n",
    "    for i in range(1, len(response.choices)):\n",
    "        content = response.choices[i].message.content\n",
    "        \n",
    "        try:\n",
    "            content_json = json.loads(content)\n",
    "            for para in content_json['paraphrases']:\n",
    "                if para != phrase:\n",
    "                    if (lowercase) & (para.lower() != phrase):\n",
    "                        paraphrase_list.append(para.lower())\n",
    "                    else:\n",
    "                        paraphrase_list.append(para)  \n",
    "        except Exception:\n",
    "            continue\n",
    "        \n",
    "    unique_list = list(set(paraphrase_list))\n",
    "    \n",
    "    return unique_list\n",
    "\n",
    "def parse_args():\n",
    "    ap = argparse.ArgumentParser(description=\"OpenAI N-gram paraphrase pipeline\")\n",
    "    # Paths\n",
    "    ap.add_argument(\"--known_loc\")\n",
    "    ap.add_argument(\"--unknown_loc\")\n",
    "    ap.add_argument(\"--metadata_loc\")\n",
    "    ap.add_argument(\"--model_loc\")\n",
    "    ap.add_argument(\"--save_loc\")\n",
    "    ap.add_argument(\"--completed_loc\", default=None)\n",
    "    # Dataset hinting\n",
    "    ap.add_argument(\"--corpus\", default=\"Wiki\")\n",
    "    ap.add_argument(\"--data_type\", default=\"training\")\n",
    "    ap.add_argument(\"--known_doc\")\n",
    "    ap.add_argument(\"--unknown_doc\")\n",
    "    # Env\n",
    "    ap.add_argument(\"--credentials_loc\", default=str(from_root(\"credentials.json\")))\n",
    "    ap.add_argument(\"--prompt_loc\", default=str(from_root(\"prompts\", \"exhaustive_constrained_ngram_paraphraser_prompt_JSON_new.txt\")))\n",
    "    # N-gram\n",
    "    ap.add_argument(\"--ngram_n\", type=int, default=2)\n",
    "    ap.add_argument(\"--lowercase\", action=\"store_true\")\n",
    "    ap.add_argument(\"--no_lowercase\", dest=\"lowercase\", action=\"store_false\")\n",
    "    ap.set_defaults(lowercase=True)\n",
    "    ap.add_argument(\"--order\", default=\"len_desc\", help=\"Order for pretty_print_common_ngrams\")\n",
    "    ap.add_argument(\"--score_texts\", action=\"store_true\")\n",
    "    # OpenAI\n",
    "    ap.add_argument(\"--openai_model\", default=\"gpt-4.1\")\n",
    "    ap.add_argument(\"--max_tokens\", type=int, default=5000)\n",
    "    ap.add_argument(\"--temperature\", type=float, default=0.7)\n",
    "    ap.add_argument(\"--n\", type=int, default=10)\n",
    "\n",
    "    return ap.parse_args()\n",
    "\n",
    "def main():\n",
    "    \n",
    "    args=parse_args()\n",
    "    \n",
    "    # Ensure the directory exists before beginning\n",
    "    os.makedirs(args.save_loc, exist_ok=True)\n",
    "    \n",
    "    # -----\n",
    "    # LOAD DATA & LOCAL MODEL\n",
    "    # -----\n",
    "    specific_problem = f\"{args.known_doc} vs {args.unknown_doc}\"\n",
    "    save_loc = f\"{args.save_loc}/{specific_problem}.xlsx\"\n",
    "    \n",
    "    if args.completed_loc:\n",
    "        completed_loc = f\"{args.completed_loc}/{specific_problem}.xlsx\"\n",
    "        if os.path.exists(completed_loc):\n",
    "            print(f\"Result for {specific_problem} already exists in the completed folder. Exiting.\")\n",
    "            sys.exit()\n",
    "    \n",
    "    # Skip the problem if already exists\n",
    "    if os.path.exists(save_loc):\n",
    "        print(f\"Path {save_loc} already exists. Exiting.\")\n",
    "        sys.exit()\n",
    "        \n",
    "    print(f\"Working on problem: {specific_problem}\")\n",
    "    \n",
    "    print(\"Loading model\")\n",
    "    tokenizer, model = load_model(args.model_loc)\n",
    "    special_tokens = distinct_special_chars(tokenizer=tokenizer)\n",
    "    \n",
    "    print(\"Loading data\")\n",
    "    known = read_jsonl(args.known_loc)\n",
    "    known = apply_temp_doc_id(known)\n",
    "    \n",
    "    unknown = read_jsonl(args.unknown_loc)\n",
    "    unknown = apply_temp_doc_id(unknown)\n",
    "\n",
    "    print(\"Data loaded\")\n",
    "    \n",
    "    # NOTE - Is this used?\n",
    "    metadata = read_rds(args.metadata_loc)\n",
    "    filtered_metadata = metadata[metadata['corpus'] == args.corpus]\n",
    "    agg_metadata = build_metadata_df(filtered_metadata, known, unknown)\n",
    "\n",
    "    # -----\n",
    "    # Get the chosen text & metadata\n",
    "    # -----\n",
    "    \n",
    "    known_text = known[known['doc_id'] == args.known_doc].reset_index().loc[0, 'text'].lower()\n",
    "    unknown_text = unknown[unknown['doc_id'] == args.unknown_doc].reset_index().loc[0, 'text'].lower()\n",
    "    \n",
    "    problem_metadata = agg_metadata[(agg_metadata['known_doc_id'] == args.known_doc)\n",
    "                                    & (agg_metadata['unknown_doc_id'] == args.unknown_doc)].reset_index()\n",
    "    problem_metadata['target'] = problem_metadata['known_author'] == problem_metadata['unknown_author']\n",
    "\n",
    "    # -----\n",
    "    # Create document dataframe\n",
    "    # -----\n",
    "    \n",
    "    # This is used to display the text\n",
    "    docs_df = pd.DataFrame(\n",
    "    {\n",
    "        \"known\":   [args.corpus, args.data_type, args.known_doc, known_text],\n",
    "        \"unknown\": [args.corpus, args.data_type, args.unknown_doc, unknown_text],\n",
    "    },\n",
    "    index=[\"corpus\", \"data type\", \"doc\", \"text\"],\n",
    "    )\n",
    "    \n",
    "    # -----\n",
    "    # Get common n-grams\n",
    "    # -----\n",
    "    \n",
    "    print(\"Getting common n-grams\")\n",
    "    common = common_ngrams(known_text, unknown_text, args.ngram_n, model, tokenizer, lowercase=args.lowercase)\n",
    "    \n",
    "    # Filter to remove smaller n-grams which don't satisfy the rules\n",
    "    common = filter_ngrams(common, special_tokens=special_tokens)\n",
    "    n_gram_list = pretty_print_common_ngrams(common, tokenizer=tokenizer, order=args.order, return_format='flat', show_raw=True)\n",
    "    print(f\"There are {len(n_gram_list)} n-grams in common!\")\n",
    "    \n",
    "    # -----\n",
    "    # OpenAI bits\n",
    "    # -----\n",
    "    \n",
    "    print(\"Generating paraphrases\")\n",
    "    client = initialise_client(args.credentials_loc)\n",
    "    \n",
    "    n_gram_dict = {}\n",
    "    width = len(str(len(n_gram_list)))  # e.g., 10 -> 2, 100 -> 3\n",
    "\n",
    "    for idx, (phrase_pretty, phrase_raw) in enumerate(n_gram_list, start=1):\n",
    "        user_prompt = create_user_prompt(known_text, phrase_pretty, raw_phrase=phrase_raw)\n",
    "        response = llm(\n",
    "            create_system_prompt(args.prompt_loc),\n",
    "            user_prompt,\n",
    "            client,\n",
    "            model=args.openai_model,\n",
    "            max_tokens=args.max_tokens,\n",
    "            temperature=args.temperature,\n",
    "            n=args.n,\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "        )\n",
    "        paraphrases = parse_paraphrases(response, phrase_pretty)\n",
    "        key = f\"phrase_{idx:0{width}d}\"  # -> phrase_01, phrase_002, etc.\n",
    "        n_gram_dict[key] = {\"phrase\": phrase_pretty, \"paraphrases\": paraphrases}\n",
    "        \n",
    "    # -----\n",
    "    # Score phrases\n",
    "    # -----\n",
    "    if args.score_texts:\n",
    "        print(\"Scoring phrases\")\n",
    "        print(\"    Scoring known text\")\n",
    "        known_scored = get_scored_df(n_gram_dict, known_text, tokenizer, model)\n",
    "        \n",
    "        print(\"    Scoring unknown text\")\n",
    "        unknown_scored = get_scored_df(n_gram_dict, unknown_text, tokenizer, model)\n",
    "        \n",
    "        print(\"    Scoring phrases with no context\")\n",
    "        score_df_no_context = get_scored_df_no_context(n_gram_dict, tokenizer, model)\n",
    "        \n",
    "        # -----\n",
    "        # Final cleaning and saving\n",
    "        # -----\n",
    "        \n",
    "        print(f\"Writing file: {specific_problem}\")\n",
    "        \n",
    "        # Run the new Excel function\n",
    "        create_excel_template(\n",
    "            known=known_scored,\n",
    "            unknown=unknown_scored,\n",
    "            no_context=score_df_no_context,\n",
    "            metadata=problem_metadata,\n",
    "            docs=docs_df,\n",
    "            path=save_loc,\n",
    "            known_sheet=\"known\",\n",
    "            unknown_sheet=\"unknown\",\n",
    "            nc_sheet=\"no context\",\n",
    "            metadata_sheet=\"metadata\",\n",
    "            docs_sheet=\"docs\",\n",
    "            llr_sheet=\"LLR\",\n",
    "            use_xlookup=False,\n",
    "            highlight_phrases=False\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        print(\"Not scoring texts\")\n",
    "        print(\"<<<RESULT_JSON_START>>>\")\n",
    "        print(json.dumps(n_gram_dict))\n",
    "        print(\"<<<RESULT_JSON_END>>>\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
