{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f0352db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from from_root import from_root\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure we can import from src/\n",
    "sys.path.insert(0, str(from_root(\"src\")))\n",
    "\n",
    "from read_and_write_docs import read_jsonl, read_rds\n",
    "from model_loading import load_model, distinct_special_chars, load_model_efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67d9bd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loc = \"/Volumes/BCross/models/ModernBERT/ModernBERT-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77cff219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_loc)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c1c0737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token:  is\n"
     ]
    }
   ],
   "source": [
    "text = \"The capital of France [MASK] [MASK].\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# To get predictions for the mask:\n",
    "masked_index = inputs[\"input_ids\"][0].tolist().index(tokenizer.mask_token_id)\n",
    "predicted_token_id = outputs.logits[0, masked_index].argmax(axis=-1)\n",
    "predicted_token = tokenizer.decode(predicted_token_id)\n",
    "print(\"Predicted token:\", predicted_token)\n",
    "# Predicted token:  Paris\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9784ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1 — Joint multi-mask fill with beam search (ModernBERT or any MLM)\n",
    "import math\n",
    "from typing import List, Dict, Tuple, Optional, Iterable\n",
    "import torch\n",
    "from transformers import PreTrainedTokenizerBase, PreTrainedModel, AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "@torch.no_grad()\n",
    "def fill_masks_beam(\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    masked_text: str,\n",
    "    top_k_per_mask: int = 25,   # widen/limit local options at each mask\n",
    "    beam_size: int = 100,       # how many partial hypotheses to keep\n",
    "    max_candidates: int = 50,   # truncate final list\n",
    "    banned_token_ids: Optional[Iterable[int]] = None,\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Jointly fill one or more [MASK] tokens using a simple beam search.\n",
    "\n",
    "    Returns a list of dicts: {text, score, tokens, token_ids, mask_positions}\n",
    "    where 'score' is the sum of log-probs across all masks (higher is better).\n",
    "    \"\"\"\n",
    "    if tokenizer.mask_token_id is None:\n",
    "        raise ValueError(\"Tokenizer must define mask_token_id (MLM required).\")\n",
    "\n",
    "    enc = tokenizer(masked_text, return_tensors=\"pt\", add_special_tokens=True, truncation=True)\n",
    "    input_ids = enc[\"input_ids\"]\n",
    "    mask_id = tokenizer.mask_token_id\n",
    "    mask_positions = (input_ids[0] == mask_id).nonzero(as_tuple=False).flatten().tolist()\n",
    "    if not mask_positions:\n",
    "        return []\n",
    "\n",
    "    outputs = model(**enc)\n",
    "    log_probs = outputs.logits.log_softmax(-1)  # [1, seq_len, vocab]\n",
    "\n",
    "    specials = set(getattr(tokenizer, \"all_special_ids\", []) or [])\n",
    "    banned = set(banned_token_ids or []) | specials\n",
    "\n",
    "    # Precompute top-k candidates at each mask position\n",
    "    per_mask_cands = []\n",
    "    for pos in mask_positions:\n",
    "        lp = log_probs[0, pos].clone()\n",
    "        if banned:\n",
    "            idx = torch.tensor(sorted(banned), dtype=torch.long)\n",
    "            lp.index_fill_(0, idx, float(\"-inf\"))\n",
    "        topk = torch.topk(lp, k=min(top_k_per_mask, lp.numel()))\n",
    "        per_mask_cands.append((topk.indices.tolist(), topk.values.tolist()))\n",
    "\n",
    "    # Beam over masks (left-to-right in token order)\n",
    "    beam = [(0.0, [])]  # (cum_logprob, chosen_token_ids_so_far)\n",
    "    for cand_ids, cand_lps in per_mask_cands:\n",
    "        new_beam = []\n",
    "        for cum_lp, chosen in beam:\n",
    "            for tid, lp in zip(cand_ids, cand_lps):\n",
    "                new_beam.append((cum_lp + float(lp), chosen + [tid]))\n",
    "        new_beam.sort(key=lambda x: x[0], reverse=True)\n",
    "        beam = new_beam[:beam_size]\n",
    "\n",
    "    # Materialize and deduplicate by decoded text\n",
    "    best = {}\n",
    "    for cum_lp, choice_ids in beam:\n",
    "        filled = input_ids.clone()\n",
    "        for pos, tid in zip(mask_positions, choice_ids):\n",
    "            filled[0, pos] = tid\n",
    "        text_out = tokenizer.decode(filled[0], skip_special_tokens=True)\n",
    "        prev = best.get(text_out)\n",
    "        if (prev is None) or (cum_lp > prev[\"score\"]):\n",
    "            best[text_out] = {\n",
    "                \"text\": text_out,\n",
    "                \"score\": cum_lp,\n",
    "                \"tokens\": tokenizer.convert_ids_to_tokens(choice_ids),\n",
    "                \"token_ids\": choice_ids,\n",
    "                \"mask_positions\": mask_positions,\n",
    "            }\n",
    "    return sorted(best.values(), key=lambda r: r[\"score\"], reverse=True)[:max_candidates]\n",
    "\n",
    "# --- Example usage ---\n",
    "# tok = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\n",
    "# mdl = AutoModelForMaskedLM.from_pretrained(\"answerdotai/ModernBERT-base\").eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3953998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I didn't have time to finish the report.   (score=-0.06)  tokens=[\"'t\", 'Ġhave']\n",
      "I didn't get time to finish the report.   (score=-3.87)  tokens=[\"'t\", 'Ġget']\n",
      "I didn't find time to finish the report.   (score=-4.45)  tokens=[\"'t\", 'Ġfind']\n",
      "I didn't take time to finish the report.   (score=-5.00)  tokens=[\"'t\", 'Ġtake']\n",
      "I didn't had time to finish the report.   (score=-5.91)  tokens=[\"'t\", 'Ġhad']\n",
      "I didn t have time to finish the report.   (score=-6.10)  tokens=['Ġt', 'Ġhave']\n",
      "I didn't make time to finish the report.   (score=-6.16)  tokens=[\"'t\", 'Ġmake']\n",
      "I didn’ have time to finish the report.   (score=-6.68)  tokens=['âĢĻ', 'Ġhave']\n",
      "I didn't enough time to finish the report.   (score=-7.33)  tokens=[\"'t\", 'Ġenough']\n",
      "I didn't leave time to finish the report.   (score=-7.47)  tokens=[\"'t\", 'Ġleave']\n",
      "I didn't need time to finish the report.   (score=-7.69)  tokens=[\"'t\", 'Ġneed']\n",
      "I didn not have time to finish the report.   (score=-7.90)  tokens=['Ġnot', 'Ġhave']\n",
      "I didn't see time to finish the report.   (score=-7.92)  tokens=[\"'t\", 'Ġsee']\n",
      "I didn't spare time to finish the report.   (score=-8.21)  tokens=[\"'t\", 'Ġspare']\n",
      "I didn't allocate time to finish the report.   (score=-8.41)  tokens=[\"'t\", 'Ġallocate']\n",
      "I didn't allow time to finish the report.   (score=-8.41)  tokens=[\"'t\", 'Ġallow']\n",
      "I didn` have time to finish the report.   (score=-8.50)  tokens=['`', 'Ġhave']\n",
      "I didnot have time to finish the report.   (score=-8.61)  tokens=['ot', 'Ġhave']\n",
      "I didn't receive time to finish the report.   (score=-8.66)  tokens=[\"'t\", 'Ġreceive']\n",
      "I didn' have time to finish the report.   (score=-8.71)  tokens=[\"'\", 'Ġhave']\n",
      "I didn't spend time to finish the report.   (score=-8.81)  tokens=[\"'t\", 'Ġspend']\n",
      "I didnt have time to finish the report.   (score=-8.91)  tokens=['t', 'Ġhave']\n",
      "I didn't the time to finish the report.   (score=-8.91)  tokens=[\"'t\", 'Ġthe']\n",
      "I didn't want time to finish the report.   (score=-9.08)  tokens=[\"'t\", 'Ġwant']\n",
      "I didn´ have time to finish the report.   (score=-9.09)  tokens=['Â´', 'Ġhave']\n",
      "I didn't give time to finish the report.   (score=-9.09)  tokens=[\"'t\", 'Ġgive']\n",
      "I didn't manage time to finish the report.   (score=-9.18)  tokens=[\"'t\", 'Ġmanage']\n",
      "I didnít have time to finish the report.   (score=-9.24)  tokens=['ÃŃt', 'Ġhave']\n",
      "I didn't got time to finish the report.   (score=-9.28)  tokens=[\"'t\", 'Ġgot']\n",
      "I didn't waste time to finish the report.   (score=-9.32)  tokens=[\"'t\", 'Ġwaste']\n",
      "I didn'thave time to finish the report.   (score=-9.47)  tokens=[\"'t\", 'have']\n",
      "I didn't feel time to finish the report.   (score=-9.61)  tokens=[\"'t\", 'Ġfeel']\n",
      "I didn; have time to finish the report.   (score=-9.62)  tokens=[';', 'Ġhave']\n",
      "I didn'have time to finish the report.   (score=-9.73)  tokens=[\"Ġ'\", 'Ġhave']\n",
      "I didn have have time to finish the report.   (score=-9.79)  tokens=['Ġhave', 'Ġhave']\n",
      "I didn't finish time to finish the report.   (score=-9.82)  tokens=[\"'t\", 'Ġfinish']\n",
      "I didn I have time to finish the report.   (score=-9.83)  tokens=['ĠI', 'Ġhave']\n",
      "I didn t get time to finish the report.   (score=-9.92)  tokens=['Ġt', 'Ġget']\n",
      "I didn'tt time to finish the report.   (score=-9.94)  tokens=[\"'t\", 't']\n",
      "I didn't bring time to finish the report.   (score=-9.97)  tokens=[\"'t\", 'Ġbring']\n"
     ]
    }
   ],
   "source": [
    "masked = \"I didn[MASK] [MASK] time to finish the report.\"\n",
    "cands = fill_masks_beam(model, tokenizer, masked, top_k_per_mask=30, beam_size=150, max_candidates=40)\n",
    "for c in cands:\n",
    "    print(f\"{c['text']}   (score={c['score']:.2f})  tokens={c['tokens']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74343568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C1 — Try multiple lengths at one masked span and rank globally\n",
    "from typing import Iterable, List, Dict\n",
    "\n",
    "def variable_length_infill(\n",
    "    model, tokenizer,\n",
    "    masked_template: str,              # contains ONE [MASK] span\n",
    "    length_options: Iterable[int] = (1, 2, 3, 4),\n",
    "    per_length_topk: int = 10,\n",
    "    normalize_by_masks: bool = True,   # de-bias longer spans\n",
    "    **beam_kwargs\n",
    ") -> List[Dict]:\n",
    "    mask_tok = tokenizer.mask_token\n",
    "    assert mask_tok in masked_template, \"Template must contain one [MASK] span.\"\n",
    "    rows = []\n",
    "    for L in length_options:\n",
    "        expanded = masked_template.replace(mask_tok, \" \".join([mask_tok]*L), 1)\n",
    "        outs = fill_masks_beam(model, tokenizer, expanded, **beam_kwargs)\n",
    "        for o in outs[:per_length_topk]:\n",
    "            score = o[\"score\"] / L if normalize_by_masks else o[\"score\"]\n",
    "            rows.append({\"text\": o[\"text\"], \"length\": L, \"score\": score, \"raw_score\": o[\"score\"], \"tokens\": o[\"tokens\"]})\n",
    "    return sorted(rows, key=lambda r: r[\"score\"], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf07ab93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L=3 :: I don't have time to finish the report. (norm_score=-1.02)\n",
      "L=1 :: I have time to finish the report. (norm_score=-1.03)\n",
      "L=4 :: I don't have enough time to finish the report. (norm_score=-1.05)\n",
      "L=4 :: I don not have enough time to finish the report. (norm_score=-1.07)\n",
      "L=2 :: I have no time to finish the report. (norm_score=-1.11)\n",
      "L=3 :: I don not have time to finish the report. (norm_score=-1.11)\n",
      "L=1 :: I need time to finish the report. (norm_score=-1.13)\n",
      "L=3 :: I did't have time to finish the report. (norm_score=-1.14)\n",
      "L=3 :: I do't have time to finish the report. (norm_score=-1.14)\n",
      "L=4 :: I don't have have time to finish the report. (norm_score=-1.18)\n",
      "L=4 :: I don not have have time to finish the report. (norm_score=-1.20)\n",
      "L=3 :: I did not have time to finish the report. (norm_score=-1.23)\n",
      "L=3 :: I do not have time to finish the report. (norm_score=-1.23)\n",
      "L=3 :: I have't have time to finish the report. (norm_score=-1.24)\n",
      "L=3 :: I will't have time to finish the report. (norm_score=-1.25)\n",
      "L=4 :: I don't have the time to finish the report. (norm_score=-1.25)\n",
      "L=4 :: I didn't have enough time to finish the report. (norm_score=-1.27)\n",
      "L=4 :: I don not have the time to finish the report. (norm_score=-1.27)\n",
      "L=4 :: I didn not have enough time to finish the report. (norm_score=-1.29)\n",
      "L=2 :: I have have time to finish the report. (norm_score=-1.37)\n",
      "L=2 :: I have had time to finish the report. (norm_score=-1.65)\n",
      "L=2 :: I have the time to finish the report. (norm_score=-1.73)\n",
      "L=2 :: I have enough time to finish the report. (norm_score=-1.79)\n",
      "L=2 :: I have some time to finish the report. (norm_score=-1.89)\n",
      "L=2 :: I had no time to finish the report. (norm_score=-1.95)\n",
      "L=2 :: I will no time to finish the report. (norm_score=-2.05)\n",
      "L=1 :: I had time to finish the report. (norm_score=-2.13)\n",
      "L=1 :: I needed time to finish the report. (norm_score=-2.98)\n",
      "L=1 :: I want time to finish the report. (norm_score=-3.70)\n",
      "L=1 :: I took time to finish the report. (norm_score=-3.97)\n",
      "L=1 :: I found time to finish the report. (norm_score=-3.99)\n",
      "L=1 :: I find time to finish the report. (norm_score=-4.58)\n"
     ]
    }
   ],
   "source": [
    "tmpl = \"I [MASK] time to finish the report.\"  # one span to grow/shrink\n",
    "best = variable_length_infill(model, tokenizer, tmpl, length_options=range(1,5), per_length_topk=8,\n",
    "                              top_k_per_mask=30, beam_size=200, max_candidates=80)\n",
    "for r in best:\n",
    "    print(f\"L={r['length']} :: {r['text']} (norm_score={r['score']:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88e6b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D1 — PLL scorer (ModernBERT) to rerank finished candidates\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def pll_score(model, tokenizer, text: str) -> float:\n",
    "    enc = tokenizer(text, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    ids = enc[\"input_ids\"]\n",
    "    mask_id = tokenizer.mask_token_id\n",
    "    total = 0.0\n",
    "    # For each token position (excluding specials), predict that token given the rest\n",
    "    for i in range(1, ids.size(1)-1):                      # skip [CLS]/[SEP]-like specials\n",
    "        masked = ids.clone()\n",
    "        target = masked[0, i].item()\n",
    "        masked[0, i] = mask_id\n",
    "        out = model(input_ids=masked, attention_mask=enc[\"attention_mask\"])\n",
    "        lp = out.logits[0, i].log_softmax(-1)[target].item()\n",
    "        total += lp\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b4da96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-11.02 :: I didn't have time to finish the report.\n",
      "-15.29 :: I didn't get time to finish the report.\n",
      "-16.22 :: I didn't find time to finish the report.\n",
      "-17.65 :: I didn t have time to finish the report.\n",
      "-18.63 :: I didn't make time to finish the report.\n",
      "-19.23 :: I didn't leave time to finish the report.\n",
      "-19.79 :: I didn't take time to finish the report.\n",
      "-20.11 :: I didn't had time to finish the report.\n",
      "-21.42 :: I didn't enough time to finish the report.\n",
      "-27.79 :: I didn’ have time to finish the report.\n"
     ]
    }
   ],
   "source": [
    "# Example: rerank top-N ModernBERT candidates from section B/C\n",
    "scored = [(pll_score(model, tokenizer, c['text']), c['text']) for c in [cands[i] for i in range(min(10, len(cands)))]]\n",
    "for s, t in sorted(scored, key=lambda x: x[0], reverse=True):\n",
    "    print(f\"{s:.2f} :: {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a64891",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
