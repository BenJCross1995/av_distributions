{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "549615db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5Model\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5Model.from_pretrained(\"t5-small\")\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
    ").input_ids  # Batch size 1\n",
    "decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n",
    "\n",
    "# forward pass\n",
    "outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "last_hidden_states = outputs.last_hidden_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "40e7d4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exercise Click here to learn more.', 'exercise Click here for more information.', 'exercise Learn more here.', 'exercise Learn more.', 'exercise Read more about it here.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "\n",
    "input_ids = tokenizer(\"Studies have been shown that <extra_id_0> is a great way to stay healthy.<extra_id_1>\", return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=30, num_return_sequences=5, num_beams=5)\n",
    "decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f2a3b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t5_model_generation(model, tokenizer, text):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(input_ids=input_ids, max_new_tokens=30, num_return_sequences=5, num_beams=5)\n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    \n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea603dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\"if they actually censor anything is another question.\n",
    "unlike others, medvedev is an internationally recognized historian.\n",
    "he tells that these people are governmental bureaucrats although some of them have degrees.\n",
    "main point this is a can anyone clarify, please, where the 21 million number for three countries in version 3 comes from?\n",
    "i hope you do not suggest to replace three large sections about kirov assassination by his single paragraph?\n",
    "of course, if there is any sourced information in his text that currently missing, it might be 'added' to current version.\n",
    "perhaps this article should be merged, but this must be properly done.\n",
    "yes, that was one of the reasons why she left big sport so early.\n",
    "of course he did not write about his abuse in reports to kgb superiors.\n",
    "i like book by radzinsky, but this source provides much more details with a lot of references radzinsky works on a bigger 3-volume biography of stalin right now.\n",
    "so, you are very welcome to improve this and 'other' articles on the subject using this book.\n",
    "just remember to focus on content, not contributors.\n",
    "unfortunately, this is not what sources tell as was already discussed above.\n",
    "higher estimates include not only mass murders or executions but also avoidable lives lost due to famine and disease due to confiscation or destruction of property, in addition to deaths in forced labor camps or during forced relocation.\n",
    "the number also does not include civilians executed during russian civil war, etc.\n",
    "unfortunately i do not have time right now, but i will check some sources that describe repression of children in the soviet union to make changes in relevant articles accordingly - as time allows.\n",
    "just wanted to tell you about this case.\n",
    "the grandson of stalin went to court against the radio because the remark was a slander.\n",
    "the radio's lawyer and editor in chef didn't manage to prove in court that children were executed in ussr, simply because there was no proof.\n",
    "i hope you reconsider your position in the light of this info.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0193390",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "masked_text = \"\"\"if they actually censor anything is another question.\n",
    "unlike others, medvedev is an internationally recognized historian.\n",
    "he tells that these people are governmental bureaucrats although some of them have degrees.\n",
    "main point this is a can anyone clarify, please, where the 21 million number for three countries in version 3 comes from?\n",
    "i hope you do not suggest to replace three large sections about kirov assassination by his single paragraph?\n",
    "of course, if there is any sourced information in his text that currently missing, it might be 'added' to current version.\n",
    "perhaps this article should be merged, but this must be properly done.\n",
    "yes, that was one of the reasons why she left big sport so early.\n",
    "of course he did not write about his abuse in reports to kgb superiors.\n",
    "i like book by radzinsky, but this source provides much more details with a lot of references radzinsky works on a bigger 3-volume biography of stalin right now.\n",
    "so, you are very welcome to improve this and 'other' articles on the subject using this book.\n",
    "just remember to focus on content, not contributors.\n",
    "unfortunately, <extra_id_0> what sources tell as was already discussed above.\n",
    "higher estimates include not only mass murders or executions but also avoidable lives lost due to famine and disease due to confiscation or destruction of property, in addition to deaths in forced labor camps or during forced relocation.\n",
    "the number also does not include civilians executed during russian civil war, etc.\n",
    "unfortunately i do not have time right now, but i will check some sources that describe repression of children in the soviet union to make changes in relevant articles accordingly - as time allows.\n",
    "just wanted to tell you about this case.\n",
    "the grandson of stalin went to court against the radio because the remark was a slander.\n",
    "the radio's lawyer and editor in chef didn't manage to prove in court that children were executed in ussr, simply because there was no proof.\n",
    "i hope you reconsider your position in the light of this info.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cc4b1679",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_text = \"\"\"\n",
    "i like book by radzinsky, but this source provides much more details with a lot of references radzinsky works on a bigger 3-volume biography of stalin right now.\n",
    "so, you are very welcome to improve this and 'other' articles on the subject using this book.\n",
    "just remember to focus on content, not contributors.\n",
    "unfortunately, <extra_id_0> what sources tell as was already discussed above.\n",
    "higher estimates include not only mass murders or executions but also avoidable lives lost due to famine and disease due to confiscation or destruction of property, in addition to deaths in forced labor camps or during forced relocation.\n",
    "the number also does not include civilians executed during russian civil war, etc.\n",
    "unfortunately i do not have time right now, but i will check some sources that describe repression of children in the soviet union to make changes in relevant articles accordingly - as time allows.\n",
    "just wanted to tell you about this case.<extra_id_1>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dc8fbaf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is only i am very sorry to hear about it.',\n",
       " 'this is only i am very sorry for this.',\n",
       " 'this is only i am very sorry about that.',\n",
       " 'this is only i am very sorry to hear about this.',\n",
       " 'this is only i am very sorry to hear this.']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_model_generation(model, tokenizer, masked_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb4cc6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 32099,    25,    54,   103,   424, 32098,   533,     5,     1]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f0f51a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2828,  0.0979,  0.1090,  ..., -0.2794,  0.2154,  0.0370],\n",
       "         [ 0.1593,  0.0956, -0.0922,  ...,  0.1854,  0.0993,  0.1548],\n",
       "         [ 0.0076,  0.0090, -0.0092,  ...,  0.0050,  0.0019,  0.0076],\n",
       "         ...,\n",
       "         [-0.0119, -0.1228, -0.0638,  ..., -0.0602, -0.1791, -0.0499],\n",
       "         [-0.0342, -0.0511, -0.1488,  ...,  0.3007, -0.2612, -0.1452],\n",
       "         [ 0.0670,  0.1300, -0.0356,  ..., -0.0100,  0.0690, -0.0531]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.encoder_last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e2cad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.5253e-01,  1.5952e-01, -1.9853e-01,  ...,  1.0275e-01,\n",
       "          -3.6560e-04, -8.1293e-03],\n",
       "         [ 1.8470e-01,  1.0938e-01, -1.7418e-01,  ...,  3.2740e-02,\n",
       "          -5.3408e-04, -5.0218e-02],\n",
       "         [ 2.8846e-01,  2.3717e-01, -7.3225e-02,  ...,  5.7853e-02,\n",
       "          -3.4904e-04, -9.3356e-02],\n",
       "         [ 4.6172e-02,  4.3064e-01, -7.4659e-02,  ...,  5.6104e-02,\n",
       "          -3.3899e-04, -1.2440e-01]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ab43c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "\n",
    "text = \"The results <extra_id_0> beyond doubt.\"  # masked text\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "outputs = model.generate(**inputs, max_new_tokens=10, num_return_sequences=5, \n",
    "                         do_sample=True, top_k=50, temperature=1.0)\n",
    "candidates = tokenizer.batch_decode(outputs, skip_special_tokens=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07cf235d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad> <extra_id_0> cannot be challenged <extra_id_1> . ... <extra_id_2> are',\n",
       " '<pad> <extra_id_0> were <extra_id_1> are <extra_id_2> are <extra_id_3> are <extra_id_4> are',\n",
       " '<pad> <extra_id_0> are <extra_id_1> are, and still are, <extra_id_2>',\n",
       " '<pad> <extra_id_0> of all these evaluations are <extra_id_1> .</s>',\n",
       " '<pad> <extra_id_0> are <extra_id_1> are definitive and are beyond doubt.']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6302be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "def paraphrase_span_t5(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prefix: str,\n",
    "    suffix: str,\n",
    "    num_return_sequences: int = 5,\n",
    "    num_beams: int = None,\n",
    "    max_new_tokens: int = 30,\n",
    "    do_sample: bool = True,\n",
    "    top_k: int = 50,\n",
    "    top_p: float = 0.95,\n",
    "    temperature: float = 0.9,\n",
    "    device: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Use a T5-like model to paraphrase ONLY a missing span between prefix and suffix.\n",
    "    Returns a list of paraphrased spans (not full sentences).\n",
    "\n",
    "    Arguments:\n",
    "      - prefix: text before the span to paraphrase\n",
    "      - suffix: text after the span to paraphrase\n",
    "      - model: a seq2seq model (e.g. T5ForConditionalGeneration) already loaded\n",
    "      - tokenizer: the corresponding tokenizer\n",
    "      - num_return_sequences: how many paraphrases to return\n",
    "      - num_beams: if specified, enables beam search; else uses sampling\n",
    "      - max_new_tokens: max length for generated span\n",
    "      - do_sample, top_k, top_p, temperature: sampling parameters for diversity\n",
    "      - device: optionally the torch device (e.g. 'cuda' or 'cpu'); if None, uses model.device\n",
    "    \"\"\"\n",
    "\n",
    "    # Build the T5‑style infilling input\n",
    "    # Note: space / punctuation is sensitive — ensure prefix/suffix punctuation/spacing is correct\n",
    "    inp = f\"<extra_id_0> {prefix.strip()} <extra_id_1> {suffix.strip()}\"\n",
    "\n",
    "    # Encode\n",
    "    inputs = tokenizer(inp, return_tensors=\"pt\")\n",
    "    if device is not None:\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        model = model.to(device)\n",
    "\n",
    "    # Generation arguments\n",
    "    gen_kwargs = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"num_return_sequences\": num_return_sequences,\n",
    "    }\n",
    "\n",
    "    if num_beams is not None:\n",
    "        gen_kwargs[\"num_beams\"] = num_beams\n",
    "        # When using beams, you might disable sampling\n",
    "        gen_kwargs[\"do_sample\"] = False\n",
    "    else:\n",
    "        gen_kwargs[\"do_sample\"] = do_sample\n",
    "        gen_kwargs[\"top_k\"] = top_k\n",
    "        gen_kwargs[\"top_p\"] = top_p\n",
    "        gen_kwargs[\"temperature\"] = temperature\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        **gen_kwargs\n",
    "    )\n",
    "\n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "\n",
    "    paraphrases = []\n",
    "    for seq in decoded:\n",
    "        # Extract text between <extra_id_0> and <extra_id_1>\n",
    "        # Some outputs may not contain both markers — guard against that\n",
    "        if \"<extra_id_0>\" in seq and \"<extra_id_1>\" in seq:\n",
    "            span = seq.split(\"<extra_id_0>\", 1)[1].split(\"<extra_id_1>\", 1)[0].strip()\n",
    "            # Basic cleanup: drop leading/trailing punctuation if needed\n",
    "            paraphrases.append(span)\n",
    "\n",
    "    return paraphrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "419d4ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_changed_span_tokens(\n",
    "    original_ids: list[int],\n",
    "    new_ids: list[int],\n",
    "    span_token_idxs: list[int],\n",
    "    tokenizer\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a list of decoded tokens that differ between the original\n",
    "    token sequence and the new sequence, but only inside span_token_idxs.\n",
    "    \n",
    "    Example output:\n",
    "        [\"awesome\", \"movie\"]\n",
    "    \"\"\"\n",
    "    changed_token_ids = []\n",
    "\n",
    "    for i in span_token_idxs:\n",
    "        if original_ids[i] != new_ids[i]:\n",
    "            changed_token_ids.append(new_ids[i])\n",
    "\n",
    "    # Decode each token individually (important for subword handling)\n",
    "    decoded = [tokenizer.decode([tid], skip_special_tokens=True).strip()\n",
    "               for tid in changed_token_ids]\n",
    "\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d94f291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"roberta-base\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "def incremental_mlm_paraphrase(\n",
    "    sentence: str,\n",
    "    span_token_idxs: list[int],  # indices of tokens of the target n‑gram (after tokenization)\n",
    "    top_k: int = 10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Try to paraphrase a multi‑token span by masking and filling one token at a time.\n",
    "    Returns a list of candidate paraphrased sentences.\n",
    "    \"\"\"\n",
    "    # Tokenize once\n",
    "    encoding = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    input_ids = encoding.input_ids[0].tolist()\n",
    "    results = [\"\"]  # start with empty, we will build up\n",
    "\n",
    "    # We'll build candidates — but this explodes combinatorially, so careful\n",
    "    sentences = [input_ids]  # list of token id sequences\n",
    "\n",
    "    for idx in span_token_idxs:\n",
    "        new_sentences = []\n",
    "        for seq in sentences:\n",
    "            masked = seq.copy()\n",
    "            masked[idx] = tokenizer.mask_token_id\n",
    "            input_ids_batch = torch.tensor([masked], device=device)\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_ids=input_ids_batch).logits\n",
    "            # get top_k predictions for masked position\n",
    "            probs = torch.softmax(logits[0, idx], dim=-1)\n",
    "            topk_ids = torch.topk(probs, top_k).indices.tolist()\n",
    "            for tid in topk_ids:\n",
    "                new_seq = seq.copy()\n",
    "                new_seq[idx] = tid\n",
    "                new_sentences.append(new_seq)\n",
    "        sentences = new_sentences\n",
    "\n",
    "    # Decode all final candidates\n",
    "    paraphrases = []\n",
    "    for seq in sentences:\n",
    "        changed_tokens = get_changed_span_tokens(input_ids, seq, span_token_idxs, tokenizer)\n",
    "        text = tokenizer.decode(seq, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        paraphrases.append(text)\n",
    "    return paraphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e46775be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Union\n",
    "from transformers import PreTrainedTokenizerFast, PreTrainedTokenizer\n",
    "\n",
    "def find_subtoken_spans(\n",
    "    tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],\n",
    "    text: str,\n",
    "    target: Union[str, List[str]],\n",
    "    *,\n",
    "    normalize_spaces: bool = True\n",
    ") -> List[Tuple[int,int]]:\n",
    "    \"\"\"\n",
    "    Tokenize `text`, then find all spans of tokens matching `target`.\n",
    "    Returns list of (start_idx, end_idx) inclusive token spans.\n",
    "    If target is a string, it will be tokenized by tokenizer first.\n",
    "    \"\"\"\n",
    "    # 1. Tokenize full text (get token list)\n",
    "    encoding = tokenizer(text, return_attention_mask=False, return_tensors=None)\n",
    "    tokens = encoding.tokens()  # list of token strings\n",
    "\n",
    "    # 2. Prepare target token list\n",
    "    if isinstance(target, str):\n",
    "        target_tokens = tokenizer.tokenize(target)\n",
    "    else:\n",
    "        target_tokens = target\n",
    "\n",
    "    if normalize_spaces:\n",
    "        # some tokenizers may produce '▁' or 'Ġ' or '##' prefixes; \n",
    "        # we rely on exact match of token strings.\n",
    "        pass\n",
    "\n",
    "    spans = []\n",
    "    m = len(target_tokens)\n",
    "    if m == 0:\n",
    "        return spans\n",
    "\n",
    "    # 3. Slide over tokens to find matches\n",
    "    for i in range(len(tokens) - m + 1):\n",
    "        if tokens[i:i+m] == target_tokens:\n",
    "            spans.append((i, i+m-1))\n",
    "\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "725978c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_phrase_token_spans(text: str, phrase_pretty: str, tokenizer):\n",
    "    \"\"\"\n",
    "    Find all token spans for phrase_pretty inside text using tokenizer-consistent tokenization.\n",
    "    Works for any tokenizer: BERT, RoBERTa, ALBERT, GPT-like, SentencePiece, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize full text WITH special tokens\n",
    "    enc = tokenizer(text, add_special_tokens=True)\n",
    "    text_tokens = tokenizer.convert_ids_to_tokens(enc[\"input_ids\"])\n",
    "\n",
    "    # Tokenize phrase using model rules\n",
    "    phrase_tokens = tokenizer.tokenize(phrase_pretty)\n",
    "    n = len(phrase_tokens)\n",
    "\n",
    "    spans = []\n",
    "\n",
    "    for i in range(len(text_tokens) - n + 1):\n",
    "        if text_tokens[i:i+n] == phrase_tokens:\n",
    "            spans.append(list(range(i, i+n)))\n",
    "\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "90f921f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\"if they actually censor anything is another question.\n",
    "unlike others, medvedev is an internationally recognized historian.\n",
    "he tells that these people are governmental bureaucrats although some of them have degrees.\n",
    "main point this is a can anyone clarify, please, where the 21 million number for three countries in version 3 comes from?\n",
    "i hope you do not suggest to replace three large sections about kirov assassination by his single paragraph?\n",
    "of course, if there is any sourced information in his text that currently missing, it might be 'added' to current version.\n",
    "perhaps this article should be merged, but this must be properly done.\n",
    "yes, that was one of the reasons why she left big sport so early.\n",
    "of course he did not write about his abuse in reports to kgb superiors.\n",
    "i like book by radzinsky, but this source provides much more details with a lot of references radzinsky works on a bigger 3-volume biography of stalin right now.\n",
    "so, you are very welcome to improve this and 'other' articles on the subject using this book.\n",
    "just remember to focus on content, not contributors.\n",
    "unfortunately, this is not what sources tell as was already discussed above.\n",
    "higher estimates include not only mass murders or executions but also avoidable lives lost due to famine and disease due to confiscation or destruction of property, in addition to deaths in forced labor camps or during forced relocation.\n",
    "the number also does not include civilians executed during russian civil war, etc.\n",
    "unfortunately i do not have time right now, but i will check some sources that describe repression of children in the soviet union to make changes in relevant articles accordingly - as time allows.\n",
    "just wanted to tell you about this case.\n",
    "the grandson of stalin went to court against the radio because the remark was a slander.\n",
    "the radio's lawyer and editor in chef didn't manage to prove in court that children were executed in ussr, simply because there was no proof.\n",
    "i hope you reconsider your position in the light of this info.\"\"\"\n",
    "\n",
    "raw_phrase=\", but this\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "908d2646",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = find_phrase_token_spans(\n",
    "    tokenizer=tokenizer,\n",
    "    text=raw_text,\n",
    "    phrase_pretty=raw_phrase\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5cf63b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[122, 123, 124], [172, 173, 174]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "33a34578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_mlm_paraphrase(\n",
    "    sentence: str,\n",
    "    span_token_idxs: list[int],\n",
    "    original_phrase: str,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    top_k: int = 10\n",
    "):\n",
    "    # Normalize original phrase for comparison\n",
    "    original_phrase = original_phrase.lower().strip()\n",
    "\n",
    "    encoding = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    original_ids = encoding.input_ids[0].tolist()\n",
    "\n",
    "    # Determine anchor positions\n",
    "    before_idx = span_token_idxs[0] - 1\n",
    "    after_idx  = span_token_idxs[-1] + 1\n",
    "\n",
    "    before_token = original_ids[before_idx] if before_idx >= 0 else None\n",
    "    after_token  = original_ids[after_idx]  if after_idx < len(original_ids) else None\n",
    "\n",
    "    # Start from the original token ID sequence\n",
    "    sequences = [original_ids]\n",
    "\n",
    "    # ---- Incremental masked-token replacement ----\n",
    "    for idx in span_token_idxs:\n",
    "        new_sequences = []\n",
    "\n",
    "        for seq in sequences:\n",
    "            masked = seq.copy()\n",
    "            masked[idx] = tokenizer.mask_token_id\n",
    "\n",
    "            input_ids_batch = torch.tensor([masked], device=device)\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_ids=input_ids_batch).logits\n",
    "\n",
    "            probs = torch.softmax(logits[0, idx], dim=-1)\n",
    "            topk_ids = torch.topk(probs, top_k).indices.tolist()\n",
    "\n",
    "            # Each option sprouts a new sequence\n",
    "            for tid in topk_ids:\n",
    "                new_seq = seq.copy()\n",
    "                new_seq[idx] = tid\n",
    "                new_sequences.append(new_seq)\n",
    "\n",
    "        sequences = new_sequences\n",
    "\n",
    "    # ---- Extract paraphrased spans using anchors ----\n",
    "    paraphrases = set()\n",
    "    results = []\n",
    "\n",
    "    for seq in sequences:\n",
    "        # Extract span boundaries via anchor method\n",
    "        start = before_idx + 1 if before_token is not None else 0\n",
    "        end   = after_idx if after_token is not None else len(seq)\n",
    "\n",
    "        span_ids = seq[start:end]\n",
    "\n",
    "        # Decode the phrase\n",
    "        phrase = tokenizer.decode(\n",
    "            span_ids,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        ).lower().strip()\n",
    "\n",
    "        # Skip empty, unchanged, or duplicate phrases\n",
    "        if not phrase or phrase == original_phrase:\n",
    "            continue\n",
    "        if phrase in paraphrases:\n",
    "            continue\n",
    "\n",
    "        paraphrases.add(phrase)\n",
    "\n",
    "        # Token-level decoding\n",
    "        token_list = [\n",
    "            tokenizer.decode([tid], skip_special_tokens=True).strip().lower()\n",
    "            for tid in span_ids\n",
    "        ]\n",
    "\n",
    "        results.append({\n",
    "            \"phrase\": phrase,\n",
    "            \"tokens\": token_list\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "84422914",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"roberta-base\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4947eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "cands = incremental_mlm_paraphrase(raw_text, location[0], raw_phrase, top_k=50, model=model, tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "81ce46f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2305"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d47f7e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'phrase': ', but it', 'tokens': [',', 'but', 'it']},\n",
       " {'phrase': ', but that', 'tokens': [',', 'but', 'that']},\n",
       " {'phrase': ', but everything', 'tokens': [',', 'but', 'everything']},\n",
       " {'phrase': ', but editing', 'tokens': [',', 'but', 'editing']},\n",
       " {'phrase': ', but something', 'tokens': [',', 'but', 'something']},\n",
       " {'phrase': ', but so', 'tokens': [',', 'but', 'so']},\n",
       " {'phrase': ', but they', 'tokens': [',', 'but', 'they']},\n",
       " {'phrase': ', but things', 'tokens': [',', 'but', 'things']},\n",
       " {'phrase': ', but correction', 'tokens': [',', 'but', 'correction']},\n",
       " {'phrase': ', but work', 'tokens': [',', 'but', 'work']},\n",
       " {'phrase': ', but translation', 'tokens': [',', 'but', 'translation']},\n",
       " {'phrase': ', but corrections', 'tokens': [',', 'but', 'corrections']},\n",
       " {'phrase': ', but its', 'tokens': [',', 'but', 'its']},\n",
       " {'phrase': ', but revision', 'tokens': [',', 'but', 'revision']},\n",
       " {'phrase': ', but all', 'tokens': [',', 'but', 'all']},\n",
       " {'phrase': ', but both', 'tokens': [',', 'but', 'both']},\n",
       " {'phrase': ', but nothing', 'tokens': [',', 'but', 'nothing']},\n",
       " {'phrase': ', but attribution', 'tokens': [',', 'but', 'attribution']},\n",
       " {'phrase': ', but moderation', 'tokens': [',', 'but', 'moderation']},\n",
       " {'phrase': ', but thing', 'tokens': [',', 'but', 'thing']},\n",
       " {'phrase': ', but such', 'tokens': [',', 'but', 'such']},\n",
       " {'phrase': ', but change', 'tokens': [',', 'but', 'change']},\n",
       " {'phrase': ', but which', 'tokens': [',', 'but', 'which']},\n",
       " {'phrase': ', but formatting', 'tokens': [',', 'but', 'formatting']},\n",
       " {'phrase': ', but same', 'tokens': [',', 'but', 'same']},\n",
       " {'phrase': ', but research', 'tokens': [',', 'but', 'research']},\n",
       " {'phrase': ', but otherwise', 'tokens': [',', 'but', 'otherwise']},\n",
       " {'phrase': ', but analysis', 'tokens': [',', 'but', 'analysis']},\n",
       " {'phrase': ', but these', 'tokens': [',', 'but', 'these']},\n",
       " {'phrase': ', but copying', 'tokens': [',', 'but', 'copying']},\n",
       " {'phrase': ', but integration', 'tokens': [',', 'but', 'integration']},\n",
       " {'phrase': ', but cleanup', 'tokens': [',', 'but', 'cleanup']},\n",
       " {'phrase': ', but if', 'tokens': [',', 'but', 'if']},\n",
       " {'phrase': ', but changes', 'tokens': [',', 'but', 'changes']},\n",
       " {'phrase': ', but what', 'tokens': [',', 'but', 'what']},\n",
       " {'phrase': ', but investigation', 'tokens': [',', 'but', 'investigation']},\n",
       " {'phrase': ', but not', 'tokens': [',', 'but', 'not']},\n",
       " {'phrase': ', but one', 'tokens': [',', 'but', 'one']},\n",
       " {'phrase': ', but the', 'tokens': [',', 'but', 'the']},\n",
       " {'phrase': ', but also', 'tokens': [',', 'but', 'also']},\n",
       " {'phrase': ', but justice', 'tokens': [',', 'but', 'justice']},\n",
       " {'phrase': ', but check', 'tokens': [',', 'but', 'check']},\n",
       " {'phrase': ', but still', 'tokens': [',', 'but', 'still']},\n",
       " {'phrase': ', but as', 'tokens': [',', 'but', 'as']},\n",
       " {'phrase': ', but checking', 'tokens': [',', 'but', 'checking']},\n",
       " {'phrase': ', but is', 'tokens': [',', 'but', 'is']},\n",
       " {'phrase': ', but censorship', 'tokens': [',', 'but', 'censorship']},\n",
       " {'phrase': ', but search', 'tokens': [',', 'but', 'search']},\n",
       " {'phrase': ', but review', 'tokens': [',', 'but', 'review']},\n",
       " {'phrase': '- but it', 'tokens': ['-', 'but', 'it']},\n",
       " {'phrase': '- but this', 'tokens': ['-', 'but', 'this']},\n",
       " {'phrase': '- but that', 'tokens': ['-', 'but', 'that']},\n",
       " {'phrase': '- but everything', 'tokens': ['-', 'but', 'everything']},\n",
       " {'phrase': '- but something', 'tokens': ['-', 'but', 'something']},\n",
       " {'phrase': '- but editing', 'tokens': ['-', 'but', 'editing']},\n",
       " {'phrase': '- but so', 'tokens': ['-', 'but', 'so']},\n",
       " {'phrase': '- but they', 'tokens': ['-', 'but', 'they']},\n",
       " {'phrase': '- but correction', 'tokens': ['-', 'but', 'correction']},\n",
       " {'phrase': '- but things', 'tokens': ['-', 'but', 'things']},\n",
       " {'phrase': '- but work', 'tokens': ['-', 'but', 'work']},\n",
       " {'phrase': '- but its', 'tokens': ['-', 'but', 'its']},\n",
       " {'phrase': '- but translation', 'tokens': ['-', 'but', 'translation']},\n",
       " {'phrase': '- but corrections', 'tokens': ['-', 'but', 'corrections']},\n",
       " {'phrase': '- but revision', 'tokens': ['-', 'but', 'revision']},\n",
       " {'phrase': '- but both', 'tokens': ['-', 'but', 'both']},\n",
       " {'phrase': '- but all', 'tokens': ['-', 'but', 'all']},\n",
       " {'phrase': '- but such', 'tokens': ['-', 'but', 'such']},\n",
       " {'phrase': '- but moderation', 'tokens': ['-', 'but', 'moderation']},\n",
       " {'phrase': '- but attribution', 'tokens': ['-', 'but', 'attribution']},\n",
       " {'phrase': '- but change', 'tokens': ['-', 'but', 'change']},\n",
       " {'phrase': '- but which', 'tokens': ['-', 'but', 'which']},\n",
       " {'phrase': '- but thing', 'tokens': ['-', 'but', 'thing']},\n",
       " {'phrase': '- but nothing', 'tokens': ['-', 'but', 'nothing']},\n",
       " {'phrase': '- but these', 'tokens': ['-', 'but', 'these']},\n",
       " {'phrase': '- but if', 'tokens': ['-', 'but', 'if']},\n",
       " {'phrase': '- but integration', 'tokens': ['-', 'but', 'integration']},\n",
       " {'phrase': '- but same', 'tokens': ['-', 'but', 'same']},\n",
       " {'phrase': '- but what', 'tokens': ['-', 'but', 'what']},\n",
       " {'phrase': '- but also', 'tokens': ['-', 'but', 'also']},\n",
       " {'phrase': '- but formatting', 'tokens': ['-', 'but', 'formatting']},\n",
       " {'phrase': '- but copying', 'tokens': ['-', 'but', 'copying']},\n",
       " {'phrase': '- but research', 'tokens': ['-', 'but', 'research']},\n",
       " {'phrase': '- but one', 'tokens': ['-', 'but', 'one']},\n",
       " {'phrase': '- but the', 'tokens': ['-', 'but', 'the']},\n",
       " {'phrase': '- but analysis', 'tokens': ['-', 'but', 'analysis']},\n",
       " {'phrase': '- but otherwise', 'tokens': ['-', 'but', 'otherwise']},\n",
       " {'phrase': '- but as', 'tokens': ['-', 'but', 'as']},\n",
       " {'phrase': '- but changes', 'tokens': ['-', 'but', 'changes']},\n",
       " {'phrase': '- but merging', 'tokens': ['-', 'but', 'merging']},\n",
       " {'phrase': '- but is', 'tokens': ['-', 'but', 'is']},\n",
       " {'phrase': '- but,', 'tokens': ['-', 'but', ',']},\n",
       " {'phrase': '- but cleanup', 'tokens': ['-', 'but', 'cleanup']},\n",
       " {'phrase': '- but not', 'tokens': ['-', 'but', 'not']},\n",
       " {'phrase': '- but check', 'tokens': ['-', 'but', 'check']},\n",
       " {'phrase': '- but justice', 'tokens': ['-', 'but', 'justice']},\n",
       " {'phrase': '- but investigation', 'tokens': ['-', 'but', 'investigation']},\n",
       " {'phrase': '- but now', 'tokens': ['-', 'but', 'now']},\n",
       " {'phrase': '- but still', 'tokens': ['-', 'but', 'still']},\n",
       " {'phrase': '- but checking', 'tokens': ['-', 'but', 'checking']},\n",
       " {'phrase': '; but it', 'tokens': [';', 'but', 'it']},\n",
       " {'phrase': '; but this', 'tokens': [';', 'but', 'this']},\n",
       " {'phrase': '; but that', 'tokens': [';', 'but', 'that']},\n",
       " {'phrase': '; but everything', 'tokens': [';', 'but', 'everything']},\n",
       " {'phrase': '; but editing', 'tokens': [';', 'but', 'editing']},\n",
       " {'phrase': '; but something', 'tokens': [';', 'but', 'something']},\n",
       " {'phrase': '; but so', 'tokens': [';', 'but', 'so']},\n",
       " {'phrase': '; but they', 'tokens': [';', 'but', 'they']},\n",
       " {'phrase': '; but correction', 'tokens': [';', 'but', 'correction']},\n",
       " {'phrase': '; but things', 'tokens': [';', 'but', 'things']},\n",
       " {'phrase': '; but corrections', 'tokens': [';', 'but', 'corrections']},\n",
       " {'phrase': '; but work', 'tokens': [';', 'but', 'work']},\n",
       " {'phrase': '; but translation', 'tokens': [';', 'but', 'translation']},\n",
       " {'phrase': '; but its', 'tokens': [';', 'but', 'its']},\n",
       " {'phrase': '; but revision', 'tokens': [';', 'but', 'revision']},\n",
       " {'phrase': '; but both', 'tokens': [';', 'but', 'both']},\n",
       " {'phrase': '; but all', 'tokens': [';', 'but', 'all']},\n",
       " {'phrase': '; but attribution', 'tokens': [';', 'but', 'attribution']},\n",
       " {'phrase': '; but moderation', 'tokens': [';', 'but', 'moderation']},\n",
       " {'phrase': '; but such', 'tokens': [';', 'but', 'such']},\n",
       " {'phrase': '; but thing', 'tokens': [';', 'but', 'thing']},\n",
       " {'phrase': '; but nothing', 'tokens': [';', 'but', 'nothing']},\n",
       " {'phrase': '; but change', 'tokens': [';', 'but', 'change']},\n",
       " {'phrase': '; but these', 'tokens': [';', 'but', 'these']},\n",
       " {'phrase': '; but same', 'tokens': [';', 'but', 'same']},\n",
       " {'phrase': '; but which', 'tokens': [';', 'but', 'which']},\n",
       " {'phrase': '; but integration', 'tokens': [';', 'but', 'integration']},\n",
       " {'phrase': '; but copying', 'tokens': [';', 'but', 'copying']},\n",
       " {'phrase': '; but formatting', 'tokens': [';', 'but', 'formatting']},\n",
       " {'phrase': '; but analysis', 'tokens': [';', 'but', 'analysis']},\n",
       " {'phrase': '; but research', 'tokens': [';', 'but', 'research']},\n",
       " {'phrase': '; but cleanup', 'tokens': [';', 'but', 'cleanup']},\n",
       " {'phrase': '; but merging', 'tokens': [';', 'but', 'merging']},\n",
       " {'phrase': '; but justice', 'tokens': [';', 'but', 'justice']},\n",
       " {'phrase': '; but changes', 'tokens': [';', 'but', 'changes']},\n",
       " {'phrase': '; but investigation', 'tokens': [';', 'but', 'investigation']},\n",
       " {'phrase': '; but one', 'tokens': [';', 'but', 'one']},\n",
       " {'phrase': '; but what', 'tokens': [';', 'but', 'what']},\n",
       " {'phrase': '; but the', 'tokens': [';', 'but', 'the']},\n",
       " {'phrase': '; but otherwise', 'tokens': [';', 'but', 'otherwise']},\n",
       " {'phrase': '; but if', 'tokens': [';', 'but', 'if']},\n",
       " {'phrase': '; but check', 'tokens': [';', 'but', 'check']},\n",
       " {'phrase': '; but checking', 'tokens': [';', 'but', 'checking']},\n",
       " {'phrase': '; but,', 'tokens': [';', 'but', ',']},\n",
       " {'phrase': '; but replication', 'tokens': [';', 'but', 'replication']},\n",
       " {'phrase': '; but censorship', 'tokens': [';', 'but', 'censorship']},\n",
       " {'phrase': '; but also', 'tokens': [';', 'but', 'also']},\n",
       " {'phrase': '; but insertion', 'tokens': [';', 'but', 'insertion']},\n",
       " {'phrase': '; but as', 'tokens': [';', 'but', 'as']},\n",
       " {'phrase': '; but duplication', 'tokens': [';', 'but', 'duplication']},\n",
       " {'phrase': 'together but it', 'tokens': ['together', 'but', 'it']},\n",
       " {'phrase': 'together but this', 'tokens': ['together', 'but', 'this']},\n",
       " {'phrase': 'together but that', 'tokens': ['together', 'but', 'that']},\n",
       " {'phrase': 'together but everything',\n",
       "  'tokens': ['together', 'but', 'everything']},\n",
       " {'phrase': 'together but they', 'tokens': ['together', 'but', 'they']},\n",
       " {'phrase': 'together but editing', 'tokens': ['together', 'but', 'editing']},\n",
       " {'phrase': 'together but something',\n",
       "  'tokens': ['together', 'but', 'something']},\n",
       " {'phrase': 'together but both', 'tokens': ['together', 'but', 'both']},\n",
       " {'phrase': 'together but so', 'tokens': ['together', 'but', 'so']},\n",
       " {'phrase': 'together but translation',\n",
       "  'tokens': ['together', 'but', 'translation']},\n",
       " {'phrase': 'together but things', 'tokens': ['together', 'but', 'things']},\n",
       " {'phrase': 'together but all', 'tokens': ['together', 'but', 'all']},\n",
       " {'phrase': 'together but work', 'tokens': ['together', 'but', 'work']},\n",
       " {'phrase': 'together but attribution',\n",
       "  'tokens': ['together', 'but', 'attribution']},\n",
       " {'phrase': 'together but nothing', 'tokens': ['together', 'but', 'nothing']},\n",
       " {'phrase': 'together but correction',\n",
       "  'tokens': ['together', 'but', 'correction']},\n",
       " {'phrase': 'together but these', 'tokens': ['together', 'but', 'these']},\n",
       " {'phrase': 'together but revision',\n",
       "  'tokens': ['together', 'but', 'revision']},\n",
       " {'phrase': 'together but thing', 'tokens': ['together', 'but', 'thing']},\n",
       " {'phrase': 'together but formatting',\n",
       "  'tokens': ['together', 'but', 'formatting']},\n",
       " {'phrase': 'together but which', 'tokens': ['together', 'but', 'which']},\n",
       " {'phrase': 'together but corrections',\n",
       "  'tokens': ['together', 'but', 'corrections']},\n",
       " {'phrase': 'together but moderation',\n",
       "  'tokens': ['together', 'but', 'moderation']},\n",
       " {'phrase': 'together but its', 'tokens': ['together', 'but', 'its']},\n",
       " {'phrase': 'together but integration',\n",
       "  'tokens': ['together', 'but', 'integration']},\n",
       " {'phrase': 'together but what', 'tokens': ['together', 'but', 'what']},\n",
       " {'phrase': 'together but same', 'tokens': ['together', 'but', 'same']},\n",
       " {'phrase': 'together but change', 'tokens': ['together', 'but', 'change']},\n",
       " {'phrase': 'together but otherwise',\n",
       "  'tokens': ['together', 'but', 'otherwise']},\n",
       " {'phrase': 'together but not', 'tokens': ['together', 'but', 'not']},\n",
       " {'phrase': 'together but one', 'tokens': ['together', 'but', 'one']},\n",
       " {'phrase': 'together but also', 'tokens': ['together', 'but', 'also']},\n",
       " {'phrase': 'together but such', 'tokens': ['together', 'but', 'such']},\n",
       " {'phrase': 'together but copying', 'tokens': ['together', 'but', 'copying']},\n",
       " {'phrase': 'together but still', 'tokens': ['together', 'but', 'still']},\n",
       " {'phrase': 'together but analysis',\n",
       "  'tokens': ['together', 'but', 'analysis']},\n",
       " {'phrase': 'together but now', 'tokens': ['together', 'but', 'now']},\n",
       " {'phrase': 'together but linking', 'tokens': ['together', 'but', 'linking']},\n",
       " {'phrase': 'together but the', 'tokens': ['together', 'but', 'the']},\n",
       " {'phrase': 'together but changes', 'tokens': ['together', 'but', 'changes']},\n",
       " {'phrase': 'together but as', 'tokens': ['together', 'but', 'as']},\n",
       " {'phrase': 'together but merging', 'tokens': ['together', 'but', 'merging']},\n",
       " {'phrase': 'together but translations',\n",
       "  'tokens': ['together', 'but', 'translations']},\n",
       " {'phrase': 'together but if', 'tokens': ['together', 'but', 'if']},\n",
       " {'phrase': 'together but research',\n",
       "  'tokens': ['together', 'but', 'research']},\n",
       " {'phrase': 'together but investigation',\n",
       "  'tokens': ['together', 'but', 'investigation']},\n",
       " {'phrase': 'together but justice', 'tokens': ['together', 'but', 'justice']},\n",
       " {'phrase': 'together but cleanup', 'tokens': ['together', 'but', 'cleanup']},\n",
       " {'phrase': 'together but communication',\n",
       "  'tokens': ['together', 'but', 'communication']},\n",
       " {'phrase': 'together but neither', 'tokens': ['together', 'but', 'neither']},\n",
       " {'phrase': '. but it', 'tokens': ['.', 'but', 'it']},\n",
       " {'phrase': '. but this', 'tokens': ['.', 'but', 'this']},\n",
       " {'phrase': '. but that', 'tokens': ['.', 'but', 'that']},\n",
       " {'phrase': '. but everything', 'tokens': ['.', 'but', 'everything']},\n",
       " {'phrase': '. but something', 'tokens': ['.', 'but', 'something']},\n",
       " {'phrase': '. but editing', 'tokens': ['.', 'but', 'editing']},\n",
       " {'phrase': '. but they', 'tokens': ['.', 'but', 'they']},\n",
       " {'phrase': '. but things', 'tokens': ['.', 'but', 'things']},\n",
       " {'phrase': '. but so', 'tokens': ['.', 'but', 'so']},\n",
       " {'phrase': '. but its', 'tokens': ['.', 'but', 'its']},\n",
       " {'phrase': '. but both', 'tokens': ['.', 'but', 'both']},\n",
       " {'phrase': '. but correction', 'tokens': ['.', 'but', 'correction']},\n",
       " {'phrase': '. but all', 'tokens': ['.', 'but', 'all']},\n",
       " {'phrase': '. but work', 'tokens': ['.', 'but', 'work']},\n",
       " {'phrase': '. butit', 'tokens': ['.', 'but', 'it']},\n",
       " {'phrase': '. but translation', 'tokens': ['.', 'but', 'translation']},\n",
       " {'phrase': '. but corrections', 'tokens': ['.', 'but', 'corrections']},\n",
       " {'phrase': '. but nothing', 'tokens': ['.', 'but', 'nothing']},\n",
       " {'phrase': '. but thing', 'tokens': ['.', 'but', 'thing']},\n",
       " {'phrase': '. but revision', 'tokens': ['.', 'but', 'revision']},\n",
       " {'phrase': '. but attribution', 'tokens': ['.', 'but', 'attribution']},\n",
       " {'phrase': '. but such', 'tokens': ['.', 'but', 'such']},\n",
       " {'phrase': '. butthis', 'tokens': ['.', 'but', 'this']},\n",
       " {'phrase': '. but moderation', 'tokens': ['.', 'but', 'moderation']},\n",
       " {'phrase': '. but change', 'tokens': ['.', 'but', 'change']},\n",
       " {'phrase': '. but which', 'tokens': ['.', 'but', 'which']},\n",
       " {'phrase': '. but these', 'tokens': ['.', 'but', 'these']},\n",
       " {'phrase': '. but,', 'tokens': ['.', 'but', ',']},\n",
       " {'phrase': '. but what', 'tokens': ['.', 'but', 'what']},\n",
       " {'phrase': '. but if', 'tokens': ['.', 'but', 'if']},\n",
       " {'phrase': '. but also', 'tokens': ['.', 'but', 'also']},\n",
       " {'phrase': '. but research', 'tokens': ['.', 'but', 'research']},\n",
       " {'phrase': '. but formatting', 'tokens': ['.', 'but', 'formatting']},\n",
       " {'phrase': '. but same', 'tokens': ['.', 'but', 'same']},\n",
       " {'phrase': '. but otherwise', 'tokens': ['.', 'but', 'otherwise']},\n",
       " {'phrase': '. but changes', 'tokens': ['.', 'but', 'changes']},\n",
       " {'phrase': '. but analysis', 'tokens': ['.', 'but', 'analysis']},\n",
       " {'phrase': '. but is', 'tokens': ['.', 'but', 'is']},\n",
       " {'phrase': '. but integration', 'tokens': ['.', 'but', 'integration']},\n",
       " {'phrase': '. but investigation', 'tokens': ['.', 'but', 'investigation']},\n",
       " {'phrase': '. but cleanup', 'tokens': ['.', 'but', 'cleanup']},\n",
       " {'phrase': '. but one', 'tokens': ['.', 'but', 'one']},\n",
       " {'phrase': '. but the', 'tokens': ['.', 'but', 'the']},\n",
       " {'phrase': '. but justice', 'tokens': ['.', 'but', 'justice']},\n",
       " {'phrase': '. but copying', 'tokens': ['.', 'but', 'copying']},\n",
       " {'phrase': '. but still', 'tokens': ['.', 'but', 'still']},\n",
       " {'phrase': '. but check', 'tokens': ['.', 'but', 'check']},\n",
       " {'phrase': '. but anything', 'tokens': ['.', 'but', 'anything']},\n",
       " {'phrase': '. but whatever', 'tokens': ['.', 'but', 'whatever']},\n",
       " {'phrase': '. but as', 'tokens': ['.', 'but', 'as']},\n",
       " {'phrase': '... but it', 'tokens': ['...', 'but', 'it']},\n",
       " {'phrase': '... but this', 'tokens': ['...', 'but', 'this']},\n",
       " {'phrase': '... but that', 'tokens': ['...', 'but', 'that']},\n",
       " {'phrase': '... but something', 'tokens': ['...', 'but', 'something']},\n",
       " {'phrase': '... but everything', 'tokens': ['...', 'but', 'everything']},\n",
       " {'phrase': '... but editing', 'tokens': ['...', 'but', 'editing']},\n",
       " {'phrase': '... but they', 'tokens': ['...', 'but', 'they']},\n",
       " {'phrase': '... but things', 'tokens': ['...', 'but', 'things']},\n",
       " {'phrase': '... but so', 'tokens': ['...', 'but', 'so']},\n",
       " {'phrase': '... but correction', 'tokens': ['...', 'but', 'correction']},\n",
       " {'phrase': '... but translation', 'tokens': ['...', 'but', 'translation']},\n",
       " {'phrase': '... but work', 'tokens': ['...', 'but', 'work']},\n",
       " {'phrase': '... but its', 'tokens': ['...', 'but', 'its']},\n",
       " {'phrase': '... but corrections', 'tokens': ['...', 'but', 'corrections']},\n",
       " {'phrase': '... but revision', 'tokens': ['...', 'but', 'revision']},\n",
       " {'phrase': '... but both', 'tokens': ['...', 'but', 'both']},\n",
       " {'phrase': '... but moderation', 'tokens': ['...', 'but', 'moderation']},\n",
       " {'phrase': '... but all', 'tokens': ['...', 'but', 'all']},\n",
       " {'phrase': '... but thing', 'tokens': ['...', 'but', 'thing']},\n",
       " {'phrase': '... but attribution', 'tokens': ['...', 'but', 'attribution']},\n",
       " {'phrase': '... but change', 'tokens': ['...', 'but', 'change']},\n",
       " {'phrase': '... but such', 'tokens': ['...', 'but', 'such']},\n",
       " {'phrase': '... but nothing', 'tokens': ['...', 'but', 'nothing']},\n",
       " {'phrase': '... but which', 'tokens': ['...', 'but', 'which']},\n",
       " {'phrase': '... but what', 'tokens': ['...', 'but', 'what']},\n",
       " {'phrase': '... but research', 'tokens': ['...', 'but', 'research']},\n",
       " {'phrase': '... but integration', 'tokens': ['...', 'but', 'integration']},\n",
       " {'phrase': '... but formatting', 'tokens': ['...', 'but', 'formatting']},\n",
       " {'phrase': '... but these', 'tokens': ['...', 'but', 'these']},\n",
       " {'phrase': '... but same', 'tokens': ['...', 'but', 'same']},\n",
       " {'phrase': '... but if', 'tokens': ['...', 'but', 'if']},\n",
       " {'phrase': '... but cleanup', 'tokens': ['...', 'but', 'cleanup']},\n",
       " {'phrase': '... but analysis', 'tokens': ['...', 'but', 'analysis']},\n",
       " {'phrase': '... but justice', 'tokens': ['...', 'but', 'justice']},\n",
       " {'phrase': '... but copying', 'tokens': ['...', 'but', 'copying']},\n",
       " {'phrase': '... but investigation',\n",
       "  'tokens': ['...', 'but', 'investigation']},\n",
       " {'phrase': '... but one', 'tokens': ['...', 'but', 'one']},\n",
       " {'phrase': '... but changes', 'tokens': ['...', 'but', 'changes']},\n",
       " {'phrase': '... but also', 'tokens': ['...', 'but', 'also']},\n",
       " {'phrase': '... but the', 'tokens': ['...', 'but', 'the']},\n",
       " {'phrase': '... but,', 'tokens': ['...', 'but', ',']},\n",
       " {'phrase': '... but merging', 'tokens': ['...', 'but', 'merging']},\n",
       " {'phrase': '... but check', 'tokens': ['...', 'but', 'check']},\n",
       " {'phrase': '... but otherwise', 'tokens': ['...', 'but', 'otherwise']},\n",
       " {'phrase': '... but still', 'tokens': ['...', 'but', 'still']},\n",
       " {'phrase': '... but as', 'tokens': ['...', 'but', 'as']},\n",
       " {'phrase': '... but is', 'tokens': ['...', 'but', 'is']},\n",
       " {'phrase': '... but censorship', 'tokens': ['...', 'but', 'censorship']},\n",
       " {'phrase': '... but search', 'tokens': ['...', 'but', 'search']},\n",
       " {'phrase': '... but checking', 'tokens': ['...', 'but', 'checking']},\n",
       " {'phrase': ', but now', 'tokens': [',', 'but', 'now']},\n",
       " {'phrase': ', but anything', 'tokens': [',', 'but', 'anything']},\n",
       " {'phrase': ', but merging', 'tokens': [',', 'but', 'merging']},\n",
       " {'phrase': '– but it', 'tokens': ['–', 'but', 'it']},\n",
       " {'phrase': '– but this', 'tokens': ['–', 'but', 'this']},\n",
       " {'phrase': '– but that', 'tokens': ['–', 'but', 'that']},\n",
       " {'phrase': '– but everything', 'tokens': ['–', 'but', 'everything']},\n",
       " {'phrase': '– but something', 'tokens': ['–', 'but', 'something']},\n",
       " {'phrase': '– but editing', 'tokens': ['–', 'but', 'editing']},\n",
       " {'phrase': '– but so', 'tokens': ['–', 'but', 'so']},\n",
       " {'phrase': '– but they', 'tokens': ['–', 'but', 'they']},\n",
       " {'phrase': '– but correction', 'tokens': ['–', 'but', 'correction']},\n",
       " {'phrase': '– but things', 'tokens': ['–', 'but', 'things']},\n",
       " {'phrase': '– but its', 'tokens': ['–', 'but', 'its']},\n",
       " {'phrase': '– but work', 'tokens': ['–', 'but', 'work']},\n",
       " {'phrase': '– but both', 'tokens': ['–', 'but', 'both']},\n",
       " {'phrase': '– but translation', 'tokens': ['–', 'but', 'translation']},\n",
       " {'phrase': '– but all', 'tokens': ['–', 'but', 'all']},\n",
       " {'phrase': '– but corrections', 'tokens': ['–', 'but', 'corrections']},\n",
       " {'phrase': '– but revision', 'tokens': ['–', 'but', 'revision']},\n",
       " {'phrase': '– but such', 'tokens': ['–', 'but', 'such']},\n",
       " {'phrase': '– but which', 'tokens': ['–', 'but', 'which']},\n",
       " {'phrase': '– but attribution', 'tokens': ['–', 'but', 'attribution']},\n",
       " {'phrase': '– but moderation', 'tokens': ['–', 'but', 'moderation']},\n",
       " {'phrase': '– but change', 'tokens': ['–', 'but', 'change']},\n",
       " {'phrase': '– but thing', 'tokens': ['–', 'but', 'thing']},\n",
       " {'phrase': '– but nothing', 'tokens': ['–', 'but', 'nothing']},\n",
       " {'phrase': '– but these', 'tokens': ['–', 'but', 'these']},\n",
       " {'phrase': '– but integration', 'tokens': ['–', 'but', 'integration']},\n",
       " {'phrase': '– but if', 'tokens': ['–', 'but', 'if']},\n",
       " {'phrase': '– but same', 'tokens': ['–', 'but', 'same']},\n",
       " {'phrase': '– but also', 'tokens': ['–', 'but', 'also']},\n",
       " {'phrase': '– but what', 'tokens': ['–', 'but', 'what']},\n",
       " {'phrase': '– but one', 'tokens': ['–', 'but', 'one']},\n",
       " {'phrase': '– but formatting', 'tokens': ['–', 'but', 'formatting']},\n",
       " {'phrase': '– but the', 'tokens': ['–', 'but', 'the']},\n",
       " {'phrase': '– but research', 'tokens': ['–', 'but', 'research']},\n",
       " {'phrase': '– but analysis', 'tokens': ['–', 'but', 'analysis']},\n",
       " {'phrase': '– but copying', 'tokens': ['–', 'but', 'copying']},\n",
       " {'phrase': '– but not', 'tokens': ['–', 'but', 'not']},\n",
       " {'phrase': '– but as', 'tokens': ['–', 'but', 'as']},\n",
       " {'phrase': '– but otherwise', 'tokens': ['–', 'but', 'otherwise']},\n",
       " {'phrase': '– but merging', 'tokens': ['–', 'but', 'merging']},\n",
       " {'phrase': '– but,', 'tokens': ['–', 'but', ',']},\n",
       " {'phrase': '– but changes', 'tokens': ['–', 'but', 'changes']},\n",
       " {'phrase': '– but justice', 'tokens': ['–', 'but', 'justice']},\n",
       " {'phrase': '– but is', 'tokens': ['–', 'but', 'is']},\n",
       " {'phrase': '– but cleanup', 'tokens': ['–', 'but', 'cleanup']},\n",
       " {'phrase': '– but investigation', 'tokens': ['–', 'but', 'investigation']},\n",
       " {'phrase': '– but check', 'tokens': ['–', 'but', 'check']},\n",
       " {'phrase': '– but now', 'tokens': ['–', 'but', 'now']},\n",
       " {'phrase': '– but still', 'tokens': ['–', 'but', 'still']},\n",
       " {'phrase': '– but censorship', 'tokens': ['–', 'but', 'censorship']},\n",
       " {'phrase': 'again but it', 'tokens': ['again', 'but', 'it']},\n",
       " {'phrase': 'again but this', 'tokens': ['again', 'but', 'this']},\n",
       " {'phrase': 'again but that', 'tokens': ['again', 'but', 'that']},\n",
       " {'phrase': 'again but editing', 'tokens': ['again', 'but', 'editing']},\n",
       " {'phrase': 'again but everything', 'tokens': ['again', 'but', 'everything']},\n",
       " {'phrase': 'again but something', 'tokens': ['again', 'but', 'something']},\n",
       " {'phrase': 'again but correction', 'tokens': ['again', 'but', 'correction']},\n",
       " {'phrase': 'again but they', 'tokens': ['again', 'but', 'they']},\n",
       " {'phrase': 'again but so', 'tokens': ['again', 'but', 'so']},\n",
       " {'phrase': 'again but translation',\n",
       "  'tokens': ['again', 'but', 'translation']},\n",
       " {'phrase': 'again but work', 'tokens': ['again', 'but', 'work']},\n",
       " {'phrase': 'again but revision', 'tokens': ['again', 'but', 'revision']},\n",
       " {'phrase': 'again but things', 'tokens': ['again', 'but', 'things']},\n",
       " {'phrase': 'again but corrections',\n",
       "  'tokens': ['again', 'but', 'corrections']},\n",
       " {'phrase': 'again but its', 'tokens': ['again', 'but', 'its']},\n",
       " {'phrase': 'again but all', 'tokens': ['again', 'but', 'all']},\n",
       " {'phrase': 'again but change', 'tokens': ['again', 'but', 'change']},\n",
       " {'phrase': 'again but formatting', 'tokens': ['again', 'but', 'formatting']},\n",
       " {'phrase': 'again but thing', 'tokens': ['again', 'but', 'thing']},\n",
       " {'phrase': 'again but moderation', 'tokens': ['again', 'but', 'moderation']},\n",
       " {'phrase': 'again but which', 'tokens': ['again', 'but', 'which']},\n",
       " {'phrase': 'again but attribution',\n",
       "  'tokens': ['again', 'but', 'attribution']},\n",
       " {'phrase': 'again but both', 'tokens': ['again', 'but', 'both']},\n",
       " {'phrase': 'again but same', 'tokens': ['again', 'but', 'same']},\n",
       " {'phrase': 'again but changes', 'tokens': ['again', 'but', 'changes']},\n",
       " {'phrase': 'again but nothing', 'tokens': ['again', 'but', 'nothing']},\n",
       " {'phrase': 'again but still', 'tokens': ['again', 'but', 'still']},\n",
       " {'phrase': 'again but such', 'tokens': ['again', 'but', 'such']},\n",
       " {'phrase': 'again but now', 'tokens': ['again', 'but', 'now']},\n",
       " {'phrase': 'again but what', 'tokens': ['again', 'but', 'what']},\n",
       " {'phrase': 'again but also', 'tokens': ['again', 'but', 'also']},\n",
       " {'phrase': 'again but otherwise', 'tokens': ['again', 'but', 'otherwise']},\n",
       " {'phrase': 'again but if', 'tokens': ['again', 'but', 'if']},\n",
       " {'phrase': 'again but cleanup', 'tokens': ['again', 'but', 'cleanup']},\n",
       " {'phrase': 'again but as', 'tokens': ['again', 'but', 'as']},\n",
       " {'phrase': 'again but these', 'tokens': ['again', 'but', 'these']},\n",
       " {'phrase': 'again but justice', 'tokens': ['again', 'but', 'justice']},\n",
       " {'phrase': 'again but analysis', 'tokens': ['again', 'but', 'analysis']},\n",
       " {'phrase': 'again but copying', 'tokens': ['again', 'but', 'copying']},\n",
       " {'phrase': 'again but the', 'tokens': ['again', 'but', 'the']},\n",
       " {'phrase': 'again but research', 'tokens': ['again', 'but', 'research']},\n",
       " {'phrase': 'again but investigation',\n",
       "  'tokens': ['again', 'but', 'investigation']},\n",
       " {'phrase': 'again but is', 'tokens': ['again', 'but', 'is']},\n",
       " {'phrase': 'again but check', 'tokens': ['again', 'but', 'check']},\n",
       " {'phrase': 'again but revisions', 'tokens': ['again', 'but', 'revisions']},\n",
       " {'phrase': 'again but one', 'tokens': ['again', 'but', 'one']},\n",
       " {'phrase': 'again but review', 'tokens': ['again', 'but', 'review']},\n",
       " {'phrase': 'again but updating', 'tokens': ['again', 'but', 'updating']},\n",
       " {'phrase': 'again but restoration',\n",
       "  'tokens': ['again', 'but', 'restoration']},\n",
       " {'phrase': 'again but search', 'tokens': ['again', 'but', 'search']},\n",
       " {'phrase': 'too but it', 'tokens': ['too', 'but', 'it']},\n",
       " {'phrase': 'too but this', 'tokens': ['too', 'but', 'this']},\n",
       " {'phrase': 'too but that', 'tokens': ['too', 'but', 'that']},\n",
       " {'phrase': 'too but everything', 'tokens': ['too', 'but', 'everything']},\n",
       " {'phrase': 'too but something', 'tokens': ['too', 'but', 'something']},\n",
       " {'phrase': 'too but editing', 'tokens': ['too', 'but', 'editing']},\n",
       " {'phrase': 'too but they', 'tokens': ['too', 'but', 'they']},\n",
       " {'phrase': 'too but both', 'tokens': ['too', 'but', 'both']},\n",
       " {'phrase': 'too but things', 'tokens': ['too', 'but', 'things']},\n",
       " {'phrase': 'too but all', 'tokens': ['too', 'but', 'all']},\n",
       " {'phrase': 'too but translation', 'tokens': ['too', 'but', 'translation']},\n",
       " {'phrase': 'too but work', 'tokens': ['too', 'but', 'work']},\n",
       " {'phrase': 'too but its', 'tokens': ['too', 'but', 'its']},\n",
       " {'phrase': 'too but so', 'tokens': ['too', 'but', 'so']},\n",
       " {'phrase': 'too but correction', 'tokens': ['too', 'but', 'correction']},\n",
       " {'phrase': 'too but revision', 'tokens': ['too', 'but', 'revision']},\n",
       " {'phrase': 'too but which', 'tokens': ['too', 'but', 'which']},\n",
       " {'phrase': 'too but nothing', 'tokens': ['too', 'but', 'nothing']},\n",
       " {'phrase': 'too but moderation', 'tokens': ['too', 'but', 'moderation']},\n",
       " {'phrase': 'too but attribution', 'tokens': ['too', 'but', 'attribution']},\n",
       " {'phrase': 'too but thing', 'tokens': ['too', 'but', 'thing']},\n",
       " {'phrase': 'too but formatting', 'tokens': ['too', 'but', 'formatting']},\n",
       " {'phrase': 'too but these', 'tokens': ['too', 'but', 'these']},\n",
       " {'phrase': 'too but corrections', 'tokens': ['too', 'but', 'corrections']},\n",
       " {'phrase': 'too but change', 'tokens': ['too', 'but', 'change']},\n",
       " {'phrase': 'too but still', 'tokens': ['too', 'but', 'still']},\n",
       " {'phrase': 'too but also', 'tokens': ['too', 'but', 'also']},\n",
       " {'phrase': 'too but same', 'tokens': ['too', 'but', 'same']},\n",
       " {'phrase': 'too but what', 'tokens': ['too', 'but', 'what']},\n",
       " {'phrase': 'too but changes', 'tokens': ['too', 'but', 'changes']},\n",
       " {'phrase': 'too but otherwise', 'tokens': ['too', 'but', 'otherwise']},\n",
       " {'phrase': 'too but one', 'tokens': ['too', 'but', 'one']},\n",
       " {'phrase': 'too but if', 'tokens': ['too', 'but', 'if']},\n",
       " {'phrase': 'too but such', 'tokens': ['too', 'but', 'such']},\n",
       " {'phrase': 'too but integration', 'tokens': ['too', 'but', 'integration']},\n",
       " {'phrase': 'too but now', 'tokens': ['too', 'but', 'now']},\n",
       " {'phrase': 'too but the', 'tokens': ['too', 'but', 'the']},\n",
       " {'phrase': 'too but not', 'tokens': ['too', 'but', 'not']},\n",
       " {'phrase': 'too but as', 'tokens': ['too', 'but', 'as']},\n",
       " {'phrase': 'too but is', 'tokens': ['too', 'but', 'is']},\n",
       " {'phrase': 'too but copying', 'tokens': ['too', 'but', 'copying']},\n",
       " {'phrase': 'too but research', 'tokens': ['too', 'but', 'research']},\n",
       " {'phrase': 'too but analysis', 'tokens': ['too', 'but', 'analysis']},\n",
       " {'phrase': 'too but justice', 'tokens': ['too', 'but', 'justice']},\n",
       " {'phrase': 'too but,', 'tokens': ['too', 'but', ',']},\n",
       " {'phrase': 'too but investigation',\n",
       "  'tokens': ['too', 'but', 'investigation']},\n",
       " {'phrase': 'too but cleanup', 'tokens': ['too', 'but', 'cleanup']},\n",
       " {'phrase': 'too but whatever', 'tokens': ['too', 'but', 'whatever']},\n",
       " {'phrase': 'too but merging', 'tokens': ['too', 'but', 'merging']},\n",
       " {'phrase': 'too but translations', 'tokens': ['too', 'but', 'translations']},\n",
       " {'phrase': '-- but it', 'tokens': ['--', 'but', 'it']},\n",
       " {'phrase': '-- but this', 'tokens': ['--', 'but', 'this']},\n",
       " {'phrase': '-- but that', 'tokens': ['--', 'but', 'that']},\n",
       " {'phrase': '-- but something', 'tokens': ['--', 'but', 'something']},\n",
       " {'phrase': '-- but everything', 'tokens': ['--', 'but', 'everything']},\n",
       " {'phrase': '-- but editing', 'tokens': ['--', 'but', 'editing']},\n",
       " {'phrase': '-- but so', 'tokens': ['--', 'but', 'so']},\n",
       " {'phrase': '-- but they', 'tokens': ['--', 'but', 'they']},\n",
       " {'phrase': '-- but correction', 'tokens': ['--', 'but', 'correction']},\n",
       " {'phrase': '-- but things', 'tokens': ['--', 'but', 'things']},\n",
       " {'phrase': '-- but work', 'tokens': ['--', 'but', 'work']},\n",
       " {'phrase': '-- but translation', 'tokens': ['--', 'but', 'translation']},\n",
       " {'phrase': '-- but its', 'tokens': ['--', 'but', 'its']},\n",
       " {'phrase': '-- but both', 'tokens': ['--', 'but', 'both']},\n",
       " {'phrase': '-- but corrections', 'tokens': ['--', 'but', 'corrections']},\n",
       " {'phrase': '-- but revision', 'tokens': ['--', 'but', 'revision']},\n",
       " {'phrase': '-- but all', 'tokens': ['--', 'but', 'all']},\n",
       " {'phrase': '-- but moderation', 'tokens': ['--', 'but', 'moderation']},\n",
       " {'phrase': '-- but change', 'tokens': ['--', 'but', 'change']},\n",
       " {'phrase': '-- but attribution', 'tokens': ['--', 'but', 'attribution']},\n",
       " {'phrase': '-- but such', 'tokens': ['--', 'but', 'such']},\n",
       " {'phrase': '-- but thing', 'tokens': ['--', 'but', 'thing']},\n",
       " {'phrase': '-- but which', 'tokens': ['--', 'but', 'which']},\n",
       " {'phrase': '-- but nothing', 'tokens': ['--', 'but', 'nothing']},\n",
       " {'phrase': '-- but integration', 'tokens': ['--', 'but', 'integration']},\n",
       " {'phrase': '-- but these', 'tokens': ['--', 'but', 'these']},\n",
       " {'phrase': '-- but if', 'tokens': ['--', 'but', 'if']},\n",
       " {'phrase': '-- but also', 'tokens': ['--', 'but', 'also']},\n",
       " {'phrase': '-- but what', 'tokens': ['--', 'but', 'what']},\n",
       " {'phrase': '-- but same', 'tokens': ['--', 'but', 'same']},\n",
       " {'phrase': '-- but one', 'tokens': ['--', 'but', 'one']},\n",
       " {'phrase': '-- but formatting', 'tokens': ['--', 'but', 'formatting']},\n",
       " {'phrase': '-- but research', 'tokens': ['--', 'but', 'research']},\n",
       " {'phrase': '-- but copying', 'tokens': ['--', 'but', 'copying']},\n",
       " {'phrase': '-- but analysis', 'tokens': ['--', 'but', 'analysis']},\n",
       " {'phrase': '-- but merging', 'tokens': ['--', 'but', 'merging']},\n",
       " {'phrase': '-- but,', 'tokens': ['--', 'but', ',']},\n",
       " {'phrase': '-- but the', 'tokens': ['--', 'but', 'the']},\n",
       " {'phrase': '-- but changes', 'tokens': ['--', 'but', 'changes']},\n",
       " {'phrase': '-- but cleanup', 'tokens': ['--', 'but', 'cleanup']},\n",
       " {'phrase': '-- but not', 'tokens': ['--', 'but', 'not']},\n",
       " {'phrase': '-- but as', 'tokens': ['--', 'but', 'as']},\n",
       " {'phrase': '-- but otherwise', 'tokens': ['--', 'but', 'otherwise']},\n",
       " {'phrase': '-- but investigation', 'tokens': ['--', 'but', 'investigation']},\n",
       " {'phrase': '-- but justice', 'tokens': ['--', 'but', 'justice']},\n",
       " {'phrase': '-- but check', 'tokens': ['--', 'but', 'check']},\n",
       " {'phrase': '-- but still', 'tokens': ['--', 'but', 'still']},\n",
       " {'phrase': '-- but is', 'tokens': ['--', 'but', 'is']},\n",
       " {'phrase': '-- but now', 'tokens': ['--', 'but', 'now']},\n",
       " {'phrase': '-- but censorship', 'tokens': ['--', 'but', 'censorship']},\n",
       " {'phrase': 'here but it', 'tokens': ['here', 'but', 'it']},\n",
       " {'phrase': 'here but this', 'tokens': ['here', 'but', 'this']},\n",
       " {'phrase': 'here but that', 'tokens': ['here', 'but', 'that']},\n",
       " {'phrase': 'here but editing', 'tokens': ['here', 'but', 'editing']},\n",
       " {'phrase': 'here but everything', 'tokens': ['here', 'but', 'everything']},\n",
       " {'phrase': 'here but something', 'tokens': ['here', 'but', 'something']},\n",
       " {'phrase': 'here but translation', 'tokens': ['here', 'but', 'translation']},\n",
       " {'phrase': 'here but correction', 'tokens': ['here', 'but', 'correction']},\n",
       " {'phrase': 'here but they', 'tokens': ['here', 'but', 'they']},\n",
       " {'phrase': 'here but things', 'tokens': ['here', 'but', 'things']},\n",
       " {'phrase': 'here but work', 'tokens': ['here', 'but', 'work']},\n",
       " {'phrase': 'here but attribution', 'tokens': ['here', 'but', 'attribution']},\n",
       " {'phrase': 'here but corrections', 'tokens': ['here', 'but', 'corrections']},\n",
       " {'phrase': 'here but so', 'tokens': ['here', 'but', 'so']},\n",
       " {'phrase': 'here but formatting', 'tokens': ['here', 'but', 'formatting']},\n",
       " {'phrase': 'here but revision', 'tokens': ['here', 'but', 'revision']},\n",
       " {'phrase': 'here but moderation', 'tokens': ['here', 'but', 'moderation']},\n",
       " {'phrase': 'here but all', 'tokens': ['here', 'but', 'all']},\n",
       " {'phrase': 'here but thing', 'tokens': ['here', 'but', 'thing']},\n",
       " {'phrase': 'here but its', 'tokens': ['here', 'but', 'its']},\n",
       " {'phrase': 'here but change', 'tokens': ['here', 'but', 'change']},\n",
       " {'phrase': 'here but both', 'tokens': ['here', 'but', 'both']},\n",
       " {'phrase': 'here but nothing', 'tokens': ['here', 'but', 'nothing']},\n",
       " {'phrase': 'here but copying', 'tokens': ['here', 'but', 'copying']},\n",
       " {'phrase': 'here but which', 'tokens': ['here', 'but', 'which']},\n",
       " {'phrase': 'here but justice', 'tokens': ['here', 'but', 'justice']},\n",
       " {'phrase': 'here but research', 'tokens': ['here', 'but', 'research']},\n",
       " {'phrase': 'here but same', 'tokens': ['here', 'but', 'same']},\n",
       " {'phrase': 'here but still', 'tokens': ['here', 'but', 'still']},\n",
       " {'phrase': 'here but cleanup', 'tokens': ['here', 'but', 'cleanup']},\n",
       " {'phrase': 'here but otherwise', 'tokens': ['here', 'but', 'otherwise']},\n",
       " {'phrase': 'here but analysis', 'tokens': ['here', 'but', 'analysis']},\n",
       " {'phrase': 'here but translations',\n",
       "  'tokens': ['here', 'but', 'translations']},\n",
       " {'phrase': 'here but investigation',\n",
       "  'tokens': ['here', 'but', 'investigation']},\n",
       " {'phrase': 'here but such', 'tokens': ['here', 'but', 'such']},\n",
       " {'phrase': 'here but what', 'tokens': ['here', 'but', 'what']},\n",
       " {'phrase': 'here but localization',\n",
       "  'tokens': ['here', 'but', 'localization']},\n",
       " {'phrase': 'here but changes', 'tokens': ['here', 'but', 'changes']},\n",
       " {'phrase': 'here but check', 'tokens': ['here', 'but', 'check']},\n",
       " {'phrase': 'here but also', 'tokens': ['here', 'but', 'also']},\n",
       " {'phrase': 'here but checking', 'tokens': ['here', 'but', 'checking']},\n",
       " {'phrase': 'here but as', 'tokens': ['here', 'but', 'as']},\n",
       " {'phrase': 'here but search', 'tokens': ['here', 'but', 'search']},\n",
       " {'phrase': 'here but review', 'tokens': ['here', 'but', 'review']},\n",
       " {'phrase': 'here but these', 'tokens': ['here', 'but', 'these']},\n",
       " {'phrase': 'here but integration', 'tokens': ['here', 'but', 'integration']},\n",
       " {'phrase': 'here but copyright', 'tokens': ['here', 'but', 'copyright']},\n",
       " {'phrase': 'here but verification',\n",
       "  'tokens': ['here', 'but', 'verification']},\n",
       " {'phrase': 'here but edits', 'tokens': ['here', 'but', 'edits']},\n",
       " {'phrase': 'here but if', 'tokens': ['here', 'but', 'if']},\n",
       " {'phrase': 'somewhere but it', 'tokens': ['somewhere', 'but', 'it']},\n",
       " {'phrase': 'somewhere but this', 'tokens': ['somewhere', 'but', 'this']},\n",
       " {'phrase': 'somewhere but that', 'tokens': ['somewhere', 'but', 'that']},\n",
       " {'phrase': 'somewhere but something',\n",
       "  'tokens': ['somewhere', 'but', 'something']},\n",
       " {'phrase': 'somewhere but editing',\n",
       "  'tokens': ['somewhere', 'but', 'editing']},\n",
       " {'phrase': 'somewhere but everything',\n",
       "  'tokens': ['somewhere', 'but', 'everything']},\n",
       " {'phrase': 'somewhere but they', 'tokens': ['somewhere', 'but', 'they']},\n",
       " {'phrase': 'somewhere but translation',\n",
       "  'tokens': ['somewhere', 'but', 'translation']},\n",
       " {'phrase': 'somewhere but work', 'tokens': ['somewhere', 'but', 'work']},\n",
       " {'phrase': 'somewhere but correction',\n",
       "  'tokens': ['somewhere', 'but', 'correction']},\n",
       " {'phrase': 'somewhere but things', 'tokens': ['somewhere', 'but', 'things']},\n",
       " {'phrase': 'somewhere but so', 'tokens': ['somewhere', 'but', 'so']},\n",
       " {'phrase': 'somewhere but corrections',\n",
       "  'tokens': ['somewhere', 'but', 'corrections']},\n",
       " {'phrase': 'somewhere but nothing',\n",
       "  'tokens': ['somewhere', 'but', 'nothing']},\n",
       " {'phrase': 'somewhere but revision',\n",
       "  'tokens': ['somewhere', 'but', 'revision']},\n",
       " {'phrase': 'somewhere but its', 'tokens': ['somewhere', 'but', 'its']},\n",
       " {'phrase': 'somewhere but attribution',\n",
       "  'tokens': ['somewhere', 'but', 'attribution']},\n",
       " {'phrase': 'somewhere but all', 'tokens': ['somewhere', 'but', 'all']},\n",
       " {'phrase': 'somewhere but moderation',\n",
       "  'tokens': ['somewhere', 'but', 'moderation']},\n",
       " {'phrase': 'somewhere but formatting',\n",
       "  'tokens': ['somewhere', 'but', 'formatting']},\n",
       " {'phrase': 'somewhere but change', 'tokens': ['somewhere', 'but', 'change']},\n",
       " {'phrase': 'somewhere but thing', 'tokens': ['somewhere', 'but', 'thing']},\n",
       " {'phrase': 'somewhere but research',\n",
       "  'tokens': ['somewhere', 'but', 'research']},\n",
       " {'phrase': 'somewhere but which', 'tokens': ['somewhere', 'but', 'which']},\n",
       " {'phrase': 'somewhere but what', 'tokens': ['somewhere', 'but', 'what']},\n",
       " {'phrase': 'somewhere but such', 'tokens': ['somewhere', 'but', 'such']},\n",
       " {'phrase': 'somewhere but justice',\n",
       "  'tokens': ['somewhere', 'but', 'justice']},\n",
       " {'phrase': 'somewhere but analysis',\n",
       "  'tokens': ['somewhere', 'but', 'analysis']},\n",
       " {'phrase': 'somewhere but both', 'tokens': ['somewhere', 'but', 'both']},\n",
       " {'phrase': 'somewhere but copying',\n",
       "  'tokens': ['somewhere', 'but', 'copying']},\n",
       " {'phrase': 'somewhere but investigation',\n",
       "  'tokens': ['somewhere', 'but', 'investigation']},\n",
       " {'phrase': 'somewhere but changes',\n",
       "  'tokens': ['somewhere', 'but', 'changes']},\n",
       " {'phrase': 'somewhere but otherwise',\n",
       "  'tokens': ['somewhere', 'but', 'otherwise']},\n",
       " {'phrase': 'somewhere but still', 'tokens': ['somewhere', 'but', 'still']},\n",
       " {'phrase': 'somewhere but not', 'tokens': ['somewhere', 'but', 'not']},\n",
       " {'phrase': 'somewhere but search', 'tokens': ['somewhere', 'but', 'search']},\n",
       " {'phrase': 'somewhere but cleanup',\n",
       "  'tokens': ['somewhere', 'but', 'cleanup']},\n",
       " {'phrase': 'somewhere but localization',\n",
       "  'tokens': ['somewhere', 'but', 'localization']},\n",
       " {'phrase': 'somewhere but these', 'tokens': ['somewhere', 'but', 'these']},\n",
       " {'phrase': 'somewhere but whatever',\n",
       "  'tokens': ['somewhere', 'but', 'whatever']},\n",
       " {'phrase': 'somewhere but same', 'tokens': ['somewhere', 'but', 'same']},\n",
       " {'phrase': 'somewhere but if', 'tokens': ['somewhere', 'but', 'if']},\n",
       " {'phrase': 'somewhere but is', 'tokens': ['somewhere', 'but', 'is']},\n",
       " {'phrase': 'somewhere but translations',\n",
       "  'tokens': ['somewhere', 'but', 'translations']},\n",
       " {'phrase': 'somewhere but as', 'tokens': ['somewhere', 'but', 'as']},\n",
       " {'phrase': 'somewhere but check', 'tokens': ['somewhere', 'but', 'check']},\n",
       " {'phrase': 'somewhere but checking',\n",
       "  'tokens': ['somewhere', 'but', 'checking']},\n",
       " {'phrase': 'somewhere but the', 'tokens': ['somewhere', 'but', 'the']},\n",
       " {'phrase': 'somewhere but also', 'tokens': ['somewhere', 'but', 'also']},\n",
       " {'phrase': 'somewhere but censorship',\n",
       "  'tokens': ['somewhere', 'but', 'censorship']},\n",
       " {'phrase': 'further but it', 'tokens': ['further', 'but', 'it']},\n",
       " {'phrase': 'further but this', 'tokens': ['further', 'but', 'this']},\n",
       " {'phrase': 'further but that', 'tokens': ['further', 'but', 'that']},\n",
       " {'phrase': 'further but everything',\n",
       "  'tokens': ['further', 'but', 'everything']},\n",
       " {'phrase': 'further but something',\n",
       "  'tokens': ['further', 'but', 'something']},\n",
       " {'phrase': 'further but editing', 'tokens': ['further', 'but', 'editing']},\n",
       " {'phrase': 'further but they', 'tokens': ['further', 'but', 'they']},\n",
       " {'phrase': 'further but so', 'tokens': ['further', 'but', 'so']},\n",
       " {'phrase': 'further but things', 'tokens': ['further', 'but', 'things']},\n",
       " {'phrase': 'further but work', 'tokens': ['further', 'but', 'work']},\n",
       " {'phrase': 'further but translation',\n",
       "  'tokens': ['further', 'but', 'translation']},\n",
       " {'phrase': 'further but correction',\n",
       "  'tokens': ['further', 'but', 'correction']},\n",
       " {'phrase': 'further but all', 'tokens': ['further', 'but', 'all']},\n",
       " {'phrase': 'further but its', 'tokens': ['further', 'but', 'its']},\n",
       " {'phrase': 'further but revision', 'tokens': ['further', 'but', 'revision']},\n",
       " {'phrase': 'further but corrections',\n",
       "  'tokens': ['further', 'but', 'corrections']},\n",
       " {'phrase': 'further but nothing', 'tokens': ['further', 'but', 'nothing']},\n",
       " {'phrase': 'further but thing', 'tokens': ['further', 'but', 'thing']},\n",
       " {'phrase': 'further but which', 'tokens': ['further', 'but', 'which']},\n",
       " {'phrase': 'further but such', 'tokens': ['further', 'but', 'such']},\n",
       " {'phrase': 'further but moderation',\n",
       "  'tokens': ['further', 'but', 'moderation']},\n",
       " {'phrase': 'further but both', 'tokens': ['further', 'but', 'both']},\n",
       " {'phrase': 'further but still', 'tokens': ['further', 'but', 'still']},\n",
       " {'phrase': 'further but change', 'tokens': ['further', 'but', 'change']},\n",
       " {'phrase': 'further but what', 'tokens': ['further', 'but', 'what']},\n",
       " {'phrase': 'further but research', 'tokens': ['further', 'but', 'research']},\n",
       " {'phrase': 'further but these', 'tokens': ['further', 'but', 'these']},\n",
       " {'phrase': 'further but same', 'tokens': ['further', 'but', 'same']},\n",
       " {'phrase': 'further but investigation',\n",
       "  'tokens': ['further', 'but', 'investigation']},\n",
       " {'phrase': 'further but analysis', 'tokens': ['further', 'but', 'analysis']},\n",
       " {'phrase': 'further but also', 'tokens': ['further', 'but', 'also']},\n",
       " {'phrase': 'further but attribution',\n",
       "  'tokens': ['further', 'but', 'attribution']},\n",
       " {'phrase': 'further but now', 'tokens': ['further', 'but', 'now']},\n",
       " {'phrase': 'further but otherwise',\n",
       "  'tokens': ['further', 'but', 'otherwise']},\n",
       " {'phrase': 'further but if', 'tokens': ['further', 'but', 'if']},\n",
       " {'phrase': 'further but formatting',\n",
       "  'tokens': ['further', 'but', 'formatting']},\n",
       " {'phrase': 'further but as', 'tokens': ['further', 'but', 'as']},\n",
       " {'phrase': 'further but integration',\n",
       "  'tokens': ['further', 'but', 'integration']},\n",
       " {'phrase': 'further but justice', 'tokens': ['further', 'but', 'justice']},\n",
       " {'phrase': 'further but changes', 'tokens': ['further', 'but', 'changes']},\n",
       " {'phrase': 'further but the', 'tokens': ['further', 'but', 'the']},\n",
       " {'phrase': 'further but not', 'tokens': ['further', 'but', 'not']},\n",
       " {'phrase': 'further but is', 'tokens': ['further', 'but', 'is']},\n",
       " {'phrase': 'further but cleanup', 'tokens': ['further', 'but', 'cleanup']},\n",
       " {'phrase': 'further but copying', 'tokens': ['further', 'but', 'copying']},\n",
       " {'phrase': 'further but one', 'tokens': ['further', 'but', 'one']},\n",
       " {'phrase': 'further but check', 'tokens': ['further', 'but', 'check']},\n",
       " {'phrase': 'further but checking', 'tokens': ['further', 'but', 'checking']},\n",
       " {'phrase': 'further but,', 'tokens': ['further', 'but', ',']},\n",
       " {'phrase': 'further but time', 'tokens': ['further', 'but', 'time']},\n",
       " {'phrase': '— but it', 'tokens': ['—', 'but', 'it']},\n",
       " {'phrase': '— but this', 'tokens': ['—', 'but', 'this']},\n",
       " {'phrase': '— but that', 'tokens': ['—', 'but', 'that']},\n",
       " {'phrase': '— but everything', 'tokens': ['—', 'but', 'everything']},\n",
       " {'phrase': '— but something', 'tokens': ['—', 'but', 'something']},\n",
       " {'phrase': '— but editing', 'tokens': ['—', 'but', 'editing']},\n",
       " {'phrase': '— but so', 'tokens': ['—', 'but', 'so']},\n",
       " {'phrase': '— but they', 'tokens': ['—', 'but', 'they']},\n",
       " {'phrase': '— but correction', 'tokens': ['—', 'but', 'correction']},\n",
       " {'phrase': '— but things', 'tokens': ['—', 'but', 'things']},\n",
       " {'phrase': '— but work', 'tokens': ['—', 'but', 'work']},\n",
       " {'phrase': '— but its', 'tokens': ['—', 'but', 'its']},\n",
       " {'phrase': '— but both', 'tokens': ['—', 'but', 'both']},\n",
       " {'phrase': '— but translation', 'tokens': ['—', 'but', 'translation']},\n",
       " {'phrase': '— but corrections', 'tokens': ['—', 'but', 'corrections']},\n",
       " {'phrase': '— but all', 'tokens': ['—', 'but', 'all']},\n",
       " {'phrase': '— but revision', 'tokens': ['—', 'but', 'revision']},\n",
       " {'phrase': '— but such', 'tokens': ['—', 'but', 'such']},\n",
       " {'phrase': '— but which', 'tokens': ['—', 'but', 'which']},\n",
       " {'phrase': '— but moderation', 'tokens': ['—', 'but', 'moderation']},\n",
       " {'phrase': '— but attribution', 'tokens': ['—', 'but', 'attribution']},\n",
       " {'phrase': '— but change', 'tokens': ['—', 'but', 'change']},\n",
       " {'phrase': '— but nothing', 'tokens': ['—', 'but', 'nothing']},\n",
       " {'phrase': '— but thing', 'tokens': ['—', 'but', 'thing']},\n",
       " {'phrase': '— but if', 'tokens': ['—', 'but', 'if']},\n",
       " {'phrase': '— but these', 'tokens': ['—', 'but', 'these']},\n",
       " {'phrase': '— but integration', 'tokens': ['—', 'but', 'integration']},\n",
       " {'phrase': '— but also', 'tokens': ['—', 'but', 'also']},\n",
       " {'phrase': '— but what', 'tokens': ['—', 'but', 'what']},\n",
       " {'phrase': '— but same', 'tokens': ['—', 'but', 'same']},\n",
       " {'phrase': '— but one', 'tokens': ['—', 'but', 'one']},\n",
       " {'phrase': '— but formatting', 'tokens': ['—', 'but', 'formatting']},\n",
       " {'phrase': '— but the', 'tokens': ['—', 'but', 'the']},\n",
       " {'phrase': '— but analysis', 'tokens': ['—', 'but', 'analysis']},\n",
       " {'phrase': '— but merging', 'tokens': ['—', 'but', 'merging']},\n",
       " {'phrase': '— but copying', 'tokens': ['—', 'but', 'copying']},\n",
       " {'phrase': '— but not', 'tokens': ['—', 'but', 'not']},\n",
       " {'phrase': '— but research', 'tokens': ['—', 'but', 'research']},\n",
       " {'phrase': '— but as', 'tokens': ['—', 'but', 'as']},\n",
       " {'phrase': '— but otherwise', 'tokens': ['—', 'but', 'otherwise']},\n",
       " {'phrase': '— but,', 'tokens': ['—', 'but', ',']},\n",
       " {'phrase': '— but cleanup', 'tokens': ['—', 'but', 'cleanup']},\n",
       " {'phrase': '— but changes', 'tokens': ['—', 'but', 'changes']},\n",
       " {'phrase': '— but is', 'tokens': ['—', 'but', 'is']},\n",
       " {'phrase': '— but justice', 'tokens': ['—', 'but', 'justice']},\n",
       " {'phrase': '— but still', 'tokens': ['—', 'but', 'still']},\n",
       " {'phrase': '— but now', 'tokens': ['—', 'but', 'now']},\n",
       " {'phrase': '— but check', 'tokens': ['—', 'but', 'check']},\n",
       " {'phrase': '— but investigation', 'tokens': ['—', 'but', 'investigation']},\n",
       " {'phrase': '— but censorship', 'tokens': ['—', 'but', 'censorship']},\n",
       " {'phrase': '- but replication', 'tokens': ['-', 'but', 'replication']},\n",
       " {'phrase': '... but copyright', 'tokens': ['...', 'but', 'copyright']},\n",
       " {'phrase': 'altogether but it', 'tokens': ['altogether', 'but', 'it']},\n",
       " {'phrase': 'altogether but this', 'tokens': ['altogether', 'but', 'this']},\n",
       " {'phrase': 'altogether but that', 'tokens': ['altogether', 'but', 'that']},\n",
       " {'phrase': 'altogether but editing',\n",
       "  'tokens': ['altogether', 'but', 'editing']},\n",
       " {'phrase': 'altogether but something',\n",
       "  'tokens': ['altogether', 'but', 'something']},\n",
       " {'phrase': 'altogether but everything',\n",
       "  'tokens': ['altogether', 'but', 'everything']},\n",
       " {'phrase': 'altogether but correction',\n",
       "  'tokens': ['altogether', 'but', 'correction']},\n",
       " {'phrase': 'altogether but they', 'tokens': ['altogether', 'but', 'they']},\n",
       " {'phrase': 'altogether but so', 'tokens': ['altogether', 'but', 'so']},\n",
       " {'phrase': 'altogether but things',\n",
       "  'tokens': ['altogether', 'but', 'things']},\n",
       " {'phrase': 'altogether but translation',\n",
       "  'tokens': ['altogether', 'but', 'translation']},\n",
       " {'phrase': 'altogether but work', 'tokens': ['altogether', 'but', 'work']},\n",
       " {'phrase': 'altogether but revision',\n",
       "  'tokens': ['altogether', 'but', 'revision']},\n",
       " {'phrase': 'altogether but corrections',\n",
       "  'tokens': ['altogether', 'but', 'corrections']},\n",
       " {'phrase': 'altogether but moderation',\n",
       "  'tokens': ['altogether', 'but', 'moderation']},\n",
       " {'phrase': 'altogether but all', 'tokens': ['altogether', 'but', 'all']},\n",
       " {'phrase': 'altogether but change',\n",
       "  'tokens': ['altogether', 'but', 'change']},\n",
       " {'phrase': 'altogether but nothing',\n",
       "  'tokens': ['altogether', 'but', 'nothing']},\n",
       " {'phrase': 'altogether but both', 'tokens': ['altogether', 'but', 'both']},\n",
       " {'phrase': 'altogether but attribution',\n",
       "  'tokens': ['altogether', 'but', 'attribution']},\n",
       " {'phrase': 'altogether but its', 'tokens': ['altogether', 'but', 'its']},\n",
       " {'phrase': 'altogether but thing', 'tokens': ['altogether', 'but', 'thing']},\n",
       " {'phrase': 'altogether but formatting',\n",
       "  'tokens': ['altogether', 'but', 'formatting']},\n",
       " {'phrase': 'altogether but such', 'tokens': ['altogether', 'but', 'such']},\n",
       " {'phrase': 'altogether but which', 'tokens': ['altogether', 'but', 'which']},\n",
       " {'phrase': 'altogether but what', 'tokens': ['altogether', 'but', 'what']},\n",
       " {'phrase': 'altogether but research',\n",
       "  'tokens': ['altogether', 'but', 'research']},\n",
       " {'phrase': 'altogether but justice',\n",
       "  'tokens': ['altogether', 'but', 'justice']},\n",
       " {'phrase': 'altogether but analysis',\n",
       "  'tokens': ['altogether', 'but', 'analysis']},\n",
       " {'phrase': 'altogether but investigation',\n",
       "  'tokens': ['altogether', 'but', 'investigation']},\n",
       " {'phrase': 'altogether but cleanup',\n",
       "  'tokens': ['altogether', 'but', 'cleanup']},\n",
       " {'phrase': 'altogether but changes',\n",
       "  'tokens': ['altogether', 'but', 'changes']},\n",
       " {'phrase': 'altogether but censorship',\n",
       "  'tokens': ['altogether', 'but', 'censorship']},\n",
       " {'phrase': 'altogether but otherwise',\n",
       "  'tokens': ['altogether', 'but', 'otherwise']},\n",
       " {'phrase': 'altogether but same', 'tokens': ['altogether', 'but', 'same']},\n",
       " {'phrase': 'altogether but still', 'tokens': ['altogether', 'but', 'still']},\n",
       " {'phrase': 'altogether but these', 'tokens': ['altogether', 'but', 'these']},\n",
       " {'phrase': 'altogether but copying',\n",
       "  'tokens': ['altogether', 'but', 'copying']},\n",
       " {'phrase': 'altogether but as', 'tokens': ['altogether', 'but', 'as']},\n",
       " {'phrase': 'altogether but if', 'tokens': ['altogether', 'but', 'if']},\n",
       " {'phrase': 'altogether but check', 'tokens': ['altogether', 'but', 'check']},\n",
       " {'phrase': 'altogether but review',\n",
       "  'tokens': ['altogether', 'but', 'review']},\n",
       " {'phrase': 'altogether but one', 'tokens': ['altogether', 'but', 'one']},\n",
       " {'phrase': 'altogether but not', 'tokens': ['altogether', 'but', 'not']},\n",
       " {'phrase': 'altogether but also', 'tokens': ['altogether', 'but', 'also']},\n",
       " {'phrase': 'altogether but localization',\n",
       "  'tokens': ['altogether', 'but', 'localization']},\n",
       " {'phrase': 'altogether but translations',\n",
       "  'tokens': ['altogether', 'but', 'translations']},\n",
       " {'phrase': 'altogether but checking',\n",
       "  'tokens': ['altogether', 'but', 'checking']},\n",
       " {'phrase': 'altogether but the', 'tokens': ['altogether', 'but', 'the']},\n",
       " {'phrase': 'altogether but now', 'tokens': ['altogether', 'but', 'now']},\n",
       " {'phrase': 'in but it', 'tokens': ['in', 'but', 'it']},\n",
       " {'phrase': 'in but this', 'tokens': ['in', 'but', 'this']},\n",
       " {'phrase': 'in but that', 'tokens': ['in', 'but', 'that']},\n",
       " {'phrase': 'in but everything', 'tokens': ['in', 'but', 'everything']},\n",
       " {'phrase': 'in but editing', 'tokens': ['in', 'but', 'editing']},\n",
       " {'phrase': 'in but something', 'tokens': ['in', 'but', 'something']},\n",
       " {'phrase': 'in but they', 'tokens': ['in', 'but', 'they']},\n",
       " {'phrase': 'in but so', 'tokens': ['in', 'but', 'so']},\n",
       " {'phrase': 'in but translation', 'tokens': ['in', 'but', 'translation']},\n",
       " {'phrase': 'in but things', 'tokens': ['in', 'but', 'things']},\n",
       " {'phrase': 'in but correction', 'tokens': ['in', 'but', 'correction']},\n",
       " {'phrase': 'in but work', 'tokens': ['in', 'but', 'work']},\n",
       " {'phrase': 'in but all', 'tokens': ['in', 'but', 'all']},\n",
       " {'phrase': 'in but corrections', 'tokens': ['in', 'but', 'corrections']},\n",
       " {'phrase': 'in but revision', 'tokens': ['in', 'but', 'revision']},\n",
       " {'phrase': 'in but its', 'tokens': ['in', 'but', 'its']},\n",
       " {'phrase': 'in but thing', 'tokens': ['in', 'but', 'thing']},\n",
       " {'phrase': 'in but attribution', 'tokens': ['in', 'but', 'attribution']},\n",
       " {'phrase': 'in but change', 'tokens': ['in', 'but', 'change']},\n",
       " {'phrase': 'in but moderation', 'tokens': ['in', 'but', 'moderation']},\n",
       " {'phrase': 'in but nothing', 'tokens': ['in', 'but', 'nothing']},\n",
       " {'phrase': 'in but formatting', 'tokens': ['in', 'but', 'formatting']},\n",
       " {'phrase': 'in but which', 'tokens': ['in', 'but', 'which']},\n",
       " {'phrase': 'in but both', 'tokens': ['in', 'but', 'both']},\n",
       " {'phrase': 'in but otherwise', 'tokens': ['in', 'but', 'otherwise']},\n",
       " {'phrase': 'in but still', 'tokens': ['in', 'but', 'still']},\n",
       " {'phrase': 'in but such', 'tokens': ['in', 'but', 'such']},\n",
       " {'phrase': 'in but also', 'tokens': ['in', 'but', 'also']},\n",
       " {'phrase': 'in but what', 'tokens': ['in', 'but', 'what']},\n",
       " {'phrase': 'in but analysis', 'tokens': ['in', 'but', 'analysis']},\n",
       " {'phrase': 'in but same', 'tokens': ['in', 'but', 'same']},\n",
       " {'phrase': 'in but if', 'tokens': ['in', 'but', 'if']},\n",
       " {'phrase': 'in but research', 'tokens': ['in', 'but', 'research']},\n",
       " {'phrase': 'in but changes', 'tokens': ['in', 'but', 'changes']},\n",
       " {'phrase': 'in but not', 'tokens': ['in', 'but', 'not']},\n",
       " {'phrase': 'in but investigation', 'tokens': ['in', 'but', 'investigation']},\n",
       " {'phrase': 'in but check', 'tokens': ['in', 'but', 'check']},\n",
       " {'phrase': 'in but copying', 'tokens': ['in', 'but', 'copying']},\n",
       " {'phrase': 'in but these', 'tokens': ['in', 'but', 'these']},\n",
       " {'phrase': 'in but checking', 'tokens': ['in', 'but', 'checking']},\n",
       " {'phrase': 'in but the', 'tokens': ['in', 'but', 'the']},\n",
       " {'phrase': 'in but search', 'tokens': ['in', 'but', 'search']},\n",
       " {'phrase': 'in but is', 'tokens': ['in', 'but', 'is']},\n",
       " {'phrase': 'in but integration', 'tokens': ['in', 'but', 'integration']},\n",
       " {'phrase': 'in but as', 'tokens': ['in', 'but', 'as']},\n",
       " {'phrase': 'in but cleanup', 'tokens': ['in', 'but', 'cleanup']},\n",
       " {'phrase': 'in but justice', 'tokens': ['in', 'but', 'justice']},\n",
       " {'phrase': 'in but now', 'tokens': ['in', 'but', 'now']},\n",
       " {'phrase': 'in but,', 'tokens': ['in', 'but', ',']},\n",
       " {'phrase': 'in but review', 'tokens': ['in', 'but', 'review']},\n",
       " {'phrase': 'elsewhere but it', 'tokens': ['elsewhere', 'but', 'it']},\n",
       " {'phrase': 'elsewhere but this', 'tokens': ['elsewhere', 'but', 'this']},\n",
       " {'phrase': 'elsewhere but that', 'tokens': ['elsewhere', 'but', 'that']},\n",
       " {'phrase': 'elsewhere but editing',\n",
       "  'tokens': ['elsewhere', 'but', 'editing']},\n",
       " {'phrase': 'elsewhere but something',\n",
       "  'tokens': ['elsewhere', 'but', 'something']},\n",
       " {'phrase': 'elsewhere but everything',\n",
       "  'tokens': ['elsewhere', 'but', 'everything']},\n",
       " {'phrase': 'elsewhere but correction',\n",
       "  'tokens': ['elsewhere', 'but', 'correction']},\n",
       " {'phrase': 'elsewhere but translation',\n",
       "  'tokens': ['elsewhere', 'but', 'translation']},\n",
       " {'phrase': 'elsewhere but they', 'tokens': ['elsewhere', 'but', 'they']},\n",
       " {'phrase': 'elsewhere but so', 'tokens': ['elsewhere', 'but', 'so']},\n",
       " {'phrase': 'elsewhere but work', 'tokens': ['elsewhere', 'but', 'work']},\n",
       " {'phrase': 'elsewhere but corrections',\n",
       "  'tokens': ['elsewhere', 'but', 'corrections']},\n",
       " {'phrase': 'elsewhere but things', 'tokens': ['elsewhere', 'but', 'things']},\n",
       " {'phrase': 'elsewhere but revision',\n",
       "  'tokens': ['elsewhere', 'but', 'revision']},\n",
       " {'phrase': 'elsewhere but attribution',\n",
       "  'tokens': ['elsewhere', 'but', 'attribution']},\n",
       " {'phrase': 'elsewhere but all', 'tokens': ['elsewhere', 'but', 'all']},\n",
       " {'phrase': 'elsewhere but moderation',\n",
       "  'tokens': ['elsewhere', 'but', 'moderation']},\n",
       " {'phrase': 'elsewhere but research',\n",
       "  'tokens': ['elsewhere', 'but', 'research']},\n",
       " {'phrase': 'elsewhere but both', 'tokens': ['elsewhere', 'but', 'both']},\n",
       " {'phrase': 'elsewhere but thing', 'tokens': ['elsewhere', 'but', 'thing']},\n",
       " {'phrase': 'elsewhere but change', 'tokens': ['elsewhere', 'but', 'change']},\n",
       " {'phrase': 'elsewhere but nothing',\n",
       "  'tokens': ['elsewhere', 'but', 'nothing']},\n",
       " {'phrase': 'elsewhere but its', 'tokens': ['elsewhere', 'but', 'its']},\n",
       " {'phrase': 'elsewhere but such', 'tokens': ['elsewhere', 'but', 'such']},\n",
       " {'phrase': 'elsewhere but investigation',\n",
       "  'tokens': ['elsewhere', 'but', 'investigation']},\n",
       " {'phrase': 'elsewhere but which', 'tokens': ['elsewhere', 'but', 'which']},\n",
       " {'phrase': 'elsewhere but copying',\n",
       "  'tokens': ['elsewhere', 'but', 'copying']},\n",
       " {'phrase': 'elsewhere but same', 'tokens': ['elsewhere', 'but', 'same']},\n",
       " {'phrase': 'elsewhere but analysis',\n",
       "  'tokens': ['elsewhere', 'but', 'analysis']},\n",
       " {'phrase': 'elsewhere but justice',\n",
       "  'tokens': ['elsewhere', 'but', 'justice']},\n",
       " {'phrase': 'elsewhere but what', 'tokens': ['elsewhere', 'but', 'what']},\n",
       " {'phrase': 'elsewhere but otherwise',\n",
       "  'tokens': ['elsewhere', 'but', 'otherwise']},\n",
       " {'phrase': 'elsewhere but formatting',\n",
       "  'tokens': ['elsewhere', 'but', 'formatting']},\n",
       " {'phrase': 'elsewhere but still', 'tokens': ['elsewhere', 'but', 'still']},\n",
       " {'phrase': 'elsewhere but translations',\n",
       "  'tokens': ['elsewhere', 'but', 'translations']},\n",
       " {'phrase': 'elsewhere but as', 'tokens': ['elsewhere', 'but', 'as']},\n",
       " {'phrase': 'elsewhere but cleanup',\n",
       "  'tokens': ['elsewhere', 'but', 'cleanup']},\n",
       " {'phrase': 'elsewhere but these', 'tokens': ['elsewhere', 'but', 'these']},\n",
       " {'phrase': 'elsewhere but localization',\n",
       "  'tokens': ['elsewhere', 'but', 'localization']},\n",
       " {'phrase': 'elsewhere but checking',\n",
       "  'tokens': ['elsewhere', 'but', 'checking']},\n",
       " {'phrase': 'elsewhere but check', 'tokens': ['elsewhere', 'but', 'check']},\n",
       " {'phrase': 'elsewhere but changes',\n",
       "  'tokens': ['elsewhere', 'but', 'changes']},\n",
       " {'phrase': 'elsewhere but search', 'tokens': ['elsewhere', 'but', 'search']},\n",
       " {'phrase': 'elsewhere but also', 'tokens': ['elsewhere', 'but', 'also']},\n",
       " {'phrase': 'elsewhere but censorship',\n",
       "  'tokens': ['elsewhere', 'but', 'censorship']},\n",
       " {'phrase': 'elsewhere but integration',\n",
       "  'tokens': ['elsewhere', 'but', 'integration']},\n",
       " {'phrase': 'elsewhere but updating',\n",
       "  'tokens': ['elsewhere', 'but', 'updating']},\n",
       " {'phrase': 'elsewhere but review', 'tokens': ['elsewhere', 'but', 'review']},\n",
       " {'phrase': 'elsewhere but whatever',\n",
       "  'tokens': ['elsewhere', 'but', 'whatever']},\n",
       " {'phrase': 'elsewhere but the', 'tokens': ['elsewhere', 'but', 'the']},\n",
       " {'phrase': 'separately but it', 'tokens': ['separately', 'but', 'it']},\n",
       " {'phrase': 'separately but this', 'tokens': ['separately', 'but', 'this']},\n",
       " {'phrase': 'separately but that', 'tokens': ['separately', 'but', 'that']},\n",
       " {'phrase': 'separately but both', 'tokens': ['separately', 'but', 'both']},\n",
       " {'phrase': 'separately but everything',\n",
       "  'tokens': ['separately', 'but', 'everything']},\n",
       " {'phrase': 'separately but editing',\n",
       "  'tokens': ['separately', 'but', 'editing']},\n",
       " {'phrase': 'separately but something',\n",
       "  'tokens': ['separately', 'but', 'something']},\n",
       " {'phrase': 'separately but they', 'tokens': ['separately', 'but', 'they']},\n",
       " {'phrase': 'separately but so', 'tokens': ['separately', 'but', 'so']},\n",
       " {'phrase': 'separately but translation',\n",
       "  'tokens': ['separately', 'but', 'translation']},\n",
       " {'phrase': 'separately but things',\n",
       "  'tokens': ['separately', 'but', 'things']},\n",
       " {'phrase': 'separately but all', 'tokens': ['separately', 'but', 'all']},\n",
       " {'phrase': 'separately but work', 'tokens': ['separately', 'but', 'work']},\n",
       " {'phrase': 'separately but correction',\n",
       "  'tokens': ['separately', 'but', 'correction']},\n",
       " {'phrase': 'separately but attribution',\n",
       "  'tokens': ['separately', 'but', 'attribution']},\n",
       " {'phrase': 'separately but nothing',\n",
       "  'tokens': ['separately', 'but', 'nothing']},\n",
       " {'phrase': 'separately but which', 'tokens': ['separately', 'but', 'which']},\n",
       " {'phrase': 'separately but revision',\n",
       "  'tokens': ['separately', 'but', 'revision']},\n",
       " {'phrase': 'separately but corrections',\n",
       "  'tokens': ['separately', 'but', 'corrections']},\n",
       " {'phrase': 'separately but formatting',\n",
       "  'tokens': ['separately', 'but', 'formatting']},\n",
       " {'phrase': 'separately but same', 'tokens': ['separately', 'but', 'same']},\n",
       " {'phrase': 'separately but these', 'tokens': ['separately', 'but', 'these']},\n",
       " {'phrase': 'separately but moderation',\n",
       "  'tokens': ['separately', 'but', 'moderation']},\n",
       " {'phrase': 'separately but thing', 'tokens': ['separately', 'but', 'thing']},\n",
       " {'phrase': 'separately but otherwise',\n",
       "  'tokens': ['separately', 'but', 'otherwise']},\n",
       " {'phrase': 'separately but integration',\n",
       "  'tokens': ['separately', 'but', 'integration']},\n",
       " {'phrase': 'separately but merging',\n",
       "  'tokens': ['separately', 'but', 'merging']},\n",
       " {'phrase': 'separately but change',\n",
       "  'tokens': ['separately', 'but', 'change']},\n",
       " {'phrase': 'separately but what', 'tokens': ['separately', 'but', 'what']},\n",
       " {'phrase': 'separately but its', 'tokens': ['separately', 'but', 'its']},\n",
       " {'phrase': 'separately but such', 'tokens': ['separately', 'but', 'such']},\n",
       " {'phrase': 'separately but copying',\n",
       "  'tokens': ['separately', 'but', 'copying']},\n",
       " {'phrase': 'separately but one', 'tokens': ['separately', 'but', 'one']},\n",
       " {'phrase': 'separately but linking',\n",
       "  'tokens': ['separately', 'but', 'linking']},\n",
       " {'phrase': 'separately but neither',\n",
       "  'tokens': ['separately', 'but', 'neither']},\n",
       " {'phrase': 'separately but also', 'tokens': ['separately', 'but', 'also']},\n",
       " {'phrase': 'separately but as', 'tokens': ['separately', 'but', 'as']},\n",
       " {'phrase': 'separately but analysis',\n",
       "  'tokens': ['separately', 'but', 'analysis']},\n",
       " {'phrase': 'separately but still', 'tokens': ['separately', 'but', 'still']},\n",
       " {'phrase': 'separately but changes',\n",
       "  'tokens': ['separately', 'but', 'changes']},\n",
       " {'phrase': 'separately but research',\n",
       "  'tokens': ['separately', 'but', 'research']},\n",
       " {'phrase': 'separately but not', 'tokens': ['separately', 'but', 'not']},\n",
       " {'phrase': 'separately but investigation',\n",
       "  'tokens': ['separately', 'but', 'investigation']},\n",
       " {'phrase': 'separately but the', 'tokens': ['separately', 'but', 'the']},\n",
       " {'phrase': 'separately but translations',\n",
       "  'tokens': ['separately', 'but', 'translations']},\n",
       " {'phrase': 'separately but insertion',\n",
       "  'tokens': ['separately', 'but', 'insertion']},\n",
       " {'phrase': 'separately but duplication',\n",
       "  'tokens': ['separately', 'but', 'duplication']},\n",
       " {'phrase': 'separately but communication',\n",
       "  'tokens': ['separately', 'but', 'communication']},\n",
       " {'phrase': 'separately but justice',\n",
       "  'tokens': ['separately', 'but', 'justice']},\n",
       " {'phrase': 'separately but now', 'tokens': ['separately', 'but', 'now']},\n",
       " {'phrase': 'first but it', 'tokens': ['first', 'but', 'it']},\n",
       " {'phrase': 'first but this', 'tokens': ['first', 'but', 'this']},\n",
       " {'phrase': 'first but that', 'tokens': ['first', 'but', 'that']},\n",
       " {'phrase': 'first but editing', 'tokens': ['first', 'but', 'editing']},\n",
       " {'phrase': 'first but everything', 'tokens': ['first', 'but', 'everything']},\n",
       " {'phrase': 'first but something', 'tokens': ['first', 'but', 'something']},\n",
       " {'phrase': 'first but translation',\n",
       "  'tokens': ['first', 'but', 'translation']},\n",
       " {'phrase': 'first but correction', 'tokens': ['first', 'but', 'correction']},\n",
       " {'phrase': 'first but revision', 'tokens': ['first', 'but', 'revision']},\n",
       " {'phrase': 'first but so', 'tokens': ['first', 'but', 'so']},\n",
       " {'phrase': 'first but they', 'tokens': ['first', 'but', 'they']},\n",
       " {'phrase': 'first but work', 'tokens': ['first', 'but', 'work']},\n",
       " {'phrase': 'first but things', 'tokens': ['first', 'but', 'things']},\n",
       " {'phrase': 'first but all', 'tokens': ['first', 'but', 'all']},\n",
       " {'phrase': 'first but corrections',\n",
       "  'tokens': ['first', 'but', 'corrections']},\n",
       " {'phrase': 'first but both', 'tokens': ['first', 'but', 'both']},\n",
       " {'phrase': 'first but attribution',\n",
       "  'tokens': ['first', 'but', 'attribution']},\n",
       " {'phrase': 'first but formatting', 'tokens': ['first', 'but', 'formatting']},\n",
       " {'phrase': 'first but its', 'tokens': ['first', 'but', 'its']},\n",
       " {'phrase': 'first but also', 'tokens': ['first', 'but', 'also']},\n",
       " {'phrase': 'first but moderation', 'tokens': ['first', 'but', 'moderation']},\n",
       " {'phrase': 'first but change', 'tokens': ['first', 'but', 'change']},\n",
       " {'phrase': 'first but thing', 'tokens': ['first', 'but', 'thing']},\n",
       " {'phrase': 'first but still', 'tokens': ['first', 'but', 'still']},\n",
       " {'phrase': 'first but which', 'tokens': ['first', 'but', 'which']},\n",
       " {'phrase': 'first but otherwise', 'tokens': ['first', 'but', 'otherwise']},\n",
       " {'phrase': 'first but same', 'tokens': ['first', 'but', 'same']},\n",
       " {'phrase': 'first but now', 'tokens': ['first', 'but', 'now']},\n",
       " {'phrase': 'first but copying', 'tokens': ['first', 'but', 'copying']},\n",
       " {'phrase': 'first but such', 'tokens': ['first', 'but', 'such']},\n",
       " {'phrase': 'first but analysis', 'tokens': ['first', 'but', 'analysis']},\n",
       " {'phrase': 'first but what', 'tokens': ['first', 'but', 'what']},\n",
       " {'phrase': 'first but integration',\n",
       "  'tokens': ['first', 'but', 'integration']},\n",
       " {'phrase': 'first but cleanup', 'tokens': ['first', 'but', 'cleanup']},\n",
       " {'phrase': 'first but merging', 'tokens': ['first', 'but', 'merging']},\n",
       " {'phrase': 'first but research', 'tokens': ['first', 'but', 'research']},\n",
       " {'phrase': 'first but these', 'tokens': ['first', 'but', 'these']},\n",
       " {'phrase': 'first but changes', 'tokens': ['first', 'but', 'changes']},\n",
       " {'phrase': 'first but investigation',\n",
       "  'tokens': ['first', 'but', 'investigation']},\n",
       " {'phrase': 'first but nothing', 'tokens': ['first', 'but', 'nothing']},\n",
       " {'phrase': 'first but if', 'tokens': ['first', 'but', 'if']},\n",
       " {'phrase': 'first but the', 'tokens': ['first', 'but', 'the']},\n",
       " {'phrase': 'first but translations',\n",
       "  'tokens': ['first', 'but', 'translations']},\n",
       " {'phrase': 'first but then', 'tokens': ['first', 'but', 'then']},\n",
       " {'phrase': 'first but updating', 'tokens': ['first', 'but', 'updating']},\n",
       " {'phrase': 'first but revisions', 'tokens': ['first', 'but', 'revisions']},\n",
       " {'phrase': 'first but one', 'tokens': ['first', 'but', 'one']},\n",
       " {'phrase': 'first but as', 'tokens': ['first', 'but', 'as']},\n",
       " {'phrase': 'first but justice', 'tokens': ['first', 'but', 'justice']},\n",
       " {'phrase': 'first but linking', 'tokens': ['first', 'but', 'linking']},\n",
       " {'phrase': ': but it', 'tokens': [':', 'but', 'it']},\n",
       " {'phrase': ': but this', 'tokens': [':', 'but', 'this']},\n",
       " {'phrase': ': but that', 'tokens': [':', 'but', 'that']},\n",
       " {'phrase': ': but editing', 'tokens': [':', 'but', 'editing']},\n",
       " {'phrase': ': but everything', 'tokens': [':', 'but', 'everything']},\n",
       " {'phrase': ': but something', 'tokens': [':', 'but', 'something']},\n",
       " {'phrase': ': but so', 'tokens': [':', 'but', 'so']},\n",
       " {'phrase': ': but they', 'tokens': [':', 'but', 'they']},\n",
       " {'phrase': ': but correction', 'tokens': [':', 'but', 'correction']},\n",
       " {'phrase': ': but translation', 'tokens': [':', 'but', 'translation']},\n",
       " {'phrase': ': but things', 'tokens': [':', 'but', 'things']},\n",
       " {'phrase': ': but corrections', 'tokens': [':', 'but', 'corrections']},\n",
       " {'phrase': ': but work', 'tokens': [':', 'but', 'work']},\n",
       " {'phrase': ': but revision', 'tokens': [':', 'but', 'revision']},\n",
       " {'phrase': ': but attribution', 'tokens': [':', 'but', 'attribution']},\n",
       " {'phrase': ': but its', 'tokens': [':', 'but', 'its']},\n",
       " {'phrase': ': but thing', 'tokens': [':', 'but', 'thing']},\n",
       " {'phrase': ': but both', 'tokens': [':', 'but', 'both']},\n",
       " {'phrase': ': but moderation', 'tokens': [':', 'but', 'moderation']},\n",
       " {'phrase': ': but all', 'tokens': [':', 'but', 'all']},\n",
       " {'phrase': ': but change', 'tokens': [':', 'but', 'change']},\n",
       " {'phrase': ': but merging', 'tokens': [':', 'but', 'merging']},\n",
       " {'phrase': ': but integration', 'tokens': [':', 'but', 'integration']},\n",
       " {'phrase': ': but formatting', 'tokens': [':', 'but', 'formatting']},\n",
       " {'phrase': ': but such', 'tokens': [':', 'but', 'such']},\n",
       " {'phrase': ': but copying', 'tokens': [':', 'but', 'copying']},\n",
       " {'phrase': ': but nothing', 'tokens': [':', 'but', 'nothing']},\n",
       " {'phrase': ': but which', 'tokens': [':', 'but', 'which']},\n",
       " {'phrase': ': but these', 'tokens': [':', 'but', 'these']},\n",
       " {'phrase': ': but same', 'tokens': [':', 'but', 'same']},\n",
       " {'phrase': ': but changes', 'tokens': [':', 'but', 'changes']},\n",
       " {'phrase': ': but research', 'tokens': [':', 'but', 'research']},\n",
       " {'phrase': ': but cleanup', 'tokens': [':', 'but', 'cleanup']},\n",
       " {'phrase': ': but,', 'tokens': [':', 'but', ',']},\n",
       " {'phrase': ': but analysis', 'tokens': [':', 'but', 'analysis']},\n",
       " {'phrase': ': butit', 'tokens': [':', 'but', 'it']},\n",
       " {'phrase': ': but one', 'tokens': [':', 'but', 'one']},\n",
       " {'phrase': ': but duplication', 'tokens': [':', 'but', 'duplication']},\n",
       " {'phrase': ': but merge', 'tokens': [':', 'but', 'merge']},\n",
       " {'phrase': ': but investigation', 'tokens': [':', 'but', 'investigation']},\n",
       " {'phrase': ': but if', 'tokens': [':', 'but', 'if']},\n",
       " {'phrase': ': but what', 'tokens': [':', 'but', 'what']},\n",
       " {'phrase': ': but linking', 'tokens': [':', 'but', 'linking']},\n",
       " {'phrase': ': but replication', 'tokens': [':', 'but', 'replication']},\n",
       " {'phrase': ': but the', 'tokens': [':', 'but', 'the']},\n",
       " {'phrase': ': but also', 'tokens': [':', 'but', 'also']},\n",
       " ...]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cc7c19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
