{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a04acc0",
   "metadata": {},
   "source": [
    "# Run Paraphrase Common Token N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500a216e",
   "metadata": {},
   "source": [
    "## Load Libraries\n",
    "\n",
    "Load libraries from standard modules and my own code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02e7b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List\n",
    "\n",
    "sys.path.append(os.path.abspath('../../../src'))\n",
    "\n",
    "from read_and_write_docs import read_jsonl, read_rds\n",
    "from tokenize_and_score import load_model\n",
    "from utils import get_base_location, apply_temp_doc_id, build_metadata_df\n",
    "from n_gram_functions import (\n",
    "    common_ngrams,\n",
    "    pretty_print_common_ngrams,\n",
    "    keep_before_phrase,\n",
    "    compute_log_probs_with_median\n",
    ")\n",
    "from open_ai import initialise_client, llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56192fae",
   "metadata": {},
   "source": [
    "## Set Locations & Load Base Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b0f4ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"Wiki\"\n",
    "data_type = \"training\"\n",
    "\n",
    "# Set NAS so can run on Windows laptop seamlessly\n",
    "nas_base_loc = get_base_location()\n",
    "\n",
    "# Load known data\n",
    "known_loc = f\"{nas_base_loc}/datasets/author_verification/{data_type}/{corpus}/known_raw.jsonl\"\n",
    "known = read_jsonl(known_loc)\n",
    "known = apply_temp_doc_id(known)\n",
    "\n",
    "# Load unknown data\n",
    "unknown_loc = f\"{nas_base_loc}/datasets/author_verification/{data_type}/{corpus}/unknown_raw.jsonl\"\n",
    "unknown = read_jsonl(unknown_loc)\n",
    "unknown_df = apply_temp_doc_id(unknown)\n",
    "\n",
    "# Load and build metadata\n",
    "metadata_loc = f\"{nas_base_loc}/datasets/author_verification/{data_type}/metadata.rds\"\n",
    "metadata = read_rds(metadata_loc)\n",
    "filtered_metadata = metadata[metadata['corpus'] == corpus]\n",
    "agg_metadata = build_metadata_df(filtered_metadata, known, unknown)\n",
    "\n",
    "# Load the pre-made problem datasets for speed\n",
    "problem_dataset_base = f\"{nas_base_loc}/datasets/author_verification/{data_type}/{corpus}\"\n",
    "problem_dataset_agg = read_jsonl(f\"{problem_dataset_base}/{corpus}_{data_type}_agg.jsonl\")\n",
    "problem_dataset_profile = read_jsonl(f\"{problem_dataset_base}/{corpus}_{data_type}_profile.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d85eaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = load_model(f\"{nas_base_loc}/models/Qwen 2.5/Qwen2.5-0.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e384cb",
   "metadata": {},
   "source": [
    "## View Same and Different-Author Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18fa9b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>known_author</th>\n",
       "      <th>unknown_author</th>\n",
       "      <th>known_doc_id</th>\n",
       "      <th>unknown_doc_id</th>\n",
       "      <th>highest_common_count</th>\n",
       "      <th>highest_common_ngram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>Greg_L vs Greg_L</td>\n",
       "      <td>Greg_L</td>\n",
       "      <td>Greg_L</td>\n",
       "      <td>greg_l_text_11</td>\n",
       "      <td>greg_l_text_10</td>\n",
       "      <td>9</td>\n",
       "      <td>, Ġthey Ġshould Ġhave Ġparticipated Ġin Ġthe Ġ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>Haymaker vs Haymaker</td>\n",
       "      <td>Haymaker</td>\n",
       "      <td>Haymaker</td>\n",
       "      <td>haymaker_text_3</td>\n",
       "      <td>haymaker_text_2</td>\n",
       "      <td>9</td>\n",
       "      <td>Ġat Ġthe Ġend Ġof Ġthe Ġday , Ġwe 're</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>DonaNobisPacem vs DonaNobisPacem</td>\n",
       "      <td>DonaNobisPacem</td>\n",
       "      <td>DonaNobisPacem</td>\n",
       "      <td>donanobispacem_text_5</td>\n",
       "      <td>donanobispacem_text_2</td>\n",
       "      <td>8</td>\n",
       "      <td>Ġafter Ġ 1 8 - 2 0 Ġweeks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>Fixentries vs Fixentries</td>\n",
       "      <td>Fixentries</td>\n",
       "      <td>Fixentries</td>\n",
       "      <td>fixentries_text_2</td>\n",
       "      <td>fixentries_text_5</td>\n",
       "      <td>8</td>\n",
       "      <td>Ġthe Ġindividual Ġher it ability Ġof Ġintellig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>Fragments_of_Jade vs Fragments_of_Jade</td>\n",
       "      <td>Fragments_of_Jade</td>\n",
       "      <td>Fragments_of_Jade</td>\n",
       "      <td>fragments_of_jade_text_2</td>\n",
       "      <td>fragments_of_jade_text_10</td>\n",
       "      <td>8</td>\n",
       "      <td>Ġme , Ġand Ġit 's Ġgetting Ġold .Ċ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>Erigu vs Erigu</td>\n",
       "      <td>Erigu</td>\n",
       "      <td>Erigu</td>\n",
       "      <td>erigu_text_4</td>\n",
       "      <td>erigu_text_3</td>\n",
       "      <td>3</td>\n",
       "      <td>, Ġry ulong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>Haymaker vs Haymaker</td>\n",
       "      <td>Haymaker</td>\n",
       "      <td>Haymaker</td>\n",
       "      <td>haymaker_text_4</td>\n",
       "      <td>haymaker_text_2</td>\n",
       "      <td>3</td>\n",
       "      <td>, Ġbut Ġrather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>Hardyplants vs Hardyplants</td>\n",
       "      <td>Hardyplants</td>\n",
       "      <td>Hardyplants</td>\n",
       "      <td>hardyplants_text_1</td>\n",
       "      <td>hardyplants_text_4</td>\n",
       "      <td>3</td>\n",
       "      <td>, Ġso Ġthat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>Hipocrite vs Hipocrite</td>\n",
       "      <td>Hipocrite</td>\n",
       "      <td>Hipocrite</td>\n",
       "      <td>hipocrite_text_5</td>\n",
       "      <td>hipocrite_text_4</td>\n",
       "      <td>3</td>\n",
       "      <td>Ġdo Ġnot Ġhave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>Gtadoc vs Gtadoc</td>\n",
       "      <td>Gtadoc</td>\n",
       "      <td>Gtadoc</td>\n",
       "      <td>gtadoc_text_4</td>\n",
       "      <td>gtadoc_text_2</td>\n",
       "      <td>3</td>\n",
       "      <td>, Ġand Ġthe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    problem       known_author  \\\n",
       "379                        Greg_L vs Greg_L             Greg_L   \n",
       "432                    Haymaker vs Haymaker           Haymaker   \n",
       "248        DonaNobisPacem vs DonaNobisPacem     DonaNobisPacem   \n",
       "337                Fixentries vs Fixentries         Fixentries   \n",
       "354  Fragments_of_Jade vs Fragments_of_Jade  Fragments_of_Jade   \n",
       "..                                      ...                ...   \n",
       "302                          Erigu vs Erigu              Erigu   \n",
       "433                    Haymaker vs Haymaker           Haymaker   \n",
       "420              Hardyplants vs Hardyplants        Hardyplants   \n",
       "446                  Hipocrite vs Hipocrite          Hipocrite   \n",
       "386                        Gtadoc vs Gtadoc             Gtadoc   \n",
       "\n",
       "        unknown_author              known_doc_id             unknown_doc_id  \\\n",
       "379             Greg_L            greg_l_text_11             greg_l_text_10   \n",
       "432           Haymaker           haymaker_text_3            haymaker_text_2   \n",
       "248     DonaNobisPacem     donanobispacem_text_5      donanobispacem_text_2   \n",
       "337         Fixentries         fixentries_text_2          fixentries_text_5   \n",
       "354  Fragments_of_Jade  fragments_of_jade_text_2  fragments_of_jade_text_10   \n",
       "..                 ...                       ...                        ...   \n",
       "302              Erigu              erigu_text_4               erigu_text_3   \n",
       "433           Haymaker           haymaker_text_4            haymaker_text_2   \n",
       "420        Hardyplants        hardyplants_text_1         hardyplants_text_4   \n",
       "446          Hipocrite          hipocrite_text_5           hipocrite_text_4   \n",
       "386             Gtadoc             gtadoc_text_4              gtadoc_text_2   \n",
       "\n",
       "     highest_common_count                               highest_common_ngram  \n",
       "379                     9  , Ġthey Ġshould Ġhave Ġparticipated Ġin Ġthe Ġ...  \n",
       "432                     9              Ġat Ġthe Ġend Ġof Ġthe Ġday , Ġwe 're  \n",
       "248                     8                          Ġafter Ġ 1 8 - 2 0 Ġweeks  \n",
       "337                     8  Ġthe Ġindividual Ġher it ability Ġof Ġintellig...  \n",
       "354                     8                 Ġme , Ġand Ġit 's Ġgetting Ġold .Ċ  \n",
       "..                    ...                                                ...  \n",
       "302                     3                                        , Ġry ulong  \n",
       "433                     3                                     , Ġbut Ġrather  \n",
       "420                     3                                        , Ġso Ġthat  \n",
       "446                     3                                     Ġdo Ġnot Ġhave  \n",
       "386                     3                                        , Ġand Ġthe  \n",
       "\n",
       "[197 rows x 7 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_probs = problem_dataset_agg[problem_dataset_agg['known_author'] == problem_dataset_agg['unknown_author']].copy()\n",
    "same_probs.sort_values([\"highest_common_count\"], ascending=[False], inplace=True)\n",
    "same_probs[(same_probs['highest_common_count'] >= 3) & (same_probs['highest_common_count'] <= 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2eb7681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>known_author</th>\n",
       "      <th>unknown_author</th>\n",
       "      <th>known_doc_id</th>\n",
       "      <th>unknown_doc_id</th>\n",
       "      <th>highest_common_count</th>\n",
       "      <th>highest_common_ngram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Caboga vs Chanakyathegreat</td>\n",
       "      <td>Caboga</td>\n",
       "      <td>Chanakyathegreat</td>\n",
       "      <td>caboga_text_5</td>\n",
       "      <td>chanakyathegreat_text_1</td>\n",
       "      <td>5</td>\n",
       "      <td>Ġ 2 0 0 8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>Fyunck(click) vs Garda40</td>\n",
       "      <td>Fyunck(click)</td>\n",
       "      <td>Garda40</td>\n",
       "      <td>fyunck_click_text_12</td>\n",
       "      <td>garda40_text_1</td>\n",
       "      <td>5</td>\n",
       "      <td>Ġ 2 0 1 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>Fyunck(click) vs Garda40</td>\n",
       "      <td>Fyunck(click)</td>\n",
       "      <td>Garda40</td>\n",
       "      <td>fyunck_click_text_2</td>\n",
       "      <td>garda40_text_1</td>\n",
       "      <td>5</td>\n",
       "      <td>Ġ 2 0 1 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>HeadleyDown vs Hipocrite</td>\n",
       "      <td>HeadleyDown</td>\n",
       "      <td>Hipocrite</td>\n",
       "      <td>headleydown_text_2</td>\n",
       "      <td>hipocrite_text_4</td>\n",
       "      <td>5</td>\n",
       "      <td>Ġon Ġthis Ġtalk Ġpage .Ċ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>Gwen_Gale vs Habap</td>\n",
       "      <td>Gwen_Gale</td>\n",
       "      <td>Habap</td>\n",
       "      <td>gwen_gale_text_3</td>\n",
       "      <td>habap_text_1</td>\n",
       "      <td>5</td>\n",
       "      <td>, Ġbut Ġi Ġdon 't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>Fipplet vs Fixentries</td>\n",
       "      <td>Fipplet</td>\n",
       "      <td>Fixentries</td>\n",
       "      <td>fipplet_text_1</td>\n",
       "      <td>fixentries_text_5</td>\n",
       "      <td>3</td>\n",
       "      <td>.Ċ it 's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>Falcon9x5 vs Fipplet</td>\n",
       "      <td>Falcon9x5</td>\n",
       "      <td>Fipplet</td>\n",
       "      <td>falcon9x5_text_2</td>\n",
       "      <td>fipplet_text_5</td>\n",
       "      <td>3</td>\n",
       "      <td>.Ċ it Ġwas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>Fakirbakir vs Falcon9x5</td>\n",
       "      <td>Fakirbakir</td>\n",
       "      <td>Falcon9x5</td>\n",
       "      <td>fakirbakir_text_3</td>\n",
       "      <td>falcon9x5_text_1</td>\n",
       "      <td>3</td>\n",
       "      <td>, Ġbut Ġthe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>Fragments_of_Jade vs Fyunck(click)</td>\n",
       "      <td>Fragments_of_Jade</td>\n",
       "      <td>Fyunck(click)</td>\n",
       "      <td>fragments_of_jade_text_2</td>\n",
       "      <td>fyunck_click_text_1</td>\n",
       "      <td>3</td>\n",
       "      <td>.Ċ it 's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>Fixentries vs Flamarande</td>\n",
       "      <td>Fixentries</td>\n",
       "      <td>Flamarande</td>\n",
       "      <td>fixentries_text_2</td>\n",
       "      <td>flamarande_text_1</td>\n",
       "      <td>3</td>\n",
       "      <td>i 'm Ġnot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                problem       known_author    unknown_author  \\\n",
       "161          Caboga vs Chanakyathegreat             Caboga  Chanakyathegreat   \n",
       "364            Fyunck(click) vs Garda40      Fyunck(click)           Garda40   \n",
       "365            Fyunck(click) vs Garda40      Fyunck(click)           Garda40   \n",
       "441            HeadleyDown vs Hipocrite        HeadleyDown         Hipocrite   \n",
       "395                  Gwen_Gale vs Habap          Gwen_Gale             Habap   \n",
       "..                                  ...                ...               ...   \n",
       "333               Fipplet vs Fixentries            Fipplet        Fixentries   \n",
       "327                Falcon9x5 vs Fipplet          Falcon9x5           Fipplet   \n",
       "323             Fakirbakir vs Falcon9x5         Fakirbakir         Falcon9x5   \n",
       "357  Fragments_of_Jade vs Fyunck(click)  Fragments_of_Jade     Fyunck(click)   \n",
       "340            Fixentries vs Flamarande         Fixentries        Flamarande   \n",
       "\n",
       "                 known_doc_id           unknown_doc_id  highest_common_count  \\\n",
       "161             caboga_text_5  chanakyathegreat_text_1                     5   \n",
       "364      fyunck_click_text_12           garda40_text_1                     5   \n",
       "365       fyunck_click_text_2           garda40_text_1                     5   \n",
       "441        headleydown_text_2         hipocrite_text_4                     5   \n",
       "395          gwen_gale_text_3             habap_text_1                     5   \n",
       "..                        ...                      ...                   ...   \n",
       "333            fipplet_text_1        fixentries_text_5                     3   \n",
       "327          falcon9x5_text_2           fipplet_text_5                     3   \n",
       "323         fakirbakir_text_3         falcon9x5_text_1                     3   \n",
       "357  fragments_of_jade_text_2      fyunck_click_text_1                     3   \n",
       "340         fixentries_text_2        flamarande_text_1                     3   \n",
       "\n",
       "         highest_common_ngram  \n",
       "161                 Ġ 2 0 0 8  \n",
       "364                 Ġ 2 0 1 1  \n",
       "365                 Ġ 2 0 1 2  \n",
       "441  Ġon Ġthis Ġtalk Ġpage .Ċ  \n",
       "395         , Ġbut Ġi Ġdon 't  \n",
       "..                        ...  \n",
       "333                  .Ċ it 's  \n",
       "327                .Ċ it Ġwas  \n",
       "323               , Ġbut Ġthe  \n",
       "357                  .Ċ it 's  \n",
       "340                 i 'm Ġnot  \n",
       "\n",
       "[179 rows x 7 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_probs = problem_dataset_agg[problem_dataset_agg['known_author'] != problem_dataset_agg['unknown_author']].copy()\n",
    "diff_probs.sort_values([\"highest_common_count\"], ascending=[False], inplace=True)\n",
    "diff_probs[(diff_probs['highest_common_count'] >= 3) & (diff_probs['highest_common_count'] <= 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a0c885",
   "metadata": {},
   "source": [
    "## Select Known and Unknown Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb6993e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on problem: Fixentries vs Fixentries\n"
     ]
    }
   ],
   "source": [
    "known_doc = \"fixentries_text_2\"\n",
    "known_text = known[known['doc_id'] == known_doc].reset_index().loc[0, 'text']\n",
    "\n",
    "unknown_doc = \"fixentries_text_5\"\n",
    "unknown_text = unknown[unknown['doc_id'] == unknown_doc].reset_index().loc[0, 'text']\n",
    "\n",
    "# Get the metadata for current problem, will be added to Excel\n",
    "p_metadata = agg_metadata[(agg_metadata['known_doc_id'] == known_doc) \n",
    "                          & ((agg_metadata['unknown_doc_id'] == unknown_doc))].reset_index()\n",
    "p_metadata['target'] = p_metadata['known_author'] == p_metadata['unknown_author']\n",
    "specific_problem = p_metadata.loc[0, 'problem']\n",
    "\n",
    "print(f\"Working on problem: {specific_problem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48292f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = pd.DataFrame(\n",
    "    {\n",
    "        \"known\":   [corpus, data_type, known_doc, known_text],\n",
    "        \"unknown\": [corpus, data_type, unknown_doc, unknown_text],\n",
    "    },\n",
    "    index=[\"corpus\", \"data type\", \"doc\", \"text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bdf3b5",
   "metadata": {},
   "source": [
    "## Get N-Grams in Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b72faabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' the individual heritability of intelligence.\\n',\n",
       " \".\\ni'm not\",\n",
       " ' feel free to',\n",
       " ' in the article',\n",
       " ' are not',\n",
       " ' article,',\n",
       " ' focus on',\n",
       " ' for any',\n",
       " ' here.\\n',\n",
       " ' i think',\n",
       " ' if it',\n",
       " ' if you',\n",
       " ' is not',\n",
       " ' meant to',\n",
       " ' need to',\n",
       " ' of a',\n",
       " ' of an',\n",
       " ' of the',\n",
       " ' on the',\n",
       " ' that this',\n",
       " ' there is',\n",
       " ' this article',\n",
       " ' to make',\n",
       " ', and',\n",
       " ', or',\n",
       " '.\\nalso',\n",
       " '.\\nthis']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common = common_ngrams(known_text, unknown_text, 2, model, tokenizer, lowercase=True)\n",
    "n_gram_list = pretty_print_common_ngrams(common, tokenizer=tokenizer, order='len_desc', return_format='flat')\n",
    "n_gram_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eded3645",
   "metadata": {},
   "source": [
    "## Initialise OpenAI Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f9b6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = initialise_client(\"../../../credentials.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e319289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_prompt(known_text, phrase):\n",
    "    user_prompt = f\"\"\"\n",
    "<DOC>\n",
    "{known_text}\n",
    "</DOC>\n",
    "<NGRAM>\n",
    "\"{phrase}\"\n",
    "</NGRAM>\n",
    "\"\"\"\n",
    "    \n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e3b7d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_system_prompt(prompt_loc):\n",
    "    with open(prompt_loc, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bca10da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_paraphrases(response, phrase):\n",
    "    \n",
    "    paraphrase_list = []\n",
    "    for i in range(1, len(response.choices)):\n",
    "        content = response.choices[i].message.content\n",
    "        \n",
    "        try:\n",
    "            content_json = json.loads(content)\n",
    "            for para in content_json['paraphrases']:\n",
    "                if para != phrase:\n",
    "                    paraphrase_list.append(para)  \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    unique_list = list(set(paraphrase_list))\n",
    "    \n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85daabcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = create_system_prompt(\"../../../prompts/exhaustive_constrained_ngram_paraphraser_prompt_JSON.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2466a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_dict = {}\n",
    "width = len(str(len(n_gram_list)))  # e.g., 10 -> 2, 100 -> 3\n",
    "\n",
    "for idx, phrase in enumerate(n_gram_list, start=1):\n",
    "    user_prompt = create_user_prompt(known_text, phrase)\n",
    "    response = llm(\n",
    "        system_prompt,\n",
    "        user_prompt,\n",
    "        client,\n",
    "        model=\"gpt-4.1\",\n",
    "        max_tokens=5000,\n",
    "        temperature=0.7,\n",
    "        n=10,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    paraphrases = parse_paraphrases(response, phrase)\n",
    "    key = f\"phrase_{idx:0{width}d}\"  # -> phrase_01, phrase_002, etc.\n",
    "    n_gram_dict[key] = {\"phrase\": phrase, \"paraphrases\": paraphrases}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd80e569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_phrases(\n",
    "    base_text: str,\n",
    "    ref_phrase: str,\n",
    "    paraphrases: list[str],\n",
    "    tokenizer,\n",
    "    model\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns rows for the reference and each paraphrase with:\n",
    "      sum_log_probs_phrase (log-likelihood for the phrase tokens) and\n",
    "      raw_prob = exp(sum_log_probs_phrase)\n",
    "    \"\"\"\n",
    "    # 1) score base_text\n",
    "    _, log_probs_base, _ = compute_log_probs_with_median(base_text.strip(), tokenizer, model)\n",
    "    base_total = sum(log_probs_base)\n",
    "\n",
    "    items = [(\"reference\", ref_phrase)] + [(\"paraphrase\", p) for p in paraphrases]\n",
    "    rows = []\n",
    "\n",
    "    for ptype, phrase in items:\n",
    "        # a) phrase alone → token count\n",
    "        tokens_phrase, log_probs_phrase, _ = compute_log_probs_with_median(phrase, tokenizer, model)\n",
    "        n_phrase_tokens = len(tokens_phrase)\n",
    "\n",
    "        # b) full sequence\n",
    "        full_text = base_text + phrase\n",
    "        tokens_full, log_probs_full, _ = compute_log_probs_with_median(full_text, tokenizer, model)\n",
    "\n",
    "        # c) full sum (base + phrase)\n",
    "        sum_before = sum(log_probs_full)\n",
    "\n",
    "        # d/e) last n tokens correspond to phrase\n",
    "        phrase_tokens    = tokens_full[-n_phrase_tokens:]\n",
    "        phrase_log_probs = log_probs_full[-n_phrase_tokens:]\n",
    "\n",
    "        # f) totals\n",
    "        phrase_total = sum(phrase_log_probs)\n",
    "        difference   = base_total - sum_before  # typically == -phrase_total\n",
    "\n",
    "        # raw (unnormalized) probability of the phrase given the base\n",
    "        raw_prob = math.exp(phrase_total)  # may underflow to 0.0 for long phrases; that's fine\n",
    "\n",
    "        rows.append({\n",
    "            \"phrase_type\":               ptype,\n",
    "            \"phrase\":                    phrase,\n",
    "            \"tokens\":                    phrase_tokens,\n",
    "            \"sum_log_probs_base\":        base_total,\n",
    "            \"sum_log_probs_inc_phrase\":  sum_before,\n",
    "            \"difference\":                difference,\n",
    "            \"phrase_log_probs\":          phrase_log_probs,\n",
    "            \"sum_log_probs_phrase\":      phrase_total,\n",
    "            \"raw_prob\":                  raw_prob,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\n",
    "        \"phrase_type\", \"phrase\", \"tokens\",\n",
    "        \"sum_log_probs_base\", \"sum_log_probs_inc_phrase\",\n",
    "        \"difference\", \"phrase_log_probs\", \"sum_log_probs_phrase\",\n",
    "        \"raw_prob\",\n",
    "    ])\n",
    "\n",
    "def get_scored_df(n_gram_dict, full_text, tokenizer, model):\n",
    "    \"\"\"Row-concat each scored df, add phrase_num, sort, then rank paraphrases within each phrase_num.\"\"\"\n",
    "    dfs = []\n",
    "    for phrase_num, entry in n_gram_dict.items():  # relies on insertion order\n",
    "        print(f\"Processing Phrase - {phrase_num}\")\n",
    "        phrase = entry[\"phrase\"]\n",
    "        paraphrases = entry[\"paraphrases\"]\n",
    "\n",
    "        base_text = keep_before_phrase(full_text, phrase)\n",
    "\n",
    "        df = score_phrases(base_text, phrase, paraphrases, tokenizer, model).copy()\n",
    "        df.insert(0, \"original_phrase\", phrase)\n",
    "        df.insert(0, \"phrase_num\", phrase_num)  # first column\n",
    "        dfs.append(df)\n",
    "\n",
    "    if not dfs:\n",
    "        return pd.DataFrame(columns=[\"phrase_num\"])\n",
    "\n",
    "    out = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # sort by phrase_num (zero-padded → lexicographic == numeric)\n",
    "    out = out.sort_values(\"phrase_num\", kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "    # rank within phrase_num: reference -> 0; paraphrases ranked by descending raw_prob starting at 1\n",
    "    out[\"rank\"] = None\n",
    "    mask = out[\"phrase_type\"].eq(\"paraphrase\")\n",
    "    out.loc[mask, \"rank\"] = (\n",
    "        out.loc[mask]\n",
    "           .groupby(\"phrase_num\")[\"raw_prob\"]\n",
    "           .rank(method=\"first\", ascending=False)\n",
    "           .astype(int)\n",
    "    )\n",
    "    out.loc[out[\"phrase_type\"].eq(\"reference\"), \"rank\"] = 0\n",
    "    out[\"rank\"] = out[\"rank\"].astype(int)\n",
    "\n",
    "    out = out.sort_values([\"phrase_num\", \"rank\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc75faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Phrase - phrase_01\n",
      "Processing Phrase - phrase_02\n",
      "Processing Phrase - phrase_03\n",
      "Processing Phrase - phrase_04\n",
      "Processing Phrase - phrase_05\n",
      "Processing Phrase - phrase_06\n",
      "Processing Phrase - phrase_07\n",
      "Processing Phrase - phrase_08\n",
      "Processing Phrase - phrase_09\n",
      "Processing Phrase - phrase_10\n"
     ]
    }
   ],
   "source": [
    "known_scored = get_scored_df(n_gram_dict, known_text, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f663057",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_scored = get_scored_df(n_gram_dict, unknown_text, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65e3b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_phrases_no_context(\n",
    "    ref_phrase: str,\n",
    "    paraphrases: List[str],\n",
    "    tokenizer,\n",
    "    model\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Score the reference and each paraphrase *without* any ranking.\n",
    "    Returns:\n",
    "      phrase_type, phrase, tokens, log_probs, sum_log_probs, raw_prob\n",
    "    \"\"\"\n",
    "    items = [(\"reference\", ref_phrase)] + [(\"paraphrase\", p) for p in paraphrases]\n",
    "    rows = []\n",
    "\n",
    "    for idx, (ptype, phrase) in enumerate(items, start=1):\n",
    "        print(f\"→ [{idx}/{len(items)}] Processing {ptype}…\")\n",
    "        tokens_phrase, log_probs_phrase, _ = compute_log_probs_with_median(phrase, tokenizer, model)\n",
    "        phrase_total = sum(log_probs_phrase)\n",
    "        raw_prob = math.exp(phrase_total)  # unnormalized prob\n",
    "\n",
    "        rows.append({\n",
    "            \"phrase_type\":   ptype,\n",
    "            \"phrase\":        phrase,\n",
    "            \"tokens\":        tokens_phrase,\n",
    "            \"log_probs\":     log_probs_phrase,\n",
    "            \"sum_log_probs\": phrase_total,\n",
    "            \"raw_prob\":      raw_prob,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\n",
    "        \"phrase_type\", \"phrase\", \"tokens\", \"log_probs\", \"sum_log_probs\", \"raw_prob\"\n",
    "    ])\n",
    "\n",
    "def get_scored_df_no_context(n_gram_dict, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Row-concat each score_phrases_no_context df, add phrase_num, sort by phrase_num,\n",
    "    then rank paraphrases within each phrase_num by descending raw_prob.\n",
    "    'reference' rows always get rank 0.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for phrase_num, entry in n_gram_dict.items():  # insertion order preserved\n",
    "        print(f\"Processing Phrase - {phrase_num}\")\n",
    "        phrase = entry[\"phrase\"]\n",
    "        paraphrases = entry[\"paraphrases\"]\n",
    "\n",
    "        df = score_phrases_no_context(phrase, paraphrases, tokenizer, model).copy()\n",
    "        df.insert(0, \"original_phrase\", phrase)\n",
    "        df.insert(0, \"phrase_num\", phrase_num)  # make it the first column\n",
    "        dfs.append(df)\n",
    "\n",
    "    if not dfs:\n",
    "        return pd.DataFrame(columns=[\"phrase_num\"])\n",
    "\n",
    "    out = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # zero-padded keys => lexicographic equals numeric order\n",
    "    out = out.sort_values(\"phrase_num\", kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "    # Rank within phrase_num: reference -> 0; paraphrases ranked by descending raw_prob starting at 1\n",
    "    out[\"rank\"] = None\n",
    "    mask_para = out[\"phrase_type\"].eq(\"paraphrase\")\n",
    "    out.loc[mask_para, \"rank\"] = (\n",
    "        out.loc[mask_para]\n",
    "           .groupby(\"phrase_num\")[\"raw_prob\"]\n",
    "           .rank(method=\"first\", ascending=False)  # use \"dense\" if you prefer 1,2,3 without gaps\n",
    "           .astype(int)\n",
    "    )\n",
    "    out.loc[out[\"phrase_type\"].eq(\"reference\"), \"rank\"] = 0\n",
    "    out[\"rank\"] = out[\"rank\"].astype(int)\n",
    "\n",
    "    out = out.sort_values([\"phrase_num\", \"rank\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa3c002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ [1/58] Processing reference…\n",
      "→ [2/58] Processing paraphrase…\n",
      "→ [3/58] Processing paraphrase…\n",
      "→ [4/58] Processing paraphrase…\n",
      "→ [5/58] Processing paraphrase…\n",
      "→ [6/58] Processing paraphrase…\n",
      "→ [7/58] Processing paraphrase…\n",
      "→ [8/58] Processing paraphrase…\n",
      "→ [9/58] Processing paraphrase…\n",
      "→ [10/58] Processing paraphrase…\n",
      "→ [11/58] Processing paraphrase…\n",
      "→ [12/58] Processing paraphrase…\n",
      "→ [13/58] Processing paraphrase…\n",
      "→ [14/58] Processing paraphrase…\n",
      "→ [15/58] Processing paraphrase…\n",
      "→ [16/58] Processing paraphrase…\n",
      "→ [17/58] Processing paraphrase…\n",
      "→ [18/58] Processing paraphrase…\n",
      "→ [19/58] Processing paraphrase…\n",
      "→ [20/58] Processing paraphrase…\n",
      "→ [21/58] Processing paraphrase…\n",
      "→ [22/58] Processing paraphrase…\n",
      "→ [23/58] Processing paraphrase…\n",
      "→ [24/58] Processing paraphrase…\n",
      "→ [25/58] Processing paraphrase…\n",
      "→ [26/58] Processing paraphrase…\n",
      "→ [27/58] Processing paraphrase…\n",
      "→ [28/58] Processing paraphrase…\n",
      "→ [29/58] Processing paraphrase…\n",
      "→ [30/58] Processing paraphrase…\n",
      "→ [31/58] Processing paraphrase…\n",
      "→ [32/58] Processing paraphrase…\n",
      "→ [33/58] Processing paraphrase…\n",
      "→ [34/58] Processing paraphrase…\n",
      "→ [35/58] Processing paraphrase…\n",
      "→ [36/58] Processing paraphrase…\n",
      "→ [37/58] Processing paraphrase…\n",
      "→ [38/58] Processing paraphrase…\n",
      "→ [39/58] Processing paraphrase…\n",
      "→ [40/58] Processing paraphrase…\n",
      "→ [41/58] Processing paraphrase…\n",
      "→ [42/58] Processing paraphrase…\n",
      "→ [43/58] Processing paraphrase…\n",
      "→ [44/58] Processing paraphrase…\n",
      "→ [45/58] Processing paraphrase…\n",
      "→ [46/58] Processing paraphrase…\n",
      "→ [47/58] Processing paraphrase…\n",
      "→ [48/58] Processing paraphrase…\n",
      "→ [49/58] Processing paraphrase…\n",
      "→ [50/58] Processing paraphrase…\n",
      "→ [51/58] Processing paraphrase…\n",
      "→ [52/58] Processing paraphrase…\n",
      "→ [53/58] Processing paraphrase…\n",
      "→ [54/58] Processing paraphrase…\n",
      "→ [55/58] Processing paraphrase…\n",
      "→ [56/58] Processing paraphrase…\n",
      "→ [57/58] Processing paraphrase…\n",
      "→ [58/58] Processing paraphrase…\n",
      "→ [1/12] Processing reference…\n",
      "→ [2/12] Processing paraphrase…\n",
      "→ [3/12] Processing paraphrase…\n",
      "→ [4/12] Processing paraphrase…\n",
      "→ [5/12] Processing paraphrase…\n",
      "→ [6/12] Processing paraphrase…\n",
      "→ [7/12] Processing paraphrase…\n",
      "→ [8/12] Processing paraphrase…\n",
      "→ [9/12] Processing paraphrase…\n",
      "→ [10/12] Processing paraphrase…\n",
      "→ [11/12] Processing paraphrase…\n",
      "→ [12/12] Processing paraphrase…\n",
      "→ [1/28] Processing reference…\n",
      "→ [2/28] Processing paraphrase…\n",
      "→ [3/28] Processing paraphrase…\n",
      "→ [4/28] Processing paraphrase…\n",
      "→ [5/28] Processing paraphrase…\n",
      "→ [6/28] Processing paraphrase…\n",
      "→ [7/28] Processing paraphrase…\n",
      "→ [8/28] Processing paraphrase…\n",
      "→ [9/28] Processing paraphrase…\n",
      "→ [10/28] Processing paraphrase…\n",
      "→ [11/28] Processing paraphrase…\n",
      "→ [12/28] Processing paraphrase…\n",
      "→ [13/28] Processing paraphrase…\n",
      "→ [14/28] Processing paraphrase…\n",
      "→ [15/28] Processing paraphrase…\n",
      "→ [16/28] Processing paraphrase…\n",
      "→ [17/28] Processing paraphrase…\n",
      "→ [18/28] Processing paraphrase…\n",
      "→ [19/28] Processing paraphrase…\n",
      "→ [20/28] Processing paraphrase…\n",
      "→ [21/28] Processing paraphrase…\n",
      "→ [22/28] Processing paraphrase…\n",
      "→ [23/28] Processing paraphrase…\n",
      "→ [24/28] Processing paraphrase…\n",
      "→ [25/28] Processing paraphrase…\n",
      "→ [26/28] Processing paraphrase…\n",
      "→ [27/28] Processing paraphrase…\n",
      "→ [28/28] Processing paraphrase…\n",
      "→ [1/20] Processing reference…\n",
      "→ [2/20] Processing paraphrase…\n",
      "→ [3/20] Processing paraphrase…\n",
      "→ [4/20] Processing paraphrase…\n",
      "→ [5/20] Processing paraphrase…\n",
      "→ [6/20] Processing paraphrase…\n",
      "→ [7/20] Processing paraphrase…\n",
      "→ [8/20] Processing paraphrase…\n",
      "→ [9/20] Processing paraphrase…\n",
      "→ [10/20] Processing paraphrase…\n",
      "→ [11/20] Processing paraphrase…\n",
      "→ [12/20] Processing paraphrase…\n",
      "→ [13/20] Processing paraphrase…\n",
      "→ [14/20] Processing paraphrase…\n",
      "→ [15/20] Processing paraphrase…\n",
      "→ [16/20] Processing paraphrase…\n",
      "→ [17/20] Processing paraphrase…\n",
      "→ [18/20] Processing paraphrase…\n",
      "→ [19/20] Processing paraphrase…\n",
      "→ [20/20] Processing paraphrase…\n",
      "→ [1/33] Processing reference…\n",
      "→ [2/33] Processing paraphrase…\n",
      "→ [3/33] Processing paraphrase…\n",
      "→ [4/33] Processing paraphrase…\n",
      "→ [5/33] Processing paraphrase…\n",
      "→ [6/33] Processing paraphrase…\n",
      "→ [7/33] Processing paraphrase…\n",
      "→ [8/33] Processing paraphrase…\n",
      "→ [9/33] Processing paraphrase…\n",
      "→ [10/33] Processing paraphrase…\n",
      "→ [11/33] Processing paraphrase…\n",
      "→ [12/33] Processing paraphrase…\n",
      "→ [13/33] Processing paraphrase…\n",
      "→ [14/33] Processing paraphrase…\n",
      "→ [15/33] Processing paraphrase…\n",
      "→ [16/33] Processing paraphrase…\n",
      "→ [17/33] Processing paraphrase…\n",
      "→ [18/33] Processing paraphrase…\n",
      "→ [19/33] Processing paraphrase…\n",
      "→ [20/33] Processing paraphrase…\n",
      "→ [21/33] Processing paraphrase…\n",
      "→ [22/33] Processing paraphrase…\n",
      "→ [23/33] Processing paraphrase…\n",
      "→ [24/33] Processing paraphrase…\n",
      "→ [25/33] Processing paraphrase…\n",
      "→ [26/33] Processing paraphrase…\n",
      "→ [27/33] Processing paraphrase…\n",
      "→ [28/33] Processing paraphrase…\n",
      "→ [29/33] Processing paraphrase…\n",
      "→ [30/33] Processing paraphrase…\n",
      "→ [31/33] Processing paraphrase…\n",
      "→ [32/33] Processing paraphrase…\n",
      "→ [33/33] Processing paraphrase…\n",
      "→ [1/16] Processing reference…\n",
      "→ [2/16] Processing paraphrase…\n",
      "→ [3/16] Processing paraphrase…\n",
      "→ [4/16] Processing paraphrase…\n",
      "→ [5/16] Processing paraphrase…\n",
      "→ [6/16] Processing paraphrase…\n",
      "→ [7/16] Processing paraphrase…\n",
      "→ [8/16] Processing paraphrase…\n",
      "→ [9/16] Processing paraphrase…\n",
      "→ [10/16] Processing paraphrase…\n",
      "→ [11/16] Processing paraphrase…\n",
      "→ [12/16] Processing paraphrase…\n",
      "→ [13/16] Processing paraphrase…\n",
      "→ [14/16] Processing paraphrase…\n",
      "→ [15/16] Processing paraphrase…\n",
      "→ [16/16] Processing paraphrase…\n",
      "→ [1/4] Processing reference…\n",
      "→ [2/4] Processing paraphrase…\n",
      "→ [3/4] Processing paraphrase…\n",
      "→ [4/4] Processing paraphrase…\n",
      "→ [1/22] Processing reference…\n",
      "→ [2/22] Processing paraphrase…\n",
      "→ [3/22] Processing paraphrase…\n",
      "→ [4/22] Processing paraphrase…\n",
      "→ [5/22] Processing paraphrase…\n",
      "→ [6/22] Processing paraphrase…\n",
      "→ [7/22] Processing paraphrase…\n",
      "→ [8/22] Processing paraphrase…\n",
      "→ [9/22] Processing paraphrase…\n",
      "→ [10/22] Processing paraphrase…\n",
      "→ [11/22] Processing paraphrase…\n",
      "→ [12/22] Processing paraphrase…\n",
      "→ [13/22] Processing paraphrase…\n",
      "→ [14/22] Processing paraphrase…\n",
      "→ [15/22] Processing paraphrase…\n",
      "→ [16/22] Processing paraphrase…\n",
      "→ [17/22] Processing paraphrase…\n",
      "→ [18/22] Processing paraphrase…\n",
      "→ [19/22] Processing paraphrase…\n",
      "→ [20/22] Processing paraphrase…\n",
      "→ [21/22] Processing paraphrase…\n",
      "→ [22/22] Processing paraphrase…\n",
      "→ [1/7] Processing reference…\n",
      "→ [2/7] Processing paraphrase…\n",
      "→ [3/7] Processing paraphrase…\n",
      "→ [4/7] Processing paraphrase…\n",
      "→ [5/7] Processing paraphrase…\n",
      "→ [6/7] Processing paraphrase…\n",
      "→ [7/7] Processing paraphrase…\n",
      "→ [1/4] Processing reference…\n",
      "→ [2/4] Processing paraphrase…\n",
      "→ [3/4] Processing paraphrase…\n",
      "→ [4/4] Processing paraphrase…\n",
      "→ [1/31] Processing reference…\n",
      "→ [2/31] Processing paraphrase…\n",
      "→ [3/31] Processing paraphrase…\n",
      "→ [4/31] Processing paraphrase…\n",
      "→ [5/31] Processing paraphrase…\n",
      "→ [6/31] Processing paraphrase…\n",
      "→ [7/31] Processing paraphrase…\n",
      "→ [8/31] Processing paraphrase…\n",
      "→ [9/31] Processing paraphrase…\n",
      "→ [10/31] Processing paraphrase…\n",
      "→ [11/31] Processing paraphrase…\n",
      "→ [12/31] Processing paraphrase…\n",
      "→ [13/31] Processing paraphrase…\n",
      "→ [14/31] Processing paraphrase…\n",
      "→ [15/31] Processing paraphrase…\n",
      "→ [16/31] Processing paraphrase…\n",
      "→ [17/31] Processing paraphrase…\n",
      "→ [18/31] Processing paraphrase…\n",
      "→ [19/31] Processing paraphrase…\n",
      "→ [20/31] Processing paraphrase…\n",
      "→ [21/31] Processing paraphrase…\n",
      "→ [22/31] Processing paraphrase…\n",
      "→ [23/31] Processing paraphrase…\n",
      "→ [24/31] Processing paraphrase…\n",
      "→ [25/31] Processing paraphrase…\n",
      "→ [26/31] Processing paraphrase…\n",
      "→ [27/31] Processing paraphrase…\n",
      "→ [28/31] Processing paraphrase…\n",
      "→ [29/31] Processing paraphrase…\n",
      "→ [30/31] Processing paraphrase…\n",
      "→ [31/31] Processing paraphrase…\n",
      "→ [1/17] Processing reference…\n",
      "→ [2/17] Processing paraphrase…\n",
      "→ [3/17] Processing paraphrase…\n",
      "→ [4/17] Processing paraphrase…\n",
      "→ [5/17] Processing paraphrase…\n",
      "→ [6/17] Processing paraphrase…\n",
      "→ [7/17] Processing paraphrase…\n",
      "→ [8/17] Processing paraphrase…\n",
      "→ [9/17] Processing paraphrase…\n",
      "→ [10/17] Processing paraphrase…\n",
      "→ [11/17] Processing paraphrase…\n",
      "→ [12/17] Processing paraphrase…\n",
      "→ [13/17] Processing paraphrase…\n",
      "→ [14/17] Processing paraphrase…\n",
      "→ [15/17] Processing paraphrase…\n",
      "→ [16/17] Processing paraphrase…\n",
      "→ [17/17] Processing paraphrase…\n",
      "→ [1/52] Processing reference…\n",
      "→ [2/52] Processing paraphrase…\n",
      "→ [3/52] Processing paraphrase…\n",
      "→ [4/52] Processing paraphrase…\n",
      "→ [5/52] Processing paraphrase…\n",
      "→ [6/52] Processing paraphrase…\n",
      "→ [7/52] Processing paraphrase…\n",
      "→ [8/52] Processing paraphrase…\n",
      "→ [9/52] Processing paraphrase…\n",
      "→ [10/52] Processing paraphrase…\n",
      "→ [11/52] Processing paraphrase…\n",
      "→ [12/52] Processing paraphrase…\n",
      "→ [13/52] Processing paraphrase…\n",
      "→ [14/52] Processing paraphrase…\n",
      "→ [15/52] Processing paraphrase…\n",
      "→ [16/52] Processing paraphrase…\n",
      "→ [17/52] Processing paraphrase…\n",
      "→ [18/52] Processing paraphrase…\n",
      "→ [19/52] Processing paraphrase…\n",
      "→ [20/52] Processing paraphrase…\n",
      "→ [21/52] Processing paraphrase…\n",
      "→ [22/52] Processing paraphrase…\n",
      "→ [23/52] Processing paraphrase…\n",
      "→ [24/52] Processing paraphrase…\n",
      "→ [25/52] Processing paraphrase…\n",
      "→ [26/52] Processing paraphrase…\n",
      "→ [27/52] Processing paraphrase…\n",
      "→ [28/52] Processing paraphrase…\n",
      "→ [29/52] Processing paraphrase…\n",
      "→ [30/52] Processing paraphrase…\n",
      "→ [31/52] Processing paraphrase…\n",
      "→ [32/52] Processing paraphrase…\n",
      "→ [33/52] Processing paraphrase…\n",
      "→ [34/52] Processing paraphrase…\n",
      "→ [35/52] Processing paraphrase…\n",
      "→ [36/52] Processing paraphrase…\n",
      "→ [37/52] Processing paraphrase…\n",
      "→ [38/52] Processing paraphrase…\n",
      "→ [39/52] Processing paraphrase…\n",
      "→ [40/52] Processing paraphrase…\n",
      "→ [41/52] Processing paraphrase…\n",
      "→ [42/52] Processing paraphrase…\n",
      "→ [43/52] Processing paraphrase…\n",
      "→ [44/52] Processing paraphrase…\n",
      "→ [45/52] Processing paraphrase…\n",
      "→ [46/52] Processing paraphrase…\n",
      "→ [47/52] Processing paraphrase…\n",
      "→ [48/52] Processing paraphrase…\n",
      "→ [49/52] Processing paraphrase…\n",
      "→ [50/52] Processing paraphrase…\n",
      "→ [51/52] Processing paraphrase…\n",
      "→ [52/52] Processing paraphrase…\n",
      "→ [1/10] Processing reference…\n",
      "→ [2/10] Processing paraphrase…\n",
      "→ [3/10] Processing paraphrase…\n",
      "→ [4/10] Processing paraphrase…\n",
      "→ [5/10] Processing paraphrase…\n",
      "→ [6/10] Processing paraphrase…\n",
      "→ [7/10] Processing paraphrase…\n",
      "→ [8/10] Processing paraphrase…\n",
      "→ [9/10] Processing paraphrase…\n",
      "→ [10/10] Processing paraphrase…\n",
      "→ [1/11] Processing reference…\n",
      "→ [2/11] Processing paraphrase…\n",
      "→ [3/11] Processing paraphrase…\n",
      "→ [4/11] Processing paraphrase…\n",
      "→ [5/11] Processing paraphrase…\n",
      "→ [6/11] Processing paraphrase…\n",
      "→ [7/11] Processing paraphrase…\n",
      "→ [8/11] Processing paraphrase…\n",
      "→ [9/11] Processing paraphrase…\n",
      "→ [10/11] Processing paraphrase…\n",
      "→ [11/11] Processing paraphrase…\n",
      "→ [1/4] Processing reference…\n",
      "→ [2/4] Processing paraphrase…\n",
      "→ [3/4] Processing paraphrase…\n",
      "→ [4/4] Processing paraphrase…\n",
      "→ [1/8] Processing reference…\n",
      "→ [2/8] Processing paraphrase…\n",
      "→ [3/8] Processing paraphrase…\n",
      "→ [4/8] Processing paraphrase…\n",
      "→ [5/8] Processing paraphrase…\n",
      "→ [6/8] Processing paraphrase…\n",
      "→ [7/8] Processing paraphrase…\n",
      "→ [8/8] Processing paraphrase…\n",
      "→ [1/8] Processing reference…\n",
      "→ [2/8] Processing paraphrase…\n",
      "→ [3/8] Processing paraphrase…\n",
      "→ [4/8] Processing paraphrase…\n",
      "→ [5/8] Processing paraphrase…\n",
      "→ [6/8] Processing paraphrase…\n",
      "→ [7/8] Processing paraphrase…\n",
      "→ [8/8] Processing paraphrase…\n"
     ]
    }
   ],
   "source": [
    "score_df_no_context = get_scored_df_no_context(n_gram_dict, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374ff5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_phrases = score_df_no_context[['phrase_num', 'original_phrase']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c603218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove illegal control chars (keep \\t, \\n, \\r)\n",
    "_ILLEGAL_RE = re.compile(r\"[\\x00-\\x08\\x0B-\\x0C\\x0E-\\x1F]\")\n",
    "\n",
    "def _clean_cell(x):\n",
    "    if isinstance(x, str):\n",
    "        return _ILLEGAL_RE.sub(\"\", x)\n",
    "    return x\n",
    "\n",
    "def clean_for_excel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    obj_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    df[obj_cols] = df[obj_cols].applymap(_clean_cell)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfbe8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-84>:15: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "<positron-console-cell-84>:15: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "<positron-console-cell-84>:15: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "<positron-console-cell-84>:15: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "<positron-console-cell-84>:15: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "<positron-console-cell-84>:15: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n"
     ]
    }
   ],
   "source": [
    "save_loc = f\"{nas_base_loc}/paraphrase examples/{specific_problem}.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(save_loc, engine=\"openpyxl\") as xls:\n",
    "    clean_for_excel(docs_df).to_excel(xls, sheet_name=\"docs\", index=False)\n",
    "    clean_for_excel(p_metadata).to_excel(xls, sheet_name=\"metadata\", index=False)\n",
    "    clean_for_excel(score_df_no_context).to_excel(xls, sheet_name=\"no context\", index=False)\n",
    "    clean_for_excel(known_scored).to_excel(xls, sheet_name=\"known\", index=False)\n",
    "    clean_for_excel(unknown_scored).to_excel(xls, sheet_name=\"unknown\", index=False)\n",
    "    clean_for_excel(distinct_phrases).to_excel(xls, sheet_name=\"LLR\", index=False)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
