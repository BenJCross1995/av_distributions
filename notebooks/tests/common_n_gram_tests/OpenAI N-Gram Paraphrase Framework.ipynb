{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a04acc0",
   "metadata": {},
   "source": [
    "# Run Paraphrase Common Token N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500a216e",
   "metadata": {},
   "source": [
    "## Load Libraries\n",
    "\n",
    "Load libraries from standard modules and my own code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e7b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List\n",
    "\n",
    "sys.path.append(os.path.abspath('../../../src'))\n",
    "\n",
    "from read_and_write_docs import read_jsonl, read_rds\n",
    "from tokenize_and_score import load_model\n",
    "from utils import get_base_location, apply_temp_doc_id, build_metadata_df\n",
    "from n_gram_functions import (\n",
    "    common_ngrams,\n",
    "    pretty_print_common_ngrams,\n",
    "    keep_before_phrase,\n",
    "    compute_log_probs_with_median\n",
    ")\n",
    "from open_ai import initialise_client, llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56192fae",
   "metadata": {},
   "source": [
    "## Set Locations & Load Base Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0f4ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"Wiki\"\n",
    "data_type = \"training\"\n",
    "\n",
    "# Set NAS so can run on Windows laptop seamlessly\n",
    "nas_base_loc = get_base_location()\n",
    "\n",
    "# Load known data\n",
    "known_loc = f\"{nas_base_loc}/datasets/author_verification/{data_type}/{corpus}/known_raw.jsonl\"\n",
    "known = read_jsonl(known_loc)\n",
    "known = apply_temp_doc_id(known)\n",
    "\n",
    "# Load unknown data\n",
    "unknown_loc = f\"{nas_base_loc}/datasets/author_verification/{data_type}/{corpus}/unknown_raw.jsonl\"\n",
    "unknown = read_jsonl(unknown_loc)\n",
    "unknown_df = apply_temp_doc_id(unknown)\n",
    "\n",
    "# Load and build metadata\n",
    "metadata_loc = f\"{nas_base_loc}/datasets/author_verification/{data_type}/metadata.rds\"\n",
    "metadata = read_rds(metadata_loc)\n",
    "filtered_metadata = metadata[metadata['corpus'] == corpus]\n",
    "agg_metadata = build_metadata_df(filtered_metadata, known, unknown)\n",
    "\n",
    "# Load the pre-made problem datasets for speed\n",
    "problem_dataset_base = f\"{nas_base_loc}/datasets/author_verification/{data_type}/{corpus}\"\n",
    "problem_dataset_agg = read_jsonl(f\"{problem_dataset_base}/{corpus}_{data_type}_agg.jsonl\")\n",
    "problem_dataset_profile = read_jsonl(f\"{problem_dataset_base}/{corpus}_{data_type}_profile.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d85eaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = load_model(f\"{nas_base_loc}/models/Qwen 2.5/Qwen2.5-0.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e384cb",
   "metadata": {},
   "source": [
    "## View Same and Different-Author Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fa9b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_probs = problem_dataset_agg[problem_dataset_agg['known_author'] == problem_dataset_agg['unknown_author']].copy()\n",
    "same_probs.sort_values([\"highest_common_count\"], ascending=[False], inplace=True)\n",
    "same_probs[(same_probs['highest_common_count'] >= 3) & (same_probs['highest_common_count'] <= 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eb7681",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_probs = problem_dataset_agg[problem_dataset_agg['known_author'] != problem_dataset_agg['unknown_author']].copy()\n",
    "diff_probs.sort_values([\"highest_common_count\"], ascending=[False], inplace=True)\n",
    "diff_probs[(diff_probs['highest_common_count'] >= 3) & (diff_probs['highest_common_count'] <= 10)].head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a0c885",
   "metadata": {},
   "source": [
    "## Select Known and Unknown Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6993e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_doc = \"caboga_text_5\"\n",
    "known_text = known[known['doc_id'] == known_doc].reset_index().loc[0, 'text']\n",
    "\n",
    "unknown_doc = \"chanakyathegreat_text_1\"\n",
    "unknown_text = unknown[unknown['doc_id'] == unknown_doc].reset_index().loc[0, 'text']\n",
    "\n",
    "# Get the metadata for current problem, will be added to Excel\n",
    "p_metadata = agg_metadata[(agg_metadata['known_doc_id'] == known_doc) \n",
    "                          & ((agg_metadata['unknown_doc_id'] == unknown_doc))].reset_index()\n",
    "p_metadata['target'] = p_metadata['known_author'] == p_metadata['unknown_author']\n",
    "specific_problem = p_metadata.loc[0, 'problem']\n",
    "\n",
    "print(f\"Working on problem: {specific_problem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48292f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = pd.DataFrame(\n",
    "    {\n",
    "        \"known\":   [corpus, data_type, known_doc, known_text],\n",
    "        \"unknown\": [corpus, data_type, unknown_doc, unknown_text],\n",
    "    },\n",
    "    index=[\"corpus\", \"data type\", \"doc\", \"text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bdf3b5",
   "metadata": {},
   "source": [
    "## Get N-Grams in Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72faabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "common = common_ngrams(known_text, unknown_text, 2, model, tokenizer, lowercase=True)\n",
    "n_gram_list = pretty_print_common_ngrams(common, tokenizer=tokenizer, order='len_desc', return_format='flat')\n",
    "n_gram_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eded3645",
   "metadata": {},
   "source": [
    "## Initialise OpenAI Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = initialise_client(\"../../../credentials.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e319289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_prompt(known_text, phrase):\n",
    "    user_prompt = f\"\"\"\n",
    "<DOC>\n",
    "{known_text}\n",
    "</DOC>\n",
    "<NGRAM>\n",
    "\"{phrase}\"\n",
    "</NGRAM>\n",
    "\"\"\"\n",
    "    \n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3b7d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_system_prompt(prompt_loc):\n",
    "    with open(prompt_loc, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca10da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_paraphrases(response, phrase):\n",
    "    \n",
    "    paraphrase_list = []\n",
    "    for i in range(1, len(response.choices)):\n",
    "        content = response.choices[i].message.content\n",
    "        \n",
    "        try:\n",
    "            content_json = json.loads(content)\n",
    "            for para in content_json['paraphrases']:\n",
    "                if para != phrase:\n",
    "                    paraphrase_list.append(para)  \n",
    "        except Exception:\n",
    "            continue\n",
    "        \n",
    "    unique_list = list(set(paraphrase_list))\n",
    "    \n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85daabcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = create_system_prompt(\"../../../prompts/exhaustive_constrained_ngram_paraphraser_prompt_JSON.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2466a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_dict = {}\n",
    "width = len(str(len(n_gram_list)))  # e.g., 10 -> 2, 100 -> 3\n",
    "\n",
    "for idx, phrase in enumerate(n_gram_list, start=1):\n",
    "    user_prompt = create_user_prompt(known_text, phrase)\n",
    "    response = llm(\n",
    "        system_prompt,\n",
    "        user_prompt,\n",
    "        client,\n",
    "        model=\"gpt-4.1\",\n",
    "        max_tokens=5000,\n",
    "        temperature=0.7,\n",
    "        n=10,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    paraphrases = parse_paraphrases(response, phrase)\n",
    "    key = f\"phrase_{idx:0{width}d}\"  # -> phrase_01, phrase_002, etc.\n",
    "    n_gram_dict[key] = {\"phrase\": phrase, \"paraphrases\": paraphrases}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd80e569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_phrases(\n",
    "    base_text: str,\n",
    "    ref_phrase: str,\n",
    "    paraphrases: list[str],\n",
    "    tokenizer,\n",
    "    model\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns rows for the reference and each paraphrase with:\n",
    "      sum_log_probs_phrase (log-likelihood for the phrase tokens) and\n",
    "      raw_prob = exp(sum_log_probs_phrase)\n",
    "    \"\"\"\n",
    "    # 1) score base_text\n",
    "    _, log_probs_base, _ = compute_log_probs_with_median(base_text.strip(), tokenizer, model)\n",
    "    base_total = sum(log_probs_base)\n",
    "\n",
    "    items = [(\"reference\", ref_phrase)] + [(\"paraphrase\", p) for p in paraphrases]\n",
    "    rows = []\n",
    "\n",
    "    for ptype, phrase in items:\n",
    "        # a) phrase alone → token count\n",
    "        tokens_phrase, log_probs_phrase, _ = compute_log_probs_with_median(phrase, tokenizer, model)\n",
    "        n_phrase_tokens = len(tokens_phrase)\n",
    "\n",
    "        # b) full sequence\n",
    "        full_text = base_text + phrase\n",
    "        tokens_full, log_probs_full, _ = compute_log_probs_with_median(full_text, tokenizer, model)\n",
    "\n",
    "        # c) full sum (base + phrase)\n",
    "        sum_before = sum(log_probs_full)\n",
    "\n",
    "        # d/e) last n tokens correspond to phrase\n",
    "        phrase_tokens    = tokens_full[-n_phrase_tokens:]\n",
    "        phrase_log_probs = log_probs_full[-n_phrase_tokens:]\n",
    "\n",
    "        # f) totals\n",
    "        phrase_total = sum(phrase_log_probs)\n",
    "        difference   = base_total - sum_before  # typically == -phrase_total\n",
    "\n",
    "        # raw (unnormalized) probability of the phrase given the base\n",
    "        raw_prob = math.exp(phrase_total)  # may underflow to 0.0 for long phrases; that's fine\n",
    "\n",
    "        rows.append({\n",
    "            \"phrase_type\":               ptype,\n",
    "            \"phrase\":                    phrase,\n",
    "            \"tokens\":                    phrase_tokens,\n",
    "            \"sum_log_probs_base\":        base_total,\n",
    "            \"sum_log_probs_inc_phrase\":  sum_before,\n",
    "            \"difference\":                difference,\n",
    "            \"phrase_log_probs\":          phrase_log_probs,\n",
    "            \"sum_log_probs_phrase\":      phrase_total,\n",
    "            \"raw_prob\":                  raw_prob,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\n",
    "        \"phrase_type\", \"phrase\", \"tokens\",\n",
    "        \"sum_log_probs_base\", \"sum_log_probs_inc_phrase\",\n",
    "        \"difference\", \"phrase_log_probs\", \"sum_log_probs_phrase\",\n",
    "        \"raw_prob\",\n",
    "    ])\n",
    "\n",
    "def get_scored_df(n_gram_dict, full_text, tokenizer, model):\n",
    "    \"\"\"Row-concat each scored df, add phrase_num, sort, then rank paraphrases within each phrase_num.\"\"\"\n",
    "    dfs = []\n",
    "    for phrase_num, entry in n_gram_dict.items():  # relies on insertion order\n",
    "        print(f\"Processing Phrase - {phrase_num}\")\n",
    "        phrase = entry[\"phrase\"]\n",
    "        paraphrases = entry[\"paraphrases\"]\n",
    "\n",
    "        base_text = keep_before_phrase(full_text, phrase)\n",
    "\n",
    "        df = score_phrases(base_text, phrase, paraphrases, tokenizer, model).copy()\n",
    "        df.insert(0, \"original_phrase\", phrase)\n",
    "        df.insert(0, \"phrase_num\", phrase_num)  # first column\n",
    "        dfs.append(df)\n",
    "\n",
    "    if not dfs:\n",
    "        return pd.DataFrame(columns=[\"phrase_num\"])\n",
    "\n",
    "    out = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # sort by phrase_num (zero-padded → lexicographic == numeric)\n",
    "    out = out.sort_values(\"phrase_num\", kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "    # rank within phrase_num: reference -> 0; paraphrases ranked by descending raw_prob starting at 1\n",
    "    out[\"rank\"] = None\n",
    "    mask = out[\"phrase_type\"].eq(\"paraphrase\")\n",
    "    out.loc[mask, \"rank\"] = (\n",
    "        out.loc[mask]\n",
    "           .groupby(\"phrase_num\")[\"raw_prob\"]\n",
    "           .rank(method=\"first\", ascending=False)\n",
    "           .astype(int)\n",
    "    )\n",
    "    out.loc[out[\"phrase_type\"].eq(\"reference\"), \"rank\"] = 0\n",
    "    out[\"rank\"] = out[\"rank\"].astype(int)\n",
    "\n",
    "    out = out.sort_values([\"phrase_num\", \"rank\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc75faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_scored = get_scored_df(n_gram_dict, known_text, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f663057",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_scored = get_scored_df(n_gram_dict, unknown_text, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65e3b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_phrases_no_context(\n",
    "    ref_phrase: str,\n",
    "    paraphrases: List[str],\n",
    "    tokenizer,\n",
    "    model\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Score the reference and each paraphrase *without* any ranking.\n",
    "    Returns:\n",
    "      phrase_type, phrase, tokens, log_probs, sum_log_probs, raw_prob\n",
    "    \"\"\"\n",
    "    items = [(\"reference\", ref_phrase)] + [(\"paraphrase\", p) for p in paraphrases]\n",
    "    rows = []\n",
    "\n",
    "    for idx, (ptype, phrase) in enumerate(items, start=1):\n",
    "        print(f\"→ [{idx}/{len(items)}] Processing {ptype}…\")\n",
    "        tokens_phrase, log_probs_phrase, _ = compute_log_probs_with_median(phrase, tokenizer, model)\n",
    "        phrase_total = sum(log_probs_phrase)\n",
    "        raw_prob = math.exp(phrase_total)  # unnormalized prob\n",
    "\n",
    "        rows.append({\n",
    "            \"phrase_type\":   ptype,\n",
    "            \"phrase\":        phrase,\n",
    "            \"tokens\":        tokens_phrase,\n",
    "            \"log_probs\":     log_probs_phrase,\n",
    "            \"sum_log_probs\": phrase_total,\n",
    "            \"raw_prob\":      raw_prob,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\n",
    "        \"phrase_type\", \"phrase\", \"tokens\", \"log_probs\", \"sum_log_probs\", \"raw_prob\"\n",
    "    ])\n",
    "\n",
    "def get_scored_df_no_context(n_gram_dict, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Row-concat each score_phrases_no_context df, add phrase_num, sort by phrase_num,\n",
    "    then rank paraphrases within each phrase_num by descending raw_prob.\n",
    "    'reference' rows always get rank 0.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for phrase_num, entry in n_gram_dict.items():  # insertion order preserved\n",
    "        print(f\"Processing Phrase - {phrase_num}\")\n",
    "        phrase = entry[\"phrase\"]\n",
    "        paraphrases = entry[\"paraphrases\"]\n",
    "\n",
    "        df = score_phrases_no_context(phrase, paraphrases, tokenizer, model).copy()\n",
    "        df.insert(0, \"original_phrase\", phrase)\n",
    "        df.insert(0, \"phrase_num\", phrase_num)  # make it the first column\n",
    "        dfs.append(df)\n",
    "\n",
    "    if not dfs:\n",
    "        return pd.DataFrame(columns=[\"phrase_num\"])\n",
    "\n",
    "    out = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # zero-padded keys => lexicographic equals numeric order\n",
    "    out = out.sort_values(\"phrase_num\", kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "    # Rank within phrase_num: reference -> 0; paraphrases ranked by descending raw_prob starting at 1\n",
    "    out[\"rank\"] = None\n",
    "    mask_para = out[\"phrase_type\"].eq(\"paraphrase\")\n",
    "    out.loc[mask_para, \"rank\"] = (\n",
    "        out.loc[mask_para]\n",
    "           .groupby(\"phrase_num\")[\"raw_prob\"]\n",
    "           .rank(method=\"first\", ascending=False)  # use \"dense\" if you prefer 1,2,3 without gaps\n",
    "           .astype(int)\n",
    "    )\n",
    "    out.loc[out[\"phrase_type\"].eq(\"reference\"), \"rank\"] = 0\n",
    "    out[\"rank\"] = out[\"rank\"].astype(int)\n",
    "\n",
    "    out = out.sort_values([\"phrase_num\", \"rank\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa3c002",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df_no_context = get_scored_df_no_context(n_gram_dict, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374ff5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_phrases = score_df_no_context[['phrase_num', 'original_phrase']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c603218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove illegal control chars (keep \\t, \\n, \\r)\n",
    "_ILLEGAL_RE = re.compile(r\"[\\x00-\\x08\\x0B-\\x0C\\x0E-\\x1F]\")\n",
    "\n",
    "def _clean_cell(x):\n",
    "    if isinstance(x, str):\n",
    "        return _ILLEGAL_RE.sub(\"\", x)\n",
    "    return x\n",
    "\n",
    "def clean_for_excel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    obj_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    df[obj_cols] = df[obj_cols].applymap(_clean_cell)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfbe8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loc = f\"{nas_base_loc}/paraphrase examples/{specific_problem}.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(save_loc, engine=\"openpyxl\") as xls:\n",
    "    clean_for_excel(docs_df).to_excel(xls, sheet_name=\"docs\", index=False)\n",
    "    clean_for_excel(p_metadata).to_excel(xls, sheet_name=\"metadata\", index=False)\n",
    "    clean_for_excel(score_df_no_context).to_excel(xls, sheet_name=\"no context\", index=False)\n",
    "    clean_for_excel(known_scored).to_excel(xls, sheet_name=\"known\", index=False)\n",
    "    clean_for_excel(unknown_scored).to_excel(xls, sheet_name=\"unknown\", index=False)\n",
    "    clean_for_excel(distinct_phrases).to_excel(xls, sheet_name=\"LLR\", index=False)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
