{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "32190cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "02e7b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('../../../src'))\n",
    "\n",
    "from read_and_write_docs import read_jsonl, read_rds\n",
    "from tokenize_and_score import load_model\n",
    "from utils import get_base_location, apply_temp_doc_id, build_metadata_df\n",
    "from n_gram_functions import (\n",
    "    common_ngrams,\n",
    "    pretty_print_common_ngrams,\n",
    "    keep_before_phrase,\n",
    "    score_phrases,\n",
    "    score_phrases_no_context,\n",
    "    compute_log_probs_with_median\n",
    ")\n",
    "from open_ai import initialise_client, llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0f4ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"Wiki\"\n",
    "data_type = \"training\"\n",
    "\n",
    "nas_base_loc = get_base_location()\n",
    "\n",
    "known_loc = f\"{nas_base_loc}/datasets/author_verification/{data_type}/{corpus}/known_raw.jsonl\"\n",
    "known = read_jsonl(known_loc)\n",
    "known = apply_temp_doc_id(known)\n",
    "\n",
    "unknown_loc = f\"{nas_base_loc}/datasets/author_verification/{data_type}/{corpus}/unknown_raw.jsonl\"\n",
    "unknown = read_jsonl(unknown_loc)\n",
    "unknown_df = apply_temp_doc_id(unknown)\n",
    "\n",
    "metadata_loc = f\"{nas_base_loc}/datasets/author_verification/{data_type}/metadata.rds\"\n",
    "metadata = read_rds(metadata_loc)\n",
    "filtered_metadata = metadata[metadata['corpus'] == corpus]\n",
    "agg_metadata = build_metadata_df(filtered_metadata, known, unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a16b8b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_dataset_agg = read_jsonl(\"/Users/user/Documents/test_data/n-gram_tracing/Wiki_training_agg.jsonl\")\n",
    "problem_dataset_profile = read_jsonl(\"/Users/user/Documents/test_data/n-gram_tracing/Wiki_training_profile.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "18fa9b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>known_author</th>\n",
       "      <th>unknown_author</th>\n",
       "      <th>known_doc_id</th>\n",
       "      <th>unknown_doc_id</th>\n",
       "      <th>highest_common_count</th>\n",
       "      <th>highest_common_ngram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>Greg_L vs Greg_L</td>\n",
       "      <td>Greg_L</td>\n",
       "      <td>Greg_L</td>\n",
       "      <td>greg_l_text_11</td>\n",
       "      <td>greg_l_text_10</td>\n",
       "      <td>9</td>\n",
       "      <td>, Ġthey Ġshould Ġhave Ġparticipated Ġin Ġthe Ġ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>Haymaker vs Haymaker</td>\n",
       "      <td>Haymaker</td>\n",
       "      <td>Haymaker</td>\n",
       "      <td>haymaker_text_3</td>\n",
       "      <td>haymaker_text_2</td>\n",
       "      <td>9</td>\n",
       "      <td>Ġat Ġthe Ġend Ġof Ġthe Ġday , Ġwe 're</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>Fragments_of_Jade vs Fragments_of_Jade</td>\n",
       "      <td>Fragments_of_Jade</td>\n",
       "      <td>Fragments_of_Jade</td>\n",
       "      <td>fragments_of_jade_text_2</td>\n",
       "      <td>fragments_of_jade_text_10</td>\n",
       "      <td>8</td>\n",
       "      <td>Ġme , Ġand Ġit 's Ġgetting Ġold .Ċ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>Fixentries vs Fixentries</td>\n",
       "      <td>Fixentries</td>\n",
       "      <td>Fixentries</td>\n",
       "      <td>fixentries_text_2</td>\n",
       "      <td>fixentries_text_5</td>\n",
       "      <td>8</td>\n",
       "      <td>Ġthe Ġindividual Ġher it ability Ġof Ġintellig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>DonaNobisPacem vs DonaNobisPacem</td>\n",
       "      <td>DonaNobisPacem</td>\n",
       "      <td>DonaNobisPacem</td>\n",
       "      <td>donanobispacem_text_5</td>\n",
       "      <td>donanobispacem_text_2</td>\n",
       "      <td>8</td>\n",
       "      <td>Ġafter Ġ 1 8 - 2 0 Ġweeks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>David_Shankbone vs David_Shankbone</td>\n",
       "      <td>David_Shankbone</td>\n",
       "      <td>David_Shankbone</td>\n",
       "      <td>david_shankbone_text_1</td>\n",
       "      <td>david_shankbone_text_4</td>\n",
       "      <td>3</td>\n",
       "      <td>, Ġwhich Ġis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>D7G1DX~0 vs D7G1DX~0</td>\n",
       "      <td>D7G1DX~0</td>\n",
       "      <td>D7G1DX~0</td>\n",
       "      <td>d7g1dx_0_text_2</td>\n",
       "      <td>d7g1dx_0_text_5</td>\n",
       "      <td>3</td>\n",
       "      <td>Ġdon 't Ġthink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>Collect vs Collect</td>\n",
       "      <td>Collect</td>\n",
       "      <td>Collect</td>\n",
       "      <td>collect_text_12</td>\n",
       "      <td>collect_text_11</td>\n",
       "      <td>3</td>\n",
       "      <td>, Ġand Ġi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Cptnono vs Cptnono</td>\n",
       "      <td>Cptnono</td>\n",
       "      <td>Cptnono</td>\n",
       "      <td>cptnono_text_1</td>\n",
       "      <td>cptnono_text_12</td>\n",
       "      <td>3</td>\n",
       "      <td>, Ġthough .Ċ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>Danlaycock vs Danlaycock</td>\n",
       "      <td>Danlaycock</td>\n",
       "      <td>Danlaycock</td>\n",
       "      <td>danlaycock_text_5</td>\n",
       "      <td>danlaycock_text_2</td>\n",
       "      <td>3</td>\n",
       "      <td>.Ċ also ,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    problem       known_author  \\\n",
       "379                        Greg_L vs Greg_L             Greg_L   \n",
       "432                    Haymaker vs Haymaker           Haymaker   \n",
       "354  Fragments_of_Jade vs Fragments_of_Jade  Fragments_of_Jade   \n",
       "337                Fixentries vs Fixentries         Fixentries   \n",
       "248        DonaNobisPacem vs DonaNobisPacem     DonaNobisPacem   \n",
       "..                                      ...                ...   \n",
       "222      David_Shankbone vs David_Shankbone    David_Shankbone   \n",
       "211                    D7G1DX~0 vs D7G1DX~0           D7G1DX~0   \n",
       "187                      Collect vs Collect            Collect   \n",
       "198                      Cptnono vs Cptnono            Cptnono   \n",
       "218                Danlaycock vs Danlaycock         Danlaycock   \n",
       "\n",
       "        unknown_author              known_doc_id             unknown_doc_id  \\\n",
       "379             Greg_L            greg_l_text_11             greg_l_text_10   \n",
       "432           Haymaker           haymaker_text_3            haymaker_text_2   \n",
       "354  Fragments_of_Jade  fragments_of_jade_text_2  fragments_of_jade_text_10   \n",
       "337         Fixentries         fixentries_text_2          fixentries_text_5   \n",
       "248     DonaNobisPacem     donanobispacem_text_5      donanobispacem_text_2   \n",
       "..                 ...                       ...                        ...   \n",
       "222    David_Shankbone    david_shankbone_text_1     david_shankbone_text_4   \n",
       "211           D7G1DX~0           d7g1dx_0_text_2            d7g1dx_0_text_5   \n",
       "187            Collect           collect_text_12            collect_text_11   \n",
       "198            Cptnono            cptnono_text_1            cptnono_text_12   \n",
       "218         Danlaycock         danlaycock_text_5          danlaycock_text_2   \n",
       "\n",
       "     highest_common_count                               highest_common_ngram  \n",
       "379                     9  , Ġthey Ġshould Ġhave Ġparticipated Ġin Ġthe Ġ...  \n",
       "432                     9              Ġat Ġthe Ġend Ġof Ġthe Ġday , Ġwe 're  \n",
       "354                     8                 Ġme , Ġand Ġit 's Ġgetting Ġold .Ċ  \n",
       "337                     8  Ġthe Ġindividual Ġher it ability Ġof Ġintellig...  \n",
       "248                     8                          Ġafter Ġ 1 8 - 2 0 Ġweeks  \n",
       "..                    ...                                                ...  \n",
       "222                     3                                       , Ġwhich Ġis  \n",
       "211                     3                                     Ġdon 't Ġthink  \n",
       "187                     3                                          , Ġand Ġi  \n",
       "198                     3                                       , Ġthough .Ċ  \n",
       "218                     3                                          .Ċ also ,  \n",
       "\n",
       "[197 rows x 7 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_author_problems = problem_dataset_agg[problem_dataset_agg['known_author'] == problem_dataset_agg['unknown_author']].copy()\n",
    "same_author_problems.sort_values([\"highest_common_count\"], ascending=[False], inplace=True)\n",
    "same_author_problems[(same_author_problems['highest_common_count'] >= 3) & (same_author_problems['highest_common_count'] <= 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb6993e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on problem: Enemesis vs Equanimous1\n"
     ]
    }
   ],
   "source": [
    "known_doc = \"enemesis_text_2\"\n",
    "known_text = known[known['doc_id'] == known_doc].reset_index().loc[0, 'text']\n",
    "\n",
    "unknown_doc = \"equanimous1_text_5\"\n",
    "unknown_text = unknown[unknown['doc_id'] == unknown_doc].reset_index().loc[0, 'text']\n",
    "\n",
    "specific_problem_metadata = agg_metadata[(agg_metadata['known_doc_id'] == known_doc) & ((agg_metadata['unknown_doc_id'] == unknown_doc))].reset_index()\n",
    "specific_problem_metadata['target'] = specific_problem_metadata['known_author'] == specific_problem_metadata['unknown_author']\n",
    "specific_problem = specific_problem_metadata.loc[0, 'problem']\n",
    "\n",
    "print(f\"Working on problem: {specific_problem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "097eadcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = load_model(\"/Volumes/BCross/models/Qwen 2.5/Qwen2.5-0.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b72faabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "common = common_ngrams(known_text, unknown_text, 2, model, tokenizer, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6f1787f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' the current state of the',\n",
       " ' the article and',\n",
       " '.\\nthere is',\n",
       " ' an agenda',\n",
       " ' an article',\n",
       " ' from the',\n",
       " ' in a',\n",
       " ' in the',\n",
       " ' information is',\n",
       " ' is not',\n",
       " ' this article',\n",
       " ' to be',\n",
       " ' to the',\n",
       " ' which is',\n",
       " ' you can',\n",
       " ' you have',\n",
       " '.\\nthe',\n",
       " '.\\nthey']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_list = pretty_print_common_ngrams(common, tokenizer=tokenizer, order='len_desc', return_format='flat')\n",
    "n_gram_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eded3645",
   "metadata": {},
   "source": [
    "## Initialise OpenAI Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1f9b6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = initialise_client(\"../../../credentials.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0e319289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_prompt(known_text, phrase):\n",
    "    user_prompt = f\"\"\"\n",
    "<DOC>\n",
    "{known_text}\n",
    "</DOC>\n",
    "<NGRAM>\n",
    "\"{phrase}\"\n",
    "</NGRAM>\n",
    "\"\"\"\n",
    "    \n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9e3b7d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_system_prompt(prompt_loc):\n",
    "    with open(prompt_loc,\"r\") as f:\n",
    "        system_prompt = f.read()\n",
    "        \n",
    "    return system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bca10da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_paraphrases(response, phrase):\n",
    "    \n",
    "    paraphrase_list = []\n",
    "    for i in range(1, len(response.choices)):\n",
    "        content = response.choices[i].message.content\n",
    "        \n",
    "        try:\n",
    "            content_json = json.loads(content)\n",
    "            for para in content_json['paraphrases']:\n",
    "                if para != phrase:\n",
    "                    paraphrase_list.append(para)  \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    unique_list = list(set(paraphrase_list))\n",
    "    \n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "85daabcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = create_system_prompt(\"../../../prompts/exhaustive_constrained_ngram_paraphraser_prompt_JSON.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d2466a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_dict = {}\n",
    "width = len(str(len(n_gram_list)))  # e.g., 10 -> 2, 100 -> 3\n",
    "\n",
    "for idx, phrase in enumerate(n_gram_list, start=1):\n",
    "    user_prompt = create_user_prompt(known_text, phrase)\n",
    "    response = llm(\n",
    "        system_prompt,\n",
    "        user_prompt,\n",
    "        client,\n",
    "        model=\"gpt-4.1\",\n",
    "        max_tokens=5000,\n",
    "        temperature=0.7,\n",
    "        n=10,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    paraphrases = parse_paraphrases(response, phrase)\n",
    "    key = f\"phrase_{idx:0{width}d}\"  # -> phrase_01, phrase_002, etc.\n",
    "    n_gram_dict[key] = {\"phrase\": phrase, \"paraphrases\": paraphrases}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cb0bf70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phrase_01': {'phrase': ' the current state of the',\n",
       "  'paraphrases': ['the current condition of thee',\n",
       "   'the present state ofthe',\n",
       "   'the present stateofthe',\n",
       "   \"the present state o' the\",\n",
       "   ' the current state of-the',\n",
       "   \"the present condition of th'\",\n",
       "   ' the current state of th',\n",
       "   \" the current state of th'\",\n",
       "   'the currentstate of the',\n",
       "   'the current state ofthe',\n",
       "   'the present state of the',\n",
       "   \"the present state o' th'\",\n",
       "   'the current condition of the',\n",
       "   \"the present condition o' the\",\n",
       "   \" the current state o' the\",\n",
       "   'the current state of the',\n",
       "   ' the current state ofthe',\n",
       "   'the current state of teh',\n",
       "   'the existing condition of the',\n",
       "   \"the current state of t'he\",\n",
       "   'the present condition ofthe',\n",
       "   'the present state of thee',\n",
       "   'the present condition of teh',\n",
       "   \"the current condition of th'\",\n",
       "   \" the current state of t'\",\n",
       "   'the current state of tha',\n",
       "   \"the current condition o' the\",\n",
       "   'the current state-of-the',\n",
       "   'the current condition ofthe',\n",
       "   ' the present condition of the',\n",
       "   ' the current status of the',\n",
       "   'the present condition of thee',\n",
       "   ' the present state of the',\n",
       "   \"the current state of th'\",\n",
       "   \"the current state o' the\",\n",
       "   'the current status of the',\n",
       "   ' the present status of the',\n",
       "   'the present condition of the',\n",
       "   'the existing status of the',\n",
       "   'the present state of teh',\n",
       "   'the current state of thee',\n",
       "   \"the currant state of th'\",\n",
       "   'the present state-of-the',\n",
       "   ' the current condition of the',\n",
       "   'the currant state-of-the',\n",
       "   'the current condition of teh',\n",
       "   \"the currant state o' the\",\n",
       "   \"the present state of th'\",\n",
       "   'the currant state of the',\n",
       "   'the current stateofthe',\n",
       "   'the current conditionofthe',\n",
       "   'the current condition-of-the',\n",
       "   'the currant stateofthe',\n",
       "   'the current stateof the',\n",
       "   'the existing state of the',\n",
       "   'the present status of the',\n",
       "   \"the current state o' th'\"]},\n",
       " 'phrase_02': {'phrase': ' the article and',\n",
       "  'paraphrases': [' the article & ',\n",
       "   \"the article 'n'\",\n",
       "   \"the article an'\",\n",
       "   \" the article 'n'\",\n",
       "   ' the article &',\n",
       "   \" the article an'\",\n",
       "   ' the article &amp;',\n",
       "   \" the article an' \",\n",
       "   ' the article and ',\n",
       "   'the article and',\n",
       "   'the article &']},\n",
       " 'phrase_03': {'phrase': '.\\nthere is',\n",
       "  'paraphrases': ['there is.',\n",
       "   \".\\nThere's\",\n",
       "   \"there's.\",\n",
       "   'There is',\n",
       "   '.\\nThere\\x19s',\n",
       "   'there is',\n",
       "   'there\\x19s',\n",
       "   '. There\\x19s',\n",
       "   '. there\\x19s',\n",
       "   '. There is',\n",
       "   '. there’s',\n",
       "   '.\\nTHERE IS',\n",
       "   '.\\nTHeres',\n",
       "   '\\nthere is',\n",
       "   \".\\nTHERE'S\",\n",
       "   \".\\nthere's\",\n",
       "   '.\\nthere Is',\n",
       "   '.\\nThere\\x000is',\n",
       "   '. there is',\n",
       "   \". there's\",\n",
       "   '.\\nThere is',\n",
       "   \"there's\",\n",
       "   '.\\ntheres',\n",
       "   '.\\nthere\\x19s',\n",
       "   '.\\nthere’s',\n",
       "   '.\\nThere\\x000Is',\n",
       "   'There\\x19s']},\n",
       " 'phrase_04': {'phrase': ' an agenda',\n",
       "  'paraphrases': [' an intention',\n",
       "   ' an aim',\n",
       "   ' a goal',\n",
       "   ' a purpose',\n",
       "   ' a plan',\n",
       "   'an agenda',\n",
       "   'a purpose',\n",
       "   'a plan',\n",
       "   'an agendum',\n",
       "   ' an objective',\n",
       "   'a scheme',\n",
       "   ' a motive',\n",
       "   'an objective',\n",
       "   'an aim',\n",
       "   ' a scheme',\n",
       "   'an intention',\n",
       "   ' an object',\n",
       "   ' an agendum',\n",
       "   'a motive']},\n",
       " 'phrase_05': {'phrase': ' an article',\n",
       "  'paraphrases': [' an article.',\n",
       "   ' an essay',\n",
       "   ' a write-up',\n",
       "   ' an editorial',\n",
       "   ' an-article',\n",
       "   ' a write up',\n",
       "   ' an entry.',\n",
       "   ' a report',\n",
       "   ' a writeup',\n",
       "   ' an article ',\n",
       "   ' an artical',\n",
       "   ' a paper.',\n",
       "   ' an item',\n",
       "   ' an item.',\n",
       "   ' a composition',\n",
       "   ' a story',\n",
       "   ' an entry',\n",
       "   ' a text',\n",
       "   ' a publication',\n",
       "   ' a write up.',\n",
       "   ' an_article',\n",
       "   ' a composition.',\n",
       "   ' a review',\n",
       "   ' a text.',\n",
       "   ' a piece',\n",
       "   ' a paper',\n",
       "   ' a commentary',\n",
       "   ' a write-up.',\n",
       "   ' a report.',\n",
       "   ' a feature',\n",
       "   ' a piece.',\n",
       "   'an article']},\n",
       " 'phrase_06': {'phrase': ' from the',\n",
       "  'paraphrases': [' from tbe',\n",
       "   'from the',\n",
       "   'from tha',\n",
       "   'from th’',\n",
       "   \"from th'\",\n",
       "   ' from th',\n",
       "   \" from th'\",\n",
       "   ' from thee',\n",
       "   ' from th’',\n",
       "   'fromthe',\n",
       "   \" from t'he\",\n",
       "   'from tbe',\n",
       "   'from th',\n",
       "   ' from tha',\n",
       "   ' fromthe']},\n",
       " 'phrase_07': {'phrase': ' in a', 'paraphrases': ['in an', 'in a', ' in an']},\n",
       " 'phrase_08': {'phrase': ' in the',\n",
       "  'paraphrases': [' in de',\n",
       "   'in teh',\n",
       "   ' in tbe',\n",
       "   ' in-the',\n",
       "   ' in teh',\n",
       "   'in-the',\n",
       "   \" in t'he\",\n",
       "   ' in thee',\n",
       "   \"in th'\",\n",
       "   ' in_the',\n",
       "   \"in t'he\",\n",
       "   ' in tha',\n",
       "   \" in th'\",\n",
       "   ' in the ',\n",
       "   'in tbe',\n",
       "   ' in th',\n",
       "   ' inthe',\n",
       "   'in the',\n",
       "   'inthe',\n",
       "   'in th',\n",
       "   ' in da']},\n",
       " 'phrase_09': {'phrase': ' information is',\n",
       "  'paraphrases': [\" information's\",\n",
       "   \"information's\",\n",
       "   'information is',\n",
       "   ' info is',\n",
       "   \" info's\",\n",
       "   'info is']},\n",
       " 'phrase_10': {'phrase': ' is not',\n",
       "  'paraphrases': ['’s not', \" isn't\", \" is n't\"]},\n",
       " 'phrase_11': {'phrase': ' this article',\n",
       "  'paraphrases': ['this post',\n",
       "   'this write-up',\n",
       "   'this articel',\n",
       "   'this article.',\n",
       "   'this text.',\n",
       "   'this post.',\n",
       "   'this write up.',\n",
       "   'this writeup.',\n",
       "   'this work',\n",
       "   'this composition.',\n",
       "   'this entry',\n",
       "   'this composition',\n",
       "   'this paper.',\n",
       "   'this text',\n",
       "   'this essay',\n",
       "   'this artical',\n",
       "   'this paper',\n",
       "   'this article',\n",
       "   'this document',\n",
       "   'this item.',\n",
       "   'this piece.',\n",
       "   'this entry.',\n",
       "   'this-article',\n",
       "   'this write up',\n",
       "   'this piece',\n",
       "   'this item',\n",
       "   'this document.',\n",
       "   'this writeup',\n",
       "   'this aritcle',\n",
       "   'this write-up.']},\n",
       " 'phrase_12': {'phrase': ' to be',\n",
       "  'paraphrases': [' to stay',\n",
       "   'to be',\n",
       "   ' in order to be',\n",
       "   ' to continue existing',\n",
       "   ' to remain',\n",
       "   ' to stay being',\n",
       "   ' in order to remain',\n",
       "   ' to continue to be',\n",
       "   'to exist',\n",
       "   ' in order to continue to be',\n",
       "   ' in order to exist',\n",
       "   ' to keep on being',\n",
       "   ' to keep being',\n",
       "   ' to exist',\n",
       "   ' in order to continue being',\n",
       "   ' to continue being']},\n",
       " 'phrase_13': {'phrase': ' to the',\n",
       "  'paraphrases': [\" t' too the\",\n",
       "   ' too tha',\n",
       "   ' to-tha',\n",
       "   \" t' tha\",\n",
       "   ' tto tbe',\n",
       "   \" t' thee\",\n",
       "   ' to-thee',\n",
       "   \"t' thee\",\n",
       "   \" t' to tbe\",\n",
       "   ' too the',\n",
       "   ' to teh',\n",
       "   \" too th'\",\n",
       "   ' to thee',\n",
       "   \"to th'\",\n",
       "   'too tha',\n",
       "   ' tto th',\n",
       "   \" t' th'\",\n",
       "   'to the ',\n",
       "   'to tha',\n",
       "   \" t' to the\",\n",
       "   ' to-tbe',\n",
       "   ' too teh',\n",
       "   \" t' tto the\",\n",
       "   \"t' the\",\n",
       "   \" t' to tha\",\n",
       "   \" t' the\",\n",
       "   'too thee',\n",
       "   ' tto the',\n",
       "   ' to-teh',\n",
       "   \" to th'\",\n",
       "   ' too thee',\n",
       "   ' tto teh',\n",
       "   'to-the',\n",
       "   ' to tha',\n",
       "   'too the',\n",
       "   \"t' th'\",\n",
       "   ' too th',\n",
       "   \" t' to teh\",\n",
       "   'to the',\n",
       "   ' to-the',\n",
       "   ' too tbe',\n",
       "   \" t' to thee\",\n",
       "   'tothe',\n",
       "   'to thee ',\n",
       "   ' to th',\n",
       "   \"too th'\",\n",
       "   \"t' tha\",\n",
       "   \" t'to the\",\n",
       "   ' to tbe',\n",
       "   'to thee',\n",
       "   \" to t'\"]},\n",
       " 'phrase_14': {'phrase': ' which is',\n",
       "  'paraphrases': [' which is,',\n",
       "   \" which's\",\n",
       "   ' which is ',\n",
       "   \" that's,\",\n",
       "   \" that's\",\n",
       "   ' that is',\n",
       "   \" that's \",\n",
       "   \" which's,\",\n",
       "   \" which's \"]},\n",
       " 'phrase_15': {'phrase': ' you can',\n",
       "  'paraphrases': ['you are able to',\n",
       "   'you can',\n",
       "   'you’re capable of',\n",
       "   'you’re able',\n",
       "   \"you're able to\",\n",
       "   \"you're able\",\n",
       "   'you are able',\n",
       "   'you’re able to',\n",
       "   \"you're capable of\",\n",
       "   'you are capable of']},\n",
       " 'phrase_16': {'phrase': ' you have',\n",
       "  'paraphrases': [\"you 've\", \"you've\", 'you have']},\n",
       " 'phrase_17': {'phrase': '.\\nthe',\n",
       "  'paraphrases': ['\".the\"',\n",
       "   '\" .the\"',\n",
       "   '\". the\"',\n",
       "   '.the',\n",
       "   '\" . the\"',\n",
       "   '. the',\n",
       "   ' . the']},\n",
       " 'phrase_18': {'phrase': '.\\nthey',\n",
       "  'paraphrases': ['.they',\n",
       "   '\".  they\"',\n",
       "   '\".They\"',\n",
       "   '\".they\"',\n",
       "   '. they',\n",
       "   '\". they\"',\n",
       "   '\". They\"']}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "45b6938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_scored_df(n_gram_dict, full_text, tokenizer, model):\n",
    "    \n",
    "    dfs=[]\n",
    "    \n",
    "    for phrase_num, entry in n_gram_dict.items():  # keeps insertion order\n",
    "        phrase = entry[\"phrase\"]\n",
    "        paraphrases = entry[\"paraphrases\"]\n",
    "        \n",
    "        print(f\"Completing {phrase_num} - {phrase}\")\n",
    "        base_text = keep_before_phrase(known_text, phrase)\n",
    "        \n",
    "        df = score_phrases(base_text, phrase, paraphrases, tokenizer, model).copy()\n",
    "        df.insert(0, \"original_phrase\", phrase)\n",
    "        df.insert(0, \"phrase_num\", phrase_num)  # put at beginning\n",
    "        dfs.append(df)\n",
    "\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b2b45c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# known_scored = get_scored_df(n_gram_dict, known_text, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cd80e569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "def score_phrases(\n",
    "    base_text: str,\n",
    "    ref_phrase: str,\n",
    "    paraphrases: list[str],\n",
    "    tokenizer,\n",
    "    model\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns rows for the reference and each paraphrase with:\n",
    "      sum_log_probs_phrase (log-likelihood for the phrase tokens) and\n",
    "      raw_prob = exp(sum_log_probs_phrase)\n",
    "    \"\"\"\n",
    "    # 1) score base_text\n",
    "    _, log_probs_base, _ = compute_log_probs_with_median(base_text.strip(), tokenizer, model)\n",
    "    base_total = sum(log_probs_base)\n",
    "\n",
    "    items = [(\"reference\", ref_phrase)] + [(\"paraphrase\", p) for p in paraphrases]\n",
    "    rows = []\n",
    "\n",
    "    for ptype, phrase in items:\n",
    "        # a) phrase alone → token count\n",
    "        tokens_phrase, log_probs_phrase, _ = compute_log_probs_with_median(phrase, tokenizer, model)\n",
    "        n_phrase_tokens = len(tokens_phrase)\n",
    "\n",
    "        # b) full sequence\n",
    "        full_text = base_text + phrase\n",
    "        tokens_full, log_probs_full, _ = compute_log_probs_with_median(full_text, tokenizer, model)\n",
    "\n",
    "        # c) full sum (base + phrase)\n",
    "        sum_before = sum(log_probs_full)\n",
    "\n",
    "        # d/e) last n tokens correspond to phrase\n",
    "        phrase_tokens    = tokens_full[-n_phrase_tokens:]\n",
    "        phrase_log_probs = log_probs_full[-n_phrase_tokens:]\n",
    "\n",
    "        # f) totals\n",
    "        phrase_total = sum(phrase_log_probs)\n",
    "        difference   = base_total - sum_before  # typically == -phrase_total\n",
    "\n",
    "        # raw (unnormalized) probability of the phrase given the base\n",
    "        raw_prob = math.exp(phrase_total)  # may underflow to 0.0 for long phrases; that's fine\n",
    "\n",
    "        rows.append({\n",
    "            \"phrase_type\":               ptype,\n",
    "            \"phrase\":                    phrase,\n",
    "            \"tokens\":                    phrase_tokens,\n",
    "            \"sum_log_probs_base\":        base_total,\n",
    "            \"sum_log_probs_inc_phrase\":  sum_before,\n",
    "            \"difference\":                difference,\n",
    "            \"phrase_log_probs\":          phrase_log_probs,\n",
    "            \"sum_log_probs_phrase\":      phrase_total,\n",
    "            \"raw_prob\":                  raw_prob,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\n",
    "        \"phrase_type\", \"phrase\", \"tokens\",\n",
    "        \"sum_log_probs_base\", \"sum_log_probs_inc_phrase\",\n",
    "        \"difference\", \"phrase_log_probs\", \"sum_log_probs_phrase\",\n",
    "        \"raw_prob\",\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_scored_df(n_gram_dict, full_text, tokenizer, model):\n",
    "    \"\"\"Row-concat each scored df, add phrase_num, sort, then rank paraphrases within each phrase_num.\"\"\"\n",
    "    dfs = []\n",
    "    for phrase_num, entry in n_gram_dict.items():  # relies on insertion order\n",
    "        phrase = entry[\"phrase\"]\n",
    "        paraphrases = entry[\"paraphrases\"]\n",
    "\n",
    "        base_text = keep_before_phrase(full_text, phrase)\n",
    "\n",
    "        df = score_phrases(base_text, phrase, paraphrases, tokenizer, model).copy()\n",
    "        df.insert(0, \"original_phrase\", phrase)\n",
    "        df.insert(0, \"phrase_num\", phrase_num)  # first column\n",
    "        dfs.append(df)\n",
    "\n",
    "    if not dfs:\n",
    "        return pd.DataFrame(columns=[\"phrase_num\"])\n",
    "\n",
    "    out = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # sort by phrase_num (zero-padded → lexicographic == numeric)\n",
    "    out = out.sort_values(\"phrase_num\", kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "    # rank within phrase_num: reference -> 0; paraphrases ranked by descending raw_prob starting at 1\n",
    "    out[\"rank\"] = None\n",
    "    mask = out[\"phrase_type\"].eq(\"paraphrase\")\n",
    "    out.loc[mask, \"rank\"] = (\n",
    "        out.loc[mask]\n",
    "           .groupby(\"phrase_num\")[\"raw_prob\"]\n",
    "           .rank(method=\"first\", ascending=False)\n",
    "           .astype(int)\n",
    "    )\n",
    "    out.loc[out[\"phrase_type\"].eq(\"reference\"), \"rank\"] = 0\n",
    "    out[\"rank\"] = out[\"rank\"].astype(int)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0dc75faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_scored = get_scored_df(n_gram_dict, known_text, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f663057",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_scored = get_scored_df(n_gram\n",
    "                               _dict, unknown_text, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d65e3b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "def score_phrases_no_context(\n",
    "    ref_phrase: str,\n",
    "    paraphrases: List[str],\n",
    "    tokenizer,\n",
    "    model\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Score the reference and each paraphrase *without* any ranking.\n",
    "    Returns:\n",
    "      phrase_type, phrase, tokens, log_probs, sum_log_probs, raw_prob\n",
    "    \"\"\"\n",
    "    items = [(\"reference\", ref_phrase)] + [(\"paraphrase\", p) for p in paraphrases]\n",
    "    rows = []\n",
    "\n",
    "    for idx, (ptype, phrase) in enumerate(items, start=1):\n",
    "        print(f\"→ [{idx}/{len(items)}] Processing {ptype}…\")\n",
    "        tokens_phrase, log_probs_phrase, _ = compute_log_probs_with_median(phrase, tokenizer, model)\n",
    "        phrase_total = sum(log_probs_phrase)\n",
    "        raw_prob = math.exp(phrase_total)  # unnormalized prob\n",
    "\n",
    "        rows.append({\n",
    "            \"phrase_type\":   ptype,\n",
    "            \"phrase\":        phrase,\n",
    "            \"tokens\":        tokens_phrase,\n",
    "            \"log_probs\":     log_probs_phrase,\n",
    "            \"sum_log_probs\": phrase_total,\n",
    "            \"raw_prob\":      raw_prob,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\n",
    "        \"phrase_type\", \"phrase\", \"tokens\", \"log_probs\", \"sum_log_probs\", \"raw_prob\"\n",
    "    ])\n",
    "\n",
    "def get_scored_df_no_context(n_gram_dict, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Row-concat each score_phrases_no_context df, add phrase_num, sort by phrase_num,\n",
    "    then rank paraphrases within each phrase_num by descending raw_prob.\n",
    "    'reference' rows always get rank 0.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for phrase_num, entry in n_gram_dict.items():  # insertion order preserved\n",
    "        phrase = entry[\"phrase\"]\n",
    "        paraphrases = entry[\"paraphrases\"]\n",
    "\n",
    "        df = score_phrases_no_context(phrase, paraphrases, tokenizer, model).copy()\n",
    "        df.insert(0, \"original_phrase\", phrase)\n",
    "        df.insert(0, \"phrase_num\", phrase_num)  # make it the first column\n",
    "        dfs.append(df)\n",
    "\n",
    "    if not dfs:\n",
    "        return pd.DataFrame(columns=[\"phrase_num\"])\n",
    "\n",
    "    out = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # zero-padded keys => lexicographic equals numeric order\n",
    "    out = out.sort_values(\"phrase_num\", kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "    # Rank within phrase_num: reference -> 0; paraphrases ranked by descending raw_prob starting at 1\n",
    "    out[\"rank\"] = None\n",
    "    mask_para = out[\"phrase_type\"].eq(\"paraphrase\")\n",
    "    out.loc[mask_para, \"rank\"] = (\n",
    "        out.loc[mask_para]\n",
    "           .groupby(\"phrase_num\")[\"raw_prob\"]\n",
    "           .rank(method=\"first\", ascending=False)  # use \"dense\" if you prefer 1,2,3 without gaps\n",
    "           .astype(int)\n",
    "    )\n",
    "    out.loc[out[\"phrase_type\"].eq(\"reference\"), \"rank\"] = 0\n",
    "    out[\"rank\"] = out[\"rank\"].astype(int)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ffa3c002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ [1/58] Processing reference…\n",
      "→ [2/58] Processing paraphrase…\n",
      "→ [3/58] Processing paraphrase…\n",
      "→ [4/58] Processing paraphrase…\n",
      "→ [5/58] Processing paraphrase…\n",
      "→ [6/58] Processing paraphrase…\n",
      "→ [7/58] Processing paraphrase…\n",
      "→ [8/58] Processing paraphrase…\n",
      "→ [9/58] Processing paraphrase…\n",
      "→ [10/58] Processing paraphrase…\n",
      "→ [11/58] Processing paraphrase…\n",
      "→ [12/58] Processing paraphrase…\n",
      "→ [13/58] Processing paraphrase…\n",
      "→ [14/58] Processing paraphrase…\n",
      "→ [15/58] Processing paraphrase…\n",
      "→ [16/58] Processing paraphrase…\n",
      "→ [17/58] Processing paraphrase…\n",
      "→ [18/58] Processing paraphrase…\n",
      "→ [19/58] Processing paraphrase…\n",
      "→ [20/58] Processing paraphrase…\n",
      "→ [21/58] Processing paraphrase…\n",
      "→ [22/58] Processing paraphrase…\n",
      "→ [23/58] Processing paraphrase…\n",
      "→ [24/58] Processing paraphrase…\n",
      "→ [25/58] Processing paraphrase…\n",
      "→ [26/58] Processing paraphrase…\n",
      "→ [27/58] Processing paraphrase…\n",
      "→ [28/58] Processing paraphrase…\n",
      "→ [29/58] Processing paraphrase…\n",
      "→ [30/58] Processing paraphrase…\n",
      "→ [31/58] Processing paraphrase…\n",
      "→ [32/58] Processing paraphrase…\n",
      "→ [33/58] Processing paraphrase…\n",
      "→ [34/58] Processing paraphrase…\n",
      "→ [35/58] Processing paraphrase…\n",
      "→ [36/58] Processing paraphrase…\n",
      "→ [37/58] Processing paraphrase…\n",
      "→ [38/58] Processing paraphrase…\n",
      "→ [39/58] Processing paraphrase…\n",
      "→ [40/58] Processing paraphrase…\n",
      "→ [41/58] Processing paraphrase…\n",
      "→ [42/58] Processing paraphrase…\n",
      "→ [43/58] Processing paraphrase…\n",
      "→ [44/58] Processing paraphrase…\n",
      "→ [45/58] Processing paraphrase…\n",
      "→ [46/58] Processing paraphrase…\n",
      "→ [47/58] Processing paraphrase…\n",
      "→ [48/58] Processing paraphrase…\n",
      "→ [49/58] Processing paraphrase…\n",
      "→ [50/58] Processing paraphrase…\n",
      "→ [51/58] Processing paraphrase…\n",
      "→ [52/58] Processing paraphrase…\n",
      "→ [53/58] Processing paraphrase…\n",
      "→ [54/58] Processing paraphrase…\n",
      "→ [55/58] Processing paraphrase…\n",
      "→ [56/58] Processing paraphrase…\n",
      "→ [57/58] Processing paraphrase…\n",
      "→ [58/58] Processing paraphrase…\n",
      "→ [1/12] Processing reference…\n",
      "→ [2/12] Processing paraphrase…\n",
      "→ [3/12] Processing paraphrase…\n",
      "→ [4/12] Processing paraphrase…\n",
      "→ [5/12] Processing paraphrase…\n",
      "→ [6/12] Processing paraphrase…\n",
      "→ [7/12] Processing paraphrase…\n",
      "→ [8/12] Processing paraphrase…\n",
      "→ [9/12] Processing paraphrase…\n",
      "→ [10/12] Processing paraphrase…\n",
      "→ [11/12] Processing paraphrase…\n",
      "→ [12/12] Processing paraphrase…\n",
      "→ [1/28] Processing reference…\n",
      "→ [2/28] Processing paraphrase…\n",
      "→ [3/28] Processing paraphrase…\n",
      "→ [4/28] Processing paraphrase…\n",
      "→ [5/28] Processing paraphrase…\n",
      "→ [6/28] Processing paraphrase…\n",
      "→ [7/28] Processing paraphrase…\n",
      "→ [8/28] Processing paraphrase…\n",
      "→ [9/28] Processing paraphrase…\n",
      "→ [10/28] Processing paraphrase…\n",
      "→ [11/28] Processing paraphrase…\n",
      "→ [12/28] Processing paraphrase…\n",
      "→ [13/28] Processing paraphrase…\n",
      "→ [14/28] Processing paraphrase…\n",
      "→ [15/28] Processing paraphrase…\n",
      "→ [16/28] Processing paraphrase…\n",
      "→ [17/28] Processing paraphrase…\n",
      "→ [18/28] Processing paraphrase…\n",
      "→ [19/28] Processing paraphrase…\n",
      "→ [20/28] Processing paraphrase…\n",
      "→ [21/28] Processing paraphrase…\n",
      "→ [22/28] Processing paraphrase…\n",
      "→ [23/28] Processing paraphrase…\n",
      "→ [24/28] Processing paraphrase…\n",
      "→ [25/28] Processing paraphrase…\n",
      "→ [26/28] Processing paraphrase…\n",
      "→ [27/28] Processing paraphrase…\n",
      "→ [28/28] Processing paraphrase…\n",
      "→ [1/20] Processing reference…\n",
      "→ [2/20] Processing paraphrase…\n",
      "→ [3/20] Processing paraphrase…\n",
      "→ [4/20] Processing paraphrase…\n",
      "→ [5/20] Processing paraphrase…\n",
      "→ [6/20] Processing paraphrase…\n",
      "→ [7/20] Processing paraphrase…\n",
      "→ [8/20] Processing paraphrase…\n",
      "→ [9/20] Processing paraphrase…\n",
      "→ [10/20] Processing paraphrase…\n",
      "→ [11/20] Processing paraphrase…\n",
      "→ [12/20] Processing paraphrase…\n",
      "→ [13/20] Processing paraphrase…\n",
      "→ [14/20] Processing paraphrase…\n",
      "→ [15/20] Processing paraphrase…\n",
      "→ [16/20] Processing paraphrase…\n",
      "→ [17/20] Processing paraphrase…\n",
      "→ [18/20] Processing paraphrase…\n",
      "→ [19/20] Processing paraphrase…\n",
      "→ [20/20] Processing paraphrase…\n",
      "→ [1/33] Processing reference…\n",
      "→ [2/33] Processing paraphrase…\n",
      "→ [3/33] Processing paraphrase…\n",
      "→ [4/33] Processing paraphrase…\n",
      "→ [5/33] Processing paraphrase…\n",
      "→ [6/33] Processing paraphrase…\n",
      "→ [7/33] Processing paraphrase…\n",
      "→ [8/33] Processing paraphrase…\n",
      "→ [9/33] Processing paraphrase…\n",
      "→ [10/33] Processing paraphrase…\n",
      "→ [11/33] Processing paraphrase…\n",
      "→ [12/33] Processing paraphrase…\n",
      "→ [13/33] Processing paraphrase…\n",
      "→ [14/33] Processing paraphrase…\n",
      "→ [15/33] Processing paraphrase…\n",
      "→ [16/33] Processing paraphrase…\n",
      "→ [17/33] Processing paraphrase…\n",
      "→ [18/33] Processing paraphrase…\n",
      "→ [19/33] Processing paraphrase…\n",
      "→ [20/33] Processing paraphrase…\n",
      "→ [21/33] Processing paraphrase…\n",
      "→ [22/33] Processing paraphrase…\n",
      "→ [23/33] Processing paraphrase…\n",
      "→ [24/33] Processing paraphrase…\n",
      "→ [25/33] Processing paraphrase…\n",
      "→ [26/33] Processing paraphrase…\n",
      "→ [27/33] Processing paraphrase…\n",
      "→ [28/33] Processing paraphrase…\n",
      "→ [29/33] Processing paraphrase…\n",
      "→ [30/33] Processing paraphrase…\n",
      "→ [31/33] Processing paraphrase…\n",
      "→ [32/33] Processing paraphrase…\n",
      "→ [33/33] Processing paraphrase…\n",
      "→ [1/16] Processing reference…\n",
      "→ [2/16] Processing paraphrase…\n",
      "→ [3/16] Processing paraphrase…\n",
      "→ [4/16] Processing paraphrase…\n",
      "→ [5/16] Processing paraphrase…\n",
      "→ [6/16] Processing paraphrase…\n",
      "→ [7/16] Processing paraphrase…\n",
      "→ [8/16] Processing paraphrase…\n",
      "→ [9/16] Processing paraphrase…\n",
      "→ [10/16] Processing paraphrase…\n",
      "→ [11/16] Processing paraphrase…\n",
      "→ [12/16] Processing paraphrase…\n",
      "→ [13/16] Processing paraphrase…\n",
      "→ [14/16] Processing paraphrase…\n",
      "→ [15/16] Processing paraphrase…\n",
      "→ [16/16] Processing paraphrase…\n",
      "→ [1/4] Processing reference…\n",
      "→ [2/4] Processing paraphrase…\n",
      "→ [3/4] Processing paraphrase…\n",
      "→ [4/4] Processing paraphrase…\n",
      "→ [1/22] Processing reference…\n",
      "→ [2/22] Processing paraphrase…\n",
      "→ [3/22] Processing paraphrase…\n",
      "→ [4/22] Processing paraphrase…\n",
      "→ [5/22] Processing paraphrase…\n",
      "→ [6/22] Processing paraphrase…\n",
      "→ [7/22] Processing paraphrase…\n",
      "→ [8/22] Processing paraphrase…\n",
      "→ [9/22] Processing paraphrase…\n",
      "→ [10/22] Processing paraphrase…\n",
      "→ [11/22] Processing paraphrase…\n",
      "→ [12/22] Processing paraphrase…\n",
      "→ [13/22] Processing paraphrase…\n",
      "→ [14/22] Processing paraphrase…\n",
      "→ [15/22] Processing paraphrase…\n",
      "→ [16/22] Processing paraphrase…\n",
      "→ [17/22] Processing paraphrase…\n",
      "→ [18/22] Processing paraphrase…\n",
      "→ [19/22] Processing paraphrase…\n",
      "→ [20/22] Processing paraphrase…\n",
      "→ [21/22] Processing paraphrase…\n",
      "→ [22/22] Processing paraphrase…\n",
      "→ [1/7] Processing reference…\n",
      "→ [2/7] Processing paraphrase…\n",
      "→ [3/7] Processing paraphrase…\n",
      "→ [4/7] Processing paraphrase…\n",
      "→ [5/7] Processing paraphrase…\n",
      "→ [6/7] Processing paraphrase…\n",
      "→ [7/7] Processing paraphrase…\n",
      "→ [1/4] Processing reference…\n",
      "→ [2/4] Processing paraphrase…\n",
      "→ [3/4] Processing paraphrase…\n",
      "→ [4/4] Processing paraphrase…\n",
      "→ [1/31] Processing reference…\n",
      "→ [2/31] Processing paraphrase…\n",
      "→ [3/31] Processing paraphrase…\n",
      "→ [4/31] Processing paraphrase…\n",
      "→ [5/31] Processing paraphrase…\n",
      "→ [6/31] Processing paraphrase…\n",
      "→ [7/31] Processing paraphrase…\n",
      "→ [8/31] Processing paraphrase…\n",
      "→ [9/31] Processing paraphrase…\n",
      "→ [10/31] Processing paraphrase…\n",
      "→ [11/31] Processing paraphrase…\n",
      "→ [12/31] Processing paraphrase…\n",
      "→ [13/31] Processing paraphrase…\n",
      "→ [14/31] Processing paraphrase…\n",
      "→ [15/31] Processing paraphrase…\n",
      "→ [16/31] Processing paraphrase…\n",
      "→ [17/31] Processing paraphrase…\n",
      "→ [18/31] Processing paraphrase…\n",
      "→ [19/31] Processing paraphrase…\n",
      "→ [20/31] Processing paraphrase…\n",
      "→ [21/31] Processing paraphrase…\n",
      "→ [22/31] Processing paraphrase…\n",
      "→ [23/31] Processing paraphrase…\n",
      "→ [24/31] Processing paraphrase…\n",
      "→ [25/31] Processing paraphrase…\n",
      "→ [26/31] Processing paraphrase…\n",
      "→ [27/31] Processing paraphrase…\n",
      "→ [28/31] Processing paraphrase…\n",
      "→ [29/31] Processing paraphrase…\n",
      "→ [30/31] Processing paraphrase…\n",
      "→ [31/31] Processing paraphrase…\n",
      "→ [1/17] Processing reference…\n",
      "→ [2/17] Processing paraphrase…\n",
      "→ [3/17] Processing paraphrase…\n",
      "→ [4/17] Processing paraphrase…\n",
      "→ [5/17] Processing paraphrase…\n",
      "→ [6/17] Processing paraphrase…\n",
      "→ [7/17] Processing paraphrase…\n",
      "→ [8/17] Processing paraphrase…\n",
      "→ [9/17] Processing paraphrase…\n",
      "→ [10/17] Processing paraphrase…\n",
      "→ [11/17] Processing paraphrase…\n",
      "→ [12/17] Processing paraphrase…\n",
      "→ [13/17] Processing paraphrase…\n",
      "→ [14/17] Processing paraphrase…\n",
      "→ [15/17] Processing paraphrase…\n",
      "→ [16/17] Processing paraphrase…\n",
      "→ [17/17] Processing paraphrase…\n",
      "→ [1/52] Processing reference…\n",
      "→ [2/52] Processing paraphrase…\n",
      "→ [3/52] Processing paraphrase…\n",
      "→ [4/52] Processing paraphrase…\n",
      "→ [5/52] Processing paraphrase…\n",
      "→ [6/52] Processing paraphrase…\n",
      "→ [7/52] Processing paraphrase…\n",
      "→ [8/52] Processing paraphrase…\n",
      "→ [9/52] Processing paraphrase…\n",
      "→ [10/52] Processing paraphrase…\n",
      "→ [11/52] Processing paraphrase…\n",
      "→ [12/52] Processing paraphrase…\n",
      "→ [13/52] Processing paraphrase…\n",
      "→ [14/52] Processing paraphrase…\n",
      "→ [15/52] Processing paraphrase…\n",
      "→ [16/52] Processing paraphrase…\n",
      "→ [17/52] Processing paraphrase…\n",
      "→ [18/52] Processing paraphrase…\n",
      "→ [19/52] Processing paraphrase…\n",
      "→ [20/52] Processing paraphrase…\n",
      "→ [21/52] Processing paraphrase…\n",
      "→ [22/52] Processing paraphrase…\n",
      "→ [23/52] Processing paraphrase…\n",
      "→ [24/52] Processing paraphrase…\n",
      "→ [25/52] Processing paraphrase…\n",
      "→ [26/52] Processing paraphrase…\n",
      "→ [27/52] Processing paraphrase…\n",
      "→ [28/52] Processing paraphrase…\n",
      "→ [29/52] Processing paraphrase…\n",
      "→ [30/52] Processing paraphrase…\n",
      "→ [31/52] Processing paraphrase…\n",
      "→ [32/52] Processing paraphrase…\n",
      "→ [33/52] Processing paraphrase…\n",
      "→ [34/52] Processing paraphrase…\n",
      "→ [35/52] Processing paraphrase…\n",
      "→ [36/52] Processing paraphrase…\n",
      "→ [37/52] Processing paraphrase…\n",
      "→ [38/52] Processing paraphrase…\n",
      "→ [39/52] Processing paraphrase…\n",
      "→ [40/52] Processing paraphrase…\n",
      "→ [41/52] Processing paraphrase…\n",
      "→ [42/52] Processing paraphrase…\n",
      "→ [43/52] Processing paraphrase…\n",
      "→ [44/52] Processing paraphrase…\n",
      "→ [45/52] Processing paraphrase…\n",
      "→ [46/52] Processing paraphrase…\n",
      "→ [47/52] Processing paraphrase…\n",
      "→ [48/52] Processing paraphrase…\n",
      "→ [49/52] Processing paraphrase…\n",
      "→ [50/52] Processing paraphrase…\n",
      "→ [51/52] Processing paraphrase…\n",
      "→ [52/52] Processing paraphrase…\n",
      "→ [1/10] Processing reference…\n",
      "→ [2/10] Processing paraphrase…\n",
      "→ [3/10] Processing paraphrase…\n",
      "→ [4/10] Processing paraphrase…\n",
      "→ [5/10] Processing paraphrase…\n",
      "→ [6/10] Processing paraphrase…\n",
      "→ [7/10] Processing paraphrase…\n",
      "→ [8/10] Processing paraphrase…\n",
      "→ [9/10] Processing paraphrase…\n",
      "→ [10/10] Processing paraphrase…\n",
      "→ [1/11] Processing reference…\n",
      "→ [2/11] Processing paraphrase…\n",
      "→ [3/11] Processing paraphrase…\n",
      "→ [4/11] Processing paraphrase…\n",
      "→ [5/11] Processing paraphrase…\n",
      "→ [6/11] Processing paraphrase…\n",
      "→ [7/11] Processing paraphrase…\n",
      "→ [8/11] Processing paraphrase…\n",
      "→ [9/11] Processing paraphrase…\n",
      "→ [10/11] Processing paraphrase…\n",
      "→ [11/11] Processing paraphrase…\n",
      "→ [1/4] Processing reference…\n",
      "→ [2/4] Processing paraphrase…\n",
      "→ [3/4] Processing paraphrase…\n",
      "→ [4/4] Processing paraphrase…\n",
      "→ [1/8] Processing reference…\n",
      "→ [2/8] Processing paraphrase…\n",
      "→ [3/8] Processing paraphrase…\n",
      "→ [4/8] Processing paraphrase…\n",
      "→ [5/8] Processing paraphrase…\n",
      "→ [6/8] Processing paraphrase…\n",
      "→ [7/8] Processing paraphrase…\n",
      "→ [8/8] Processing paraphrase…\n",
      "→ [1/8] Processing reference…\n",
      "→ [2/8] Processing paraphrase…\n",
      "→ [3/8] Processing paraphrase…\n",
      "→ [4/8] Processing paraphrase…\n",
      "→ [5/8] Processing paraphrase…\n",
      "→ [6/8] Processing paraphrase…\n",
      "→ [7/8] Processing paraphrase…\n",
      "→ [8/8] Processing paraphrase…\n"
     ]
    }
   ],
   "source": [
    "score_df_no_context = get_scored_df_no_context(n_gram_dict, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "374ff5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_phrases = score_df_no_context[['phrase_num', 'original_phrase']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6c603218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# remove illegal control chars (keep \\t, \\n, \\r)\n",
    "_ILLEGAL_RE = re.compile(r\"[\\x00-\\x08\\x0B-\\x0C\\x0E-\\x1F]\")\n",
    "\n",
    "def _clean_cell(x):\n",
    "    if isinstance(x, str):\n",
    "        return _ILLEGAL_RE.sub(\"\", x)\n",
    "    return x\n",
    "\n",
    "def clean_for_excel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    obj_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    df[obj_cols] = df[obj_cols].applymap(_clean_cell)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3dfbe8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-66>:15: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "<positron-console-cell-66>:15: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "<positron-console-cell-66>:15: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "<positron-console-cell-66>:15: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "<positron-console-cell-66>:15: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n"
     ]
    }
   ],
   "source": [
    "save_loc = f\"/Users/user/Documents/PhD Stuff/Supervisor Meetings/paraphrase examples/{specific_problem}_v2.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(save_loc, engine=\"openpyxl\") as xls:\n",
    "    clean_for_excel(specific_problem_metadata).to_excel(xls, sheet_name=\"metadata\", index=False)\n",
    "    clean_for_excel(score_df_no_context).to_excel(xls, sheet_name=\"no context\", index=False)\n",
    "    clean_for_excel(known_scored).to_excel(xls, sheet_name=\"known\", index=False)\n",
    "    clean_for_excel(unknown_scored).to_excel(xls, sheet_name=\"unknown\", index=False)\n",
    "    clean_for_excel(distinct_phrases).to_excel(xls, sheet_name=\"LLR\", index=False)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
