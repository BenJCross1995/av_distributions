{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a04acc0",
   "metadata": {},
   "source": [
    "# Run Paraphrase Common Token N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500a216e",
   "metadata": {},
   "source": [
    "## Load Libraries\n",
    "\n",
    "Load libraries from standard modules and my own code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02e7b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List\n",
    "\n",
    "sys.path.append(os.path.abspath('../../../src'))\n",
    "\n",
    "from read_and_write_docs import read_jsonl, read_rds\n",
    "from tokenize_and_score import load_model\n",
    "from utils import get_base_location, apply_temp_doc_id, build_metadata_df\n",
    "from n_gram_functions import (\n",
    "    common_ngrams,\n",
    "    pretty_print_common_ngrams,\n",
    "    keep_before_phrase,\n",
    "    compute_log_probs_with_median\n",
    ")\n",
    "from open_ai import initialise_client, llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56192fae",
   "metadata": {},
   "source": [
    "## Set Locations & Load Base Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b0f4ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"Wiki\"\n",
    "data_type = \"training\"\n",
    "\n",
    "# Set NAS so can run on Windows laptop seamlessly\n",
    "nas_base_loc = get_base_location()\n",
    "\n",
    "# Load known data\n",
    "known_loc = f\"{nas_base_loc}/datasets/author_verification/{data_type}/{corpus}/known_raw.jsonl\"\n",
    "known = read_jsonl(known_loc)\n",
    "known = apply_temp_doc_id(known)\n",
    "\n",
    "# Load unknown data\n",
    "unknown_loc = f\"{nas_base_loc}/datasets/author_verification/{data_type}/{corpus}/unknown_raw.jsonl\"\n",
    "unknown = read_jsonl(unknown_loc)\n",
    "unknown_df = apply_temp_doc_id(unknown)\n",
    "\n",
    "# Load and build metadata\n",
    "metadata_loc = f\"{nas_base_loc}/datasets/author_verification/{data_type}/metadata.rds\"\n",
    "metadata = read_rds(metadata_loc)\n",
    "filtered_metadata = metadata[metadata['corpus'] == corpus]\n",
    "agg_metadata = build_metadata_df(filtered_metadata, known, unknown)\n",
    "\n",
    "# Load the pre-made problem datasets for speed\n",
    "problem_dataset_base = f\"{nas_base_loc}/datasets/author_verification/{data_type}/{corpus}\"\n",
    "problem_dataset_agg = read_jsonl(f\"{problem_dataset_base}/{corpus}_{data_type}_agg.jsonl\")\n",
    "problem_dataset_profile = read_jsonl(f\"{problem_dataset_base}/{corpus}_{data_type}_profile.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d85eaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = load_model(f\"{nas_base_loc}/models/Qwen 2.5/Qwen2.5-0.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e384cb",
   "metadata": {},
   "source": [
    "## View Same and Different-Author Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18fa9b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>known_author</th>\n",
       "      <th>unknown_author</th>\n",
       "      <th>known_doc_id</th>\n",
       "      <th>unknown_doc_id</th>\n",
       "      <th>highest_common_count</th>\n",
       "      <th>highest_common_ngram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>Greg_L vs Greg_L</td>\n",
       "      <td>Greg_L</td>\n",
       "      <td>Greg_L</td>\n",
       "      <td>greg_l_text_11</td>\n",
       "      <td>greg_l_text_10</td>\n",
       "      <td>9</td>\n",
       "      <td>, Ġthey Ġshould Ġhave Ġparticipated Ġin Ġthe Ġ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>Haymaker vs Haymaker</td>\n",
       "      <td>Haymaker</td>\n",
       "      <td>Haymaker</td>\n",
       "      <td>haymaker_text_3</td>\n",
       "      <td>haymaker_text_2</td>\n",
       "      <td>9</td>\n",
       "      <td>Ġat Ġthe Ġend Ġof Ġthe Ġday , Ġwe 're</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>Fragments_of_Jade vs Fragments_of_Jade</td>\n",
       "      <td>Fragments_of_Jade</td>\n",
       "      <td>Fragments_of_Jade</td>\n",
       "      <td>fragments_of_jade_text_2</td>\n",
       "      <td>fragments_of_jade_text_10</td>\n",
       "      <td>8</td>\n",
       "      <td>Ġme , Ġand Ġit 's Ġgetting Ġold .Ċ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>Fixentries vs Fixentries</td>\n",
       "      <td>Fixentries</td>\n",
       "      <td>Fixentries</td>\n",
       "      <td>fixentries_text_2</td>\n",
       "      <td>fixentries_text_5</td>\n",
       "      <td>8</td>\n",
       "      <td>Ġthe Ġindividual Ġher it ability Ġof Ġintellig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>DonaNobisPacem vs DonaNobisPacem</td>\n",
       "      <td>DonaNobisPacem</td>\n",
       "      <td>DonaNobisPacem</td>\n",
       "      <td>donanobispacem_text_5</td>\n",
       "      <td>donanobispacem_text_2</td>\n",
       "      <td>8</td>\n",
       "      <td>Ġafter Ġ 1 8 - 2 0 Ġweeks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>David_Shankbone vs David_Shankbone</td>\n",
       "      <td>David_Shankbone</td>\n",
       "      <td>David_Shankbone</td>\n",
       "      <td>david_shankbone_text_1</td>\n",
       "      <td>david_shankbone_text_4</td>\n",
       "      <td>3</td>\n",
       "      <td>, Ġwhich Ġis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>D7G1DX~0 vs D7G1DX~0</td>\n",
       "      <td>D7G1DX~0</td>\n",
       "      <td>D7G1DX~0</td>\n",
       "      <td>d7g1dx_0_text_2</td>\n",
       "      <td>d7g1dx_0_text_5</td>\n",
       "      <td>3</td>\n",
       "      <td>Ġdon 't Ġthink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>Collect vs Collect</td>\n",
       "      <td>Collect</td>\n",
       "      <td>Collect</td>\n",
       "      <td>collect_text_12</td>\n",
       "      <td>collect_text_11</td>\n",
       "      <td>3</td>\n",
       "      <td>, Ġand Ġi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Cptnono vs Cptnono</td>\n",
       "      <td>Cptnono</td>\n",
       "      <td>Cptnono</td>\n",
       "      <td>cptnono_text_1</td>\n",
       "      <td>cptnono_text_12</td>\n",
       "      <td>3</td>\n",
       "      <td>, Ġthough .Ċ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>Danlaycock vs Danlaycock</td>\n",
       "      <td>Danlaycock</td>\n",
       "      <td>Danlaycock</td>\n",
       "      <td>danlaycock_text_5</td>\n",
       "      <td>danlaycock_text_2</td>\n",
       "      <td>3</td>\n",
       "      <td>.Ċ also ,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    problem       known_author  \\\n",
       "379                        Greg_L vs Greg_L             Greg_L   \n",
       "432                    Haymaker vs Haymaker           Haymaker   \n",
       "354  Fragments_of_Jade vs Fragments_of_Jade  Fragments_of_Jade   \n",
       "337                Fixentries vs Fixentries         Fixentries   \n",
       "248        DonaNobisPacem vs DonaNobisPacem     DonaNobisPacem   \n",
       "..                                      ...                ...   \n",
       "222      David_Shankbone vs David_Shankbone    David_Shankbone   \n",
       "211                    D7G1DX~0 vs D7G1DX~0           D7G1DX~0   \n",
       "187                      Collect vs Collect            Collect   \n",
       "198                      Cptnono vs Cptnono            Cptnono   \n",
       "218                Danlaycock vs Danlaycock         Danlaycock   \n",
       "\n",
       "        unknown_author              known_doc_id             unknown_doc_id  \\\n",
       "379             Greg_L            greg_l_text_11             greg_l_text_10   \n",
       "432           Haymaker           haymaker_text_3            haymaker_text_2   \n",
       "354  Fragments_of_Jade  fragments_of_jade_text_2  fragments_of_jade_text_10   \n",
       "337         Fixentries         fixentries_text_2          fixentries_text_5   \n",
       "248     DonaNobisPacem     donanobispacem_text_5      donanobispacem_text_2   \n",
       "..                 ...                       ...                        ...   \n",
       "222    David_Shankbone    david_shankbone_text_1     david_shankbone_text_4   \n",
       "211           D7G1DX~0           d7g1dx_0_text_2            d7g1dx_0_text_5   \n",
       "187            Collect           collect_text_12            collect_text_11   \n",
       "198            Cptnono            cptnono_text_1            cptnono_text_12   \n",
       "218         Danlaycock         danlaycock_text_5          danlaycock_text_2   \n",
       "\n",
       "     highest_common_count                               highest_common_ngram  \n",
       "379                     9  , Ġthey Ġshould Ġhave Ġparticipated Ġin Ġthe Ġ...  \n",
       "432                     9              Ġat Ġthe Ġend Ġof Ġthe Ġday , Ġwe 're  \n",
       "354                     8                 Ġme , Ġand Ġit 's Ġgetting Ġold .Ċ  \n",
       "337                     8  Ġthe Ġindividual Ġher it ability Ġof Ġintellig...  \n",
       "248                     8                          Ġafter Ġ 1 8 - 2 0 Ġweeks  \n",
       "..                    ...                                                ...  \n",
       "222                     3                                       , Ġwhich Ġis  \n",
       "211                     3                                     Ġdon 't Ġthink  \n",
       "187                     3                                          , Ġand Ġi  \n",
       "198                     3                                       , Ġthough .Ċ  \n",
       "218                     3                                          .Ċ also ,  \n",
       "\n",
       "[197 rows x 7 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_probs = problem_dataset_agg[problem_dataset_agg['known_author'] == problem_dataset_agg['unknown_author']].copy()\n",
    "same_probs.sort_values([\"highest_common_count\"], ascending=[False], inplace=True)\n",
    "same_probs[(same_probs['highest_common_count'] >= 3) & (same_probs['highest_common_count'] <= 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2eb7681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>known_author</th>\n",
       "      <th>unknown_author</th>\n",
       "      <th>known_doc_id</th>\n",
       "      <th>unknown_doc_id</th>\n",
       "      <th>highest_common_count</th>\n",
       "      <th>highest_common_ngram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>Enemesis vs Equanimous1</td>\n",
       "      <td>Enemesis</td>\n",
       "      <td>Equanimous1</td>\n",
       "      <td>enemesis_text_3</td>\n",
       "      <td>equanimous1_text_5</td>\n",
       "      <td>5</td>\n",
       "      <td>Ġthe Ġcurrent Ġstate Ġof Ġthe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Caboga vs Chanakyathegreat</td>\n",
       "      <td>Caboga</td>\n",
       "      <td>Chanakyathegreat</td>\n",
       "      <td>caboga_text_5</td>\n",
       "      <td>chanakyathegreat_text_1</td>\n",
       "      <td>5</td>\n",
       "      <td>Ġ 2 0 0 8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>Dweller vs Ecelan</td>\n",
       "      <td>Dweller</td>\n",
       "      <td>Ecelan</td>\n",
       "      <td>dweller_text_3</td>\n",
       "      <td>ecelan_text_2</td>\n",
       "      <td>5</td>\n",
       "      <td>.Ċ i Ġdon 't Ġthink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>Fyunck(click) vs Garda40</td>\n",
       "      <td>Fyunck(click)</td>\n",
       "      <td>Garda40</td>\n",
       "      <td>fyunck_click_text_12</td>\n",
       "      <td>garda40_text_1</td>\n",
       "      <td>5</td>\n",
       "      <td>Ġ 2 0 1 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>Fyunck(click) vs Garda40</td>\n",
       "      <td>Fyunck(click)</td>\n",
       "      <td>Garda40</td>\n",
       "      <td>fyunck_click_text_2</td>\n",
       "      <td>garda40_text_1</td>\n",
       "      <td>5</td>\n",
       "      <td>Ġ 2 0 1 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>DIREKTOR vs DonaNobisPacem</td>\n",
       "      <td>DIREKTOR</td>\n",
       "      <td>DonaNobisPacem</td>\n",
       "      <td>direktor_text_10</td>\n",
       "      <td>donanobispacem_text_2</td>\n",
       "      <td>3</td>\n",
       "      <td>.Ċ the Ġpoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Dennis_Brown vs DIREKTOR</td>\n",
       "      <td>Dennis_Brown</td>\n",
       "      <td>DIREKTOR</td>\n",
       "      <td>dennis_brown_text_1</td>\n",
       "      <td>direktor_text_1</td>\n",
       "      <td>3</td>\n",
       "      <td>, Ġand Ġhad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>DIREKTOR vs DonaNobisPacem</td>\n",
       "      <td>DIREKTOR</td>\n",
       "      <td>DonaNobisPacem</td>\n",
       "      <td>direktor_text_3</td>\n",
       "      <td>donanobispacem_text_2</td>\n",
       "      <td>3</td>\n",
       "      <td>Ġi 'm Ġnot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>Dennis_Brown vs DIREKTOR</td>\n",
       "      <td>Dennis_Brown</td>\n",
       "      <td>DIREKTOR</td>\n",
       "      <td>dennis_brown_text_10</td>\n",
       "      <td>direktor_text_1</td>\n",
       "      <td>3</td>\n",
       "      <td>, Ġbut Ġit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>DIREKTOR vs DonaNobisPacem</td>\n",
       "      <td>DIREKTOR</td>\n",
       "      <td>DonaNobisPacem</td>\n",
       "      <td>direktor_text_11</td>\n",
       "      <td>donanobispacem_text_2</td>\n",
       "      <td>3</td>\n",
       "      <td>Ġdon 't Ġthink</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        problem   known_author    unknown_author  \\\n",
       "287     Enemesis vs Equanimous1       Enemesis       Equanimous1   \n",
       "161  Caboga vs Chanakyathegreat         Caboga  Chanakyathegreat   \n",
       "262           Dweller vs Ecelan        Dweller            Ecelan   \n",
       "364    Fyunck(click) vs Garda40  Fyunck(click)           Garda40   \n",
       "365    Fyunck(click) vs Garda40  Fyunck(click)           Garda40   \n",
       "..                          ...            ...               ...   \n",
       "243  DIREKTOR vs DonaNobisPacem       DIREKTOR    DonaNobisPacem   \n",
       "237    Dennis_Brown vs DIREKTOR   Dennis_Brown          DIREKTOR   \n",
       "245  DIREKTOR vs DonaNobisPacem       DIREKTOR    DonaNobisPacem   \n",
       "238    Dennis_Brown vs DIREKTOR   Dennis_Brown          DIREKTOR   \n",
       "244  DIREKTOR vs DonaNobisPacem       DIREKTOR    DonaNobisPacem   \n",
       "\n",
       "             known_doc_id           unknown_doc_id  highest_common_count  \\\n",
       "287       enemesis_text_3       equanimous1_text_5                     5   \n",
       "161         caboga_text_5  chanakyathegreat_text_1                     5   \n",
       "262        dweller_text_3            ecelan_text_2                     5   \n",
       "364  fyunck_click_text_12           garda40_text_1                     5   \n",
       "365   fyunck_click_text_2           garda40_text_1                     5   \n",
       "..                    ...                      ...                   ...   \n",
       "243      direktor_text_10    donanobispacem_text_2                     3   \n",
       "237   dennis_brown_text_1          direktor_text_1                     3   \n",
       "245       direktor_text_3    donanobispacem_text_2                     3   \n",
       "238  dennis_brown_text_10          direktor_text_1                     3   \n",
       "244      direktor_text_11    donanobispacem_text_2                     3   \n",
       "\n",
       "              highest_common_ngram  \n",
       "287  Ġthe Ġcurrent Ġstate Ġof Ġthe  \n",
       "161                      Ġ 2 0 0 8  \n",
       "262            .Ċ i Ġdon 't Ġthink  \n",
       "364                      Ġ 2 0 1 1  \n",
       "365                      Ġ 2 0 1 2  \n",
       "..                             ...  \n",
       "243                  .Ċ the Ġpoint  \n",
       "237                    , Ġand Ġhad  \n",
       "245                     Ġi 'm Ġnot  \n",
       "238                     , Ġbut Ġit  \n",
       "244                 Ġdon 't Ġthink  \n",
       "\n",
       "[179 rows x 7 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_probs = problem_dataset_agg[problem_dataset_agg['known_author'] != problem_dataset_agg['unknown_author']].copy()\n",
    "diff_probs.sort_values([\"highest_common_count\"], ascending=[False], inplace=True)\n",
    "diff_probs[(diff_probs['highest_common_count'] >= 3) & (diff_probs['highest_common_count'] <= 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a0c885",
   "metadata": {},
   "source": [
    "## Select Known and Unknown Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb6993e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on problem: Dweller vs Ecelan\n"
     ]
    }
   ],
   "source": [
    "known_doc = \"dweller_text_3\"\n",
    "known_text = known[known['doc_id'] == known_doc].reset_index().loc[0, 'text']\n",
    "\n",
    "unknown_doc = \"ecelan_text_2\"\n",
    "unknown_text = unknown[unknown['doc_id'] == unknown_doc].reset_index().loc[0, 'text']\n",
    "\n",
    "# Get the metadata for current problem, will be added to Excel\n",
    "p_metadata = agg_metadata[(agg_metadata['known_doc_id'] == known_doc) \n",
    "                          & ((agg_metadata['unknown_doc_id'] == unknown_doc))].reset_index()\n",
    "p_metadata['target'] = p_metadata['known_author'] == p_metadata['unknown_author']\n",
    "specific_problem = p_metadata.loc[0, 'problem']\n",
    "\n",
    "print(f\"Working on problem: {specific_problem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48292f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = pd.DataFrame(\n",
    "    {\n",
    "        \"known\":   [corpus, data_type, known_doc, known_text],\n",
    "        \"unknown\": [corpus, data_type, unknown_doc, unknown_text],\n",
    "    },\n",
    "    index=[\"corpus\", \"data type\", \"doc\", \"text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bdf3b5",
   "metadata": {},
   "source": [
    "## Get N-Grams in Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b72faabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\".\\ni don't think\",\n",
       " ' and the',\n",
       " \" didn't\",\n",
       " ' from the',\n",
       " ' in the',\n",
       " ' is a',\n",
       " \" it's\",\n",
       " ' of my',\n",
       " ' of the',\n",
       " ' on the',\n",
       " ' questions,',\n",
       " ' should be',\n",
       " ' the article',\n",
       " ' to do',\n",
       " ' what i',\n",
       " ', even',\n",
       " ', i',\n",
       " ', in',\n",
       " ', it',\n",
       " ', or',\n",
       " '.\\nbut',\n",
       " '.\\ncan',\n",
       " '.\\nthe']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common = common_ngrams(known_text, unknown_text, 2, model, tokenizer, lowercase=True)\n",
    "n_gram_list = pretty_print_common_ngrams(common, tokenizer=tokenizer, order='len_desc', return_format='flat')\n",
    "n_gram_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eded3645",
   "metadata": {},
   "source": [
    "## Initialise OpenAI Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f9b6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = initialise_client(\"../../../credentials.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e319289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_prompt(known_text, phrase):\n",
    "    user_prompt = f\"\"\"\n",
    "<DOC>\n",
    "{known_text}\n",
    "</DOC>\n",
    "<NGRAM>\n",
    "\"{phrase}\"\n",
    "</NGRAM>\n",
    "\"\"\"\n",
    "    \n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e3b7d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_system_prompt(prompt_loc):\n",
    "    with open(prompt_loc,\"r\") as f:\n",
    "        system_prompt = f.read()\n",
    "        \n",
    "    return system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bca10da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_paraphrases(response, phrase):\n",
    "    \n",
    "    paraphrase_list = []\n",
    "    for i in range(1, len(response.choices)):\n",
    "        content = response.choices[i].message.content\n",
    "        \n",
    "        try:\n",
    "            content_json = json.loads(content)\n",
    "            for para in content_json['paraphrases']:\n",
    "                if para != phrase:\n",
    "                    paraphrase_list.append(para)  \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    unique_list = list(set(paraphrase_list))\n",
    "    \n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85daabcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = create_system_prompt(\"../../../prompts/exhaustive_constrained_ngram_paraphraser_prompt_JSON.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2466a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_dict = {}\n",
    "width = len(str(len(n_gram_list)))  # e.g., 10 -> 2, 100 -> 3\n",
    "\n",
    "for idx, phrase in enumerate(n_gram_list, start=1):\n",
    "    user_prompt = create_user_prompt(known_text, phrase)\n",
    "    response = llm(\n",
    "        system_prompt,\n",
    "        user_prompt,\n",
    "        client,\n",
    "        model=\"gpt-4.1\",\n",
    "        max_tokens=5000,\n",
    "        temperature=0.7,\n",
    "        n=10,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    paraphrases = parse_paraphrases(response, phrase)\n",
    "    key = f\"phrase_{idx:0{width}d}\"  # -> phrase_01, phrase_002, etc.\n",
    "    n_gram_dict[key] = {\"phrase\": phrase, \"paraphrases\": paraphrases}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd80e569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_phrases(\n",
    "    base_text: str,\n",
    "    ref_phrase: str,\n",
    "    paraphrases: list[str],\n",
    "    tokenizer,\n",
    "    model\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns rows for the reference and each paraphrase with:\n",
    "      sum_log_probs_phrase (log-likelihood for the phrase tokens) and\n",
    "      raw_prob = exp(sum_log_probs_phrase)\n",
    "    \"\"\"\n",
    "    # 1) score base_text\n",
    "    _, log_probs_base, _ = compute_log_probs_with_median(base_text.strip(), tokenizer, model)\n",
    "    base_total = sum(log_probs_base)\n",
    "\n",
    "    items = [(\"reference\", ref_phrase)] + [(\"paraphrase\", p) for p in paraphrases]\n",
    "    rows = []\n",
    "\n",
    "    for ptype, phrase in items:\n",
    "        # a) phrase alone → token count\n",
    "        tokens_phrase, log_probs_phrase, _ = compute_log_probs_with_median(phrase, tokenizer, model)\n",
    "        n_phrase_tokens = len(tokens_phrase)\n",
    "\n",
    "        # b) full sequence\n",
    "        full_text = base_text + phrase\n",
    "        tokens_full, log_probs_full, _ = compute_log_probs_with_median(full_text, tokenizer, model)\n",
    "\n",
    "        # c) full sum (base + phrase)\n",
    "        sum_before = sum(log_probs_full)\n",
    "\n",
    "        # d/e) last n tokens correspond to phrase\n",
    "        phrase_tokens    = tokens_full[-n_phrase_tokens:]\n",
    "        phrase_log_probs = log_probs_full[-n_phrase_tokens:]\n",
    "\n",
    "        # f) totals\n",
    "        phrase_total = sum(phrase_log_probs)\n",
    "        difference   = base_total - sum_before  # typically == -phrase_total\n",
    "\n",
    "        # raw (unnormalized) probability of the phrase given the base\n",
    "        raw_prob = math.exp(phrase_total)  # may underflow to 0.0 for long phrases; that's fine\n",
    "\n",
    "        rows.append({\n",
    "            \"phrase_type\":               ptype,\n",
    "            \"phrase\":                    phrase,\n",
    "            \"tokens\":                    phrase_tokens,\n",
    "            \"sum_log_probs_base\":        base_total,\n",
    "            \"sum_log_probs_inc_phrase\":  sum_before,\n",
    "            \"difference\":                difference,\n",
    "            \"phrase_log_probs\":          phrase_log_probs,\n",
    "            \"sum_log_probs_phrase\":      phrase_total,\n",
    "            \"raw_prob\":                  raw_prob,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\n",
    "        \"phrase_type\", \"phrase\", \"tokens\",\n",
    "        \"sum_log_probs_base\", \"sum_log_probs_inc_phrase\",\n",
    "        \"difference\", \"phrase_log_probs\", \"sum_log_probs_phrase\",\n",
    "        \"raw_prob\",\n",
    "    ])\n",
    "\n",
    "def get_scored_df(n_gram_dict, full_text, tokenizer, model):\n",
    "    \"\"\"Row-concat each scored df, add phrase_num, sort, then rank paraphrases within each phrase_num.\"\"\"\n",
    "    dfs = []\n",
    "    for phrase_num, entry in n_gram_dict.items():  # relies on insertion order\n",
    "        print(f\"Processing Phrase - {phrase_num}\")\n",
    "        phrase = entry[\"phrase\"]\n",
    "        paraphrases = entry[\"paraphrases\"]\n",
    "\n",
    "        base_text = keep_before_phrase(full_text, phrase)\n",
    "\n",
    "        df = score_phrases(base_text, phrase, paraphrases, tokenizer, model).copy()\n",
    "        df.insert(0, \"original_phrase\", phrase)\n",
    "        df.insert(0, \"phrase_num\", phrase_num)  # first column\n",
    "        dfs.append(df)\n",
    "\n",
    "    if not dfs:\n",
    "        return pd.DataFrame(columns=[\"phrase_num\"])\n",
    "\n",
    "    out = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # sort by phrase_num (zero-padded → lexicographic == numeric)\n",
    "    out = out.sort_values(\"phrase_num\", kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "    # rank within phrase_num: reference -> 0; paraphrases ranked by descending raw_prob starting at 1\n",
    "    out[\"rank\"] = None\n",
    "    mask = out[\"phrase_type\"].eq(\"paraphrase\")\n",
    "    out.loc[mask, \"rank\"] = (\n",
    "        out.loc[mask]\n",
    "           .groupby(\"phrase_num\")[\"raw_prob\"]\n",
    "           .rank(method=\"first\", ascending=False)\n",
    "           .astype(int)\n",
    "    )\n",
    "    out.loc[out[\"phrase_type\"].eq(\"reference\"), \"rank\"] = 0\n",
    "    out[\"rank\"] = out[\"rank\"].astype(int)\n",
    "\n",
    "    out = out.sort_values([\"phrase_num\", \"rank\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc75faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Phrase - phrase_01\n",
      "Processing Phrase - phrase_02\n",
      "Processing Phrase - phrase_03\n",
      "Processing Phrase - phrase_04\n",
      "Processing Phrase - phrase_05\n",
      "Processing Phrase - phrase_06\n",
      "Processing Phrase - phrase_07\n",
      "Processing Phrase - phrase_08\n",
      "Processing Phrase - phrase_09\n",
      "Processing Phrase - phrase_10\n"
     ]
    }
   ],
   "source": [
    "known_scored = get_scored_df(n_gram_dict, known_text, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f663057",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_scored = get_scored_df(n_gram_dict, unknown_text, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65e3b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_phrases_no_context(\n",
    "    ref_phrase: str,\n",
    "    paraphrases: List[str],\n",
    "    tokenizer,\n",
    "    model\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Score the reference and each paraphrase *without* any ranking.\n",
    "    Returns:\n",
    "      phrase_type, phrase, tokens, log_probs, sum_log_probs, raw_prob\n",
    "    \"\"\"\n",
    "    items = [(\"reference\", ref_phrase)] + [(\"paraphrase\", p) for p in paraphrases]\n",
    "    rows = []\n",
    "\n",
    "    for idx, (ptype, phrase) in enumerate(items, start=1):\n",
    "        print(f\"→ [{idx}/{len(items)}] Processing {ptype}…\")\n",
    "        tokens_phrase, log_probs_phrase, _ = compute_log_probs_with_median(phrase, tokenizer, model)\n",
    "        phrase_total = sum(log_probs_phrase)\n",
    "        raw_prob = math.exp(phrase_total)  # unnormalized prob\n",
    "\n",
    "        rows.append({\n",
    "            \"phrase_type\":   ptype,\n",
    "            \"phrase\":        phrase,\n",
    "            \"tokens\":        tokens_phrase,\n",
    "            \"log_probs\":     log_probs_phrase,\n",
    "            \"sum_log_probs\": phrase_total,\n",
    "            \"raw_prob\":      raw_prob,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\n",
    "        \"phrase_type\", \"phrase\", \"tokens\", \"log_probs\", \"sum_log_probs\", \"raw_prob\"\n",
    "    ])\n",
    "\n",
    "def get_scored_df_no_context(n_gram_dict, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Row-concat each score_phrases_no_context df, add phrase_num, sort by phrase_num,\n",
    "    then rank paraphrases within each phrase_num by descending raw_prob.\n",
    "    'reference' rows always get rank 0.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for phrase_num, entry in n_gram_dict.items():  # insertion order preserved\n",
    "        print(f\"Processing Phrase - {phrase_num}\")\n",
    "        phrase = entry[\"phrase\"]\n",
    "        paraphrases = entry[\"paraphrases\"]\n",
    "\n",
    "        df = score_phrases_no_context(phrase, paraphrases, tokenizer, model).copy()\n",
    "        df.insert(0, \"original_phrase\", phrase)\n",
    "        df.insert(0, \"phrase_num\", phrase_num)  # make it the first column\n",
    "        dfs.append(df)\n",
    "\n",
    "    if not dfs:\n",
    "        return pd.DataFrame(columns=[\"phrase_num\"])\n",
    "\n",
    "    out = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # zero-padded keys => lexicographic equals numeric order\n",
    "    out = out.sort_values(\"phrase_num\", kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "    # Rank within phrase_num: reference -> 0; paraphrases ranked by descending raw_prob starting at 1\n",
    "    out[\"rank\"] = None\n",
    "    mask_para = out[\"phrase_type\"].eq(\"paraphrase\")\n",
    "    out.loc[mask_para, \"rank\"] = (\n",
    "        out.loc[mask_para]\n",
    "           .groupby(\"phrase_num\")[\"raw_prob\"]\n",
    "           .rank(method=\"first\", ascending=False)  # use \"dense\" if you prefer 1,2,3 without gaps\n",
    "           .astype(int)\n",
    "    )\n",
    "    out.loc[out[\"phrase_type\"].eq(\"reference\"), \"rank\"] = 0\n",
    "    out[\"rank\"] = out[\"rank\"].astype(int)\n",
    "\n",
    "    out = out.sort_values([\"phrase_num\", \"rank\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa3c002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ [1/58] Processing reference…\n",
      "→ [2/58] Processing paraphrase…\n",
      "→ [3/58] Processing paraphrase…\n",
      "→ [4/58] Processing paraphrase…\n",
      "→ [5/58] Processing paraphrase…\n",
      "→ [6/58] Processing paraphrase…\n",
      "→ [7/58] Processing paraphrase…\n",
      "→ [8/58] Processing paraphrase…\n",
      "→ [9/58] Processing paraphrase…\n",
      "→ [10/58] Processing paraphrase…\n",
      "→ [11/58] Processing paraphrase…\n",
      "→ [12/58] Processing paraphrase…\n",
      "→ [13/58] Processing paraphrase…\n",
      "→ [14/58] Processing paraphrase…\n",
      "→ [15/58] Processing paraphrase…\n",
      "→ [16/58] Processing paraphrase…\n",
      "→ [17/58] Processing paraphrase…\n",
      "→ [18/58] Processing paraphrase…\n",
      "→ [19/58] Processing paraphrase…\n",
      "→ [20/58] Processing paraphrase…\n",
      "→ [21/58] Processing paraphrase…\n",
      "→ [22/58] Processing paraphrase…\n",
      "→ [23/58] Processing paraphrase…\n",
      "→ [24/58] Processing paraphrase…\n",
      "→ [25/58] Processing paraphrase…\n",
      "→ [26/58] Processing paraphrase…\n",
      "→ [27/58] Processing paraphrase…\n",
      "→ [28/58] Processing paraphrase…\n",
      "→ [29/58] Processing paraphrase…\n",
      "→ [30/58] Processing paraphrase…\n",
      "→ [31/58] Processing paraphrase…\n",
      "→ [32/58] Processing paraphrase…\n",
      "→ [33/58] Processing paraphrase…\n",
      "→ [34/58] Processing paraphrase…\n",
      "→ [35/58] Processing paraphrase…\n",
      "→ [36/58] Processing paraphrase…\n",
      "→ [37/58] Processing paraphrase…\n",
      "→ [38/58] Processing paraphrase…\n",
      "→ [39/58] Processing paraphrase…\n",
      "→ [40/58] Processing paraphrase…\n",
      "→ [41/58] Processing paraphrase…\n",
      "→ [42/58] Processing paraphrase…\n",
      "→ [43/58] Processing paraphrase…\n",
      "→ [44/58] Processing paraphrase…\n",
      "→ [45/58] Processing paraphrase…\n",
      "→ [46/58] Processing paraphrase…\n",
      "→ [47/58] Processing paraphrase…\n",
      "→ [48/58] Processing paraphrase…\n",
      "→ [49/58] Processing paraphrase…\n",
      "→ [50/58] Processing paraphrase…\n",
      "→ [51/58] Processing paraphrase…\n",
      "→ [52/58] Processing paraphrase…\n",
      "→ [53/58] Processing paraphrase…\n",
      "→ [54/58] Processing paraphrase…\n",
      "→ [55/58] Processing paraphrase…\n",
      "→ [56/58] Processing paraphrase…\n",
      "→ [57/58] Processing paraphrase…\n",
      "→ [58/58] Processing paraphrase…\n",
      "→ [1/12] Processing reference…\n",
      "→ [2/12] Processing paraphrase…\n",
      "→ [3/12] Processing paraphrase…\n",
      "→ [4/12] Processing paraphrase…\n",
      "→ [5/12] Processing paraphrase…\n",
      "→ [6/12] Processing paraphrase…\n",
      "→ [7/12] Processing paraphrase…\n",
      "→ [8/12] Processing paraphrase…\n",
      "→ [9/12] Processing paraphrase…\n",
      "→ [10/12] Processing paraphrase…\n",
      "→ [11/12] Processing paraphrase…\n",
      "→ [12/12] Processing paraphrase…\n",
      "→ [1/28] Processing reference…\n",
      "→ [2/28] Processing paraphrase…\n",
      "→ [3/28] Processing paraphrase…\n",
      "→ [4/28] Processing paraphrase…\n",
      "→ [5/28] Processing paraphrase…\n",
      "→ [6/28] Processing paraphrase…\n",
      "→ [7/28] Processing paraphrase…\n",
      "→ [8/28] Processing paraphrase…\n",
      "→ [9/28] Processing paraphrase…\n",
      "→ [10/28] Processing paraphrase…\n",
      "→ [11/28] Processing paraphrase…\n",
      "→ [12/28] Processing paraphrase…\n",
      "→ [13/28] Processing paraphrase…\n",
      "→ [14/28] Processing paraphrase…\n",
      "→ [15/28] Processing paraphrase…\n",
      "→ [16/28] Processing paraphrase…\n",
      "→ [17/28] Processing paraphrase…\n",
      "→ [18/28] Processing paraphrase…\n",
      "→ [19/28] Processing paraphrase…\n",
      "→ [20/28] Processing paraphrase…\n",
      "→ [21/28] Processing paraphrase…\n",
      "→ [22/28] Processing paraphrase…\n",
      "→ [23/28] Processing paraphrase…\n",
      "→ [24/28] Processing paraphrase…\n",
      "→ [25/28] Processing paraphrase…\n",
      "→ [26/28] Processing paraphrase…\n",
      "→ [27/28] Processing paraphrase…\n",
      "→ [28/28] Processing paraphrase…\n",
      "→ [1/20] Processing reference…\n",
      "→ [2/20] Processing paraphrase…\n",
      "→ [3/20] Processing paraphrase…\n",
      "→ [4/20] Processing paraphrase…\n",
      "→ [5/20] Processing paraphrase…\n",
      "→ [6/20] Processing paraphrase…\n",
      "→ [7/20] Processing paraphrase…\n",
      "→ [8/20] Processing paraphrase…\n",
      "→ [9/20] Processing paraphrase…\n",
      "→ [10/20] Processing paraphrase…\n",
      "→ [11/20] Processing paraphrase…\n",
      "→ [12/20] Processing paraphrase…\n",
      "→ [13/20] Processing paraphrase…\n",
      "→ [14/20] Processing paraphrase…\n",
      "→ [15/20] Processing paraphrase…\n",
      "→ [16/20] Processing paraphrase…\n",
      "→ [17/20] Processing paraphrase…\n",
      "→ [18/20] Processing paraphrase…\n",
      "→ [19/20] Processing paraphrase…\n",
      "→ [20/20] Processing paraphrase…\n",
      "→ [1/33] Processing reference…\n",
      "→ [2/33] Processing paraphrase…\n",
      "→ [3/33] Processing paraphrase…\n",
      "→ [4/33] Processing paraphrase…\n",
      "→ [5/33] Processing paraphrase…\n",
      "→ [6/33] Processing paraphrase…\n",
      "→ [7/33] Processing paraphrase…\n",
      "→ [8/33] Processing paraphrase…\n",
      "→ [9/33] Processing paraphrase…\n",
      "→ [10/33] Processing paraphrase…\n",
      "→ [11/33] Processing paraphrase…\n",
      "→ [12/33] Processing paraphrase…\n",
      "→ [13/33] Processing paraphrase…\n",
      "→ [14/33] Processing paraphrase…\n",
      "→ [15/33] Processing paraphrase…\n",
      "→ [16/33] Processing paraphrase…\n",
      "→ [17/33] Processing paraphrase…\n",
      "→ [18/33] Processing paraphrase…\n",
      "→ [19/33] Processing paraphrase…\n",
      "→ [20/33] Processing paraphrase…\n",
      "→ [21/33] Processing paraphrase…\n",
      "→ [22/33] Processing paraphrase…\n",
      "→ [23/33] Processing paraphrase…\n",
      "→ [24/33] Processing paraphrase…\n",
      "→ [25/33] Processing paraphrase…\n",
      "→ [26/33] Processing paraphrase…\n",
      "→ [27/33] Processing paraphrase…\n",
      "→ [28/33] Processing paraphrase…\n",
      "→ [29/33] Processing paraphrase…\n",
      "→ [30/33] Processing paraphrase…\n",
      "→ [31/33] Processing paraphrase…\n",
      "→ [32/33] Processing paraphrase…\n",
      "→ [33/33] Processing paraphrase…\n",
      "→ [1/16] Processing reference…\n",
      "→ [2/16] Processing paraphrase…\n",
      "→ [3/16] Processing paraphrase…\n",
      "→ [4/16] Processing paraphrase…\n",
      "→ [5/16] Processing paraphrase…\n",
      "→ [6/16] Processing paraphrase…\n",
      "→ [7/16] Processing paraphrase…\n",
      "→ [8/16] Processing paraphrase…\n",
      "→ [9/16] Processing paraphrase…\n",
      "→ [10/16] Processing paraphrase…\n",
      "→ [11/16] Processing paraphrase…\n",
      "→ [12/16] Processing paraphrase…\n",
      "→ [13/16] Processing paraphrase…\n",
      "→ [14/16] Processing paraphrase…\n",
      "→ [15/16] Processing paraphrase…\n",
      "→ [16/16] Processing paraphrase…\n",
      "→ [1/4] Processing reference…\n",
      "→ [2/4] Processing paraphrase…\n",
      "→ [3/4] Processing paraphrase…\n",
      "→ [4/4] Processing paraphrase…\n",
      "→ [1/22] Processing reference…\n",
      "→ [2/22] Processing paraphrase…\n",
      "→ [3/22] Processing paraphrase…\n",
      "→ [4/22] Processing paraphrase…\n",
      "→ [5/22] Processing paraphrase…\n",
      "→ [6/22] Processing paraphrase…\n",
      "→ [7/22] Processing paraphrase…\n",
      "→ [8/22] Processing paraphrase…\n",
      "→ [9/22] Processing paraphrase…\n",
      "→ [10/22] Processing paraphrase…\n",
      "→ [11/22] Processing paraphrase…\n",
      "→ [12/22] Processing paraphrase…\n",
      "→ [13/22] Processing paraphrase…\n",
      "→ [14/22] Processing paraphrase…\n",
      "→ [15/22] Processing paraphrase…\n",
      "→ [16/22] Processing paraphrase…\n",
      "→ [17/22] Processing paraphrase…\n",
      "→ [18/22] Processing paraphrase…\n",
      "→ [19/22] Processing paraphrase…\n",
      "→ [20/22] Processing paraphrase…\n",
      "→ [21/22] Processing paraphrase…\n",
      "→ [22/22] Processing paraphrase…\n",
      "→ [1/7] Processing reference…\n",
      "→ [2/7] Processing paraphrase…\n",
      "→ [3/7] Processing paraphrase…\n",
      "→ [4/7] Processing paraphrase…\n",
      "→ [5/7] Processing paraphrase…\n",
      "→ [6/7] Processing paraphrase…\n",
      "→ [7/7] Processing paraphrase…\n",
      "→ [1/4] Processing reference…\n",
      "→ [2/4] Processing paraphrase…\n",
      "→ [3/4] Processing paraphrase…\n",
      "→ [4/4] Processing paraphrase…\n",
      "→ [1/31] Processing reference…\n",
      "→ [2/31] Processing paraphrase…\n",
      "→ [3/31] Processing paraphrase…\n",
      "→ [4/31] Processing paraphrase…\n",
      "→ [5/31] Processing paraphrase…\n",
      "→ [6/31] Processing paraphrase…\n",
      "→ [7/31] Processing paraphrase…\n",
      "→ [8/31] Processing paraphrase…\n",
      "→ [9/31] Processing paraphrase…\n",
      "→ [10/31] Processing paraphrase…\n",
      "→ [11/31] Processing paraphrase…\n",
      "→ [12/31] Processing paraphrase…\n",
      "→ [13/31] Processing paraphrase…\n",
      "→ [14/31] Processing paraphrase…\n",
      "→ [15/31] Processing paraphrase…\n",
      "→ [16/31] Processing paraphrase…\n",
      "→ [17/31] Processing paraphrase…\n",
      "→ [18/31] Processing paraphrase…\n",
      "→ [19/31] Processing paraphrase…\n",
      "→ [20/31] Processing paraphrase…\n",
      "→ [21/31] Processing paraphrase…\n",
      "→ [22/31] Processing paraphrase…\n",
      "→ [23/31] Processing paraphrase…\n",
      "→ [24/31] Processing paraphrase…\n",
      "→ [25/31] Processing paraphrase…\n",
      "→ [26/31] Processing paraphrase…\n",
      "→ [27/31] Processing paraphrase…\n",
      "→ [28/31] Processing paraphrase…\n",
      "→ [29/31] Processing paraphrase…\n",
      "→ [30/31] Processing paraphrase…\n",
      "→ [31/31] Processing paraphrase…\n",
      "→ [1/17] Processing reference…\n",
      "→ [2/17] Processing paraphrase…\n",
      "→ [3/17] Processing paraphrase…\n",
      "→ [4/17] Processing paraphrase…\n",
      "→ [5/17] Processing paraphrase…\n",
      "→ [6/17] Processing paraphrase…\n",
      "→ [7/17] Processing paraphrase…\n",
      "→ [8/17] Processing paraphrase…\n",
      "→ [9/17] Processing paraphrase…\n",
      "→ [10/17] Processing paraphrase…\n",
      "→ [11/17] Processing paraphrase…\n",
      "→ [12/17] Processing paraphrase…\n",
      "→ [13/17] Processing paraphrase…\n",
      "→ [14/17] Processing paraphrase…\n",
      "→ [15/17] Processing paraphrase…\n",
      "→ [16/17] Processing paraphrase…\n",
      "→ [17/17] Processing paraphrase…\n",
      "→ [1/52] Processing reference…\n",
      "→ [2/52] Processing paraphrase…\n",
      "→ [3/52] Processing paraphrase…\n",
      "→ [4/52] Processing paraphrase…\n",
      "→ [5/52] Processing paraphrase…\n",
      "→ [6/52] Processing paraphrase…\n",
      "→ [7/52] Processing paraphrase…\n",
      "→ [8/52] Processing paraphrase…\n",
      "→ [9/52] Processing paraphrase…\n",
      "→ [10/52] Processing paraphrase…\n",
      "→ [11/52] Processing paraphrase…\n",
      "→ [12/52] Processing paraphrase…\n",
      "→ [13/52] Processing paraphrase…\n",
      "→ [14/52] Processing paraphrase…\n",
      "→ [15/52] Processing paraphrase…\n",
      "→ [16/52] Processing paraphrase…\n",
      "→ [17/52] Processing paraphrase…\n",
      "→ [18/52] Processing paraphrase…\n",
      "→ [19/52] Processing paraphrase…\n",
      "→ [20/52] Processing paraphrase…\n",
      "→ [21/52] Processing paraphrase…\n",
      "→ [22/52] Processing paraphrase…\n",
      "→ [23/52] Processing paraphrase…\n",
      "→ [24/52] Processing paraphrase…\n",
      "→ [25/52] Processing paraphrase…\n",
      "→ [26/52] Processing paraphrase…\n",
      "→ [27/52] Processing paraphrase…\n",
      "→ [28/52] Processing paraphrase…\n",
      "→ [29/52] Processing paraphrase…\n",
      "→ [30/52] Processing paraphrase…\n",
      "→ [31/52] Processing paraphrase…\n",
      "→ [32/52] Processing paraphrase…\n",
      "→ [33/52] Processing paraphrase…\n",
      "→ [34/52] Processing paraphrase…\n",
      "→ [35/52] Processing paraphrase…\n",
      "→ [36/52] Processing paraphrase…\n",
      "→ [37/52] Processing paraphrase…\n",
      "→ [38/52] Processing paraphrase…\n",
      "→ [39/52] Processing paraphrase…\n",
      "→ [40/52] Processing paraphrase…\n",
      "→ [41/52] Processing paraphrase…\n",
      "→ [42/52] Processing paraphrase…\n",
      "→ [43/52] Processing paraphrase…\n",
      "→ [44/52] Processing paraphrase…\n",
      "→ [45/52] Processing paraphrase…\n",
      "→ [46/52] Processing paraphrase…\n",
      "→ [47/52] Processing paraphrase…\n",
      "→ [48/52] Processing paraphrase…\n",
      "→ [49/52] Processing paraphrase…\n",
      "→ [50/52] Processing paraphrase…\n",
      "→ [51/52] Processing paraphrase…\n",
      "→ [52/52] Processing paraphrase…\n",
      "→ [1/10] Processing reference…\n",
      "→ [2/10] Processing paraphrase…\n",
      "→ [3/10] Processing paraphrase…\n",
      "→ [4/10] Processing paraphrase…\n",
      "→ [5/10] Processing paraphrase…\n",
      "→ [6/10] Processing paraphrase…\n",
      "→ [7/10] Processing paraphrase…\n",
      "→ [8/10] Processing paraphrase…\n",
      "→ [9/10] Processing paraphrase…\n",
      "→ [10/10] Processing paraphrase…\n",
      "→ [1/11] Processing reference…\n",
      "→ [2/11] Processing paraphrase…\n",
      "→ [3/11] Processing paraphrase…\n",
      "→ [4/11] Processing paraphrase…\n",
      "→ [5/11] Processing paraphrase…\n",
      "→ [6/11] Processing paraphrase…\n",
      "→ [7/11] Processing paraphrase…\n",
      "→ [8/11] Processing paraphrase…\n",
      "→ [9/11] Processing paraphrase…\n",
      "→ [10/11] Processing paraphrase…\n",
      "→ [11/11] Processing paraphrase…\n",
      "→ [1/4] Processing reference…\n",
      "→ [2/4] Processing paraphrase…\n",
      "→ [3/4] Processing paraphrase…\n",
      "→ [4/4] Processing paraphrase…\n",
      "→ [1/8] Processing reference…\n",
      "→ [2/8] Processing paraphrase…\n",
      "→ [3/8] Processing paraphrase…\n",
      "→ [4/8] Processing paraphrase…\n",
      "→ [5/8] Processing paraphrase…\n",
      "→ [6/8] Processing paraphrase…\n",
      "→ [7/8] Processing paraphrase…\n",
      "→ [8/8] Processing paraphrase…\n",
      "→ [1/8] Processing reference…\n",
      "→ [2/8] Processing paraphrase…\n",
      "→ [3/8] Processing paraphrase…\n",
      "→ [4/8] Processing paraphrase…\n",
      "→ [5/8] Processing paraphrase…\n",
      "→ [6/8] Processing paraphrase…\n",
      "→ [7/8] Processing paraphrase…\n",
      "→ [8/8] Processing paraphrase…\n"
     ]
    }
   ],
   "source": [
    "score_df_no_context = get_scored_df_no_context(n_gram_dict, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374ff5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_phrases = score_df_no_context[['phrase_num', 'original_phrase']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c603218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove illegal control chars (keep \\t, \\n, \\r)\n",
    "_ILLEGAL_RE = re.compile(r\"[\\x00-\\x08\\x0B-\\x0C\\x0E-\\x1F]\")\n",
    "\n",
    "def _clean_cell(x):\n",
    "    if isinstance(x, str):\n",
    "        return _ILLEGAL_RE.sub(\"\", x)\n",
    "    return x\n",
    "\n",
    "def clean_for_excel(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    obj_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    df[obj_cols] = df[obj_cols].applymap(_clean_cell)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfbe8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-84>:15: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "<positron-console-cell-84>:15: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "<positron-console-cell-84>:15: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "<positron-console-cell-84>:15: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "<positron-console-cell-84>:15: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "<positron-console-cell-84>:15: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n"
     ]
    }
   ],
   "source": [
    "save_loc = f\"{nas_base_loc}/paraphrase examples/{specific_problem}.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(save_loc, engine=\"openpyxl\") as xls:\n",
    "    clean_for_excel(docs_df).to_excel(xls, sheet_name=\"docs\", index=False)\n",
    "    clean_for_excel(p_metadata).to_excel(xls, sheet_name=\"metadata\", index=False)\n",
    "    clean_for_excel(score_df_no_context).to_excel(xls, sheet_name=\"no context\", index=False)\n",
    "    clean_for_excel(known_scored).to_excel(xls, sheet_name=\"known\", index=False)\n",
    "    clean_for_excel(unknown_scored).to_excel(xls, sheet_name=\"unknown\", index=False)\n",
    "    clean_for_excel(distinct_phrases).to_excel(xls, sheet_name=\"LLR\", index=False)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
