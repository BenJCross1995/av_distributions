{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "64ef1ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "711d8229",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_loc = '/Volumes/BCross/paraphrase examples slurm/Wiki-test/hodja_nasreddin_text_1 vs hodja_nasreddin_text_3.xlsx'\n",
    "phrase_loc = '/Volumes/BCross/paraphrase examples slurm/wiki-phrase-list-reviewed.xlsx'\n",
    "\n",
    "known = pd.read_excel(doc_loc, sheet_name=\"known\")\n",
    "unknown = pd.read_excel(doc_loc, sheet_name=\"unknown\")\n",
    "no_context = pd.read_excel(doc_loc, sheet_name=\"no context\")\n",
    "metadata = pd.read_excel(doc_loc, sheet_name=\"metadata\")\n",
    "\n",
    "phrase_list = pd.read_excel(phrase_loc)\n",
    "phrases_to_keep = phrase_list[phrase_list['keep_phrase'] == 1].copy()\n",
    "\n",
    "# Convert the stringified tuples into actual tuples, then into lists\n",
    "phrases_to_keep['tokens'] = phrases_to_keep['tokens'].apply(lambda x: list(ast.literal_eval(x)) if isinstance(x, str) else list(x))\n",
    "phrases_to_keep = phrases_to_keep[['phrase']]\n",
    "        \n",
    "reference_phrases = no_context[no_context['phrase_type'] == 'reference'].copy()\n",
    "\n",
    "# Perform the merge using the tuple-based key\n",
    "merged_phrases = pd.merge(reference_phrases, phrases_to_keep, on='phrase', how='inner')\n",
    "merged_phrases = merged_phrases[['phrase_num']]\n",
    "\n",
    "no_context = pd.merge(no_context, merged_phrases, on='phrase_num', how='inner')\n",
    "known = pd.merge(known, merged_phrases, on='phrase_num', how='inner')\n",
    "unknown= pd.merge(unknown, merged_phrases, on='phrase_num', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b025f951",
   "metadata": {},
   "source": [
    "### Create the base LLR table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b6646ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['phrase_num', 'phrase_occurence', 'original_phrase']\n",
    "\n",
    "llr_base = (\n",
    "    pd.concat([known[cols], unknown[cols]], ignore_index=True)\n",
    "      .drop_duplicates()\n",
    "      .sort_values(cols, ascending=[True, True, True])  # explicit\n",
    "      .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb28019",
   "metadata": {},
   "source": [
    "### Get phrase statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9bf78bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) No context phrase stats\n",
    "no_context_phrase_stats = (\n",
    "    no_context\n",
    "    .assign(ref_raw_prob=no_context['raw_prob']\n",
    "        .where(no_context['phrase_type'].eq('reference')))\n",
    "    .groupby('phrase_num', dropna=False)\n",
    "    .agg(\n",
    "        num_phrases=('phrase_num', 'size'),\n",
    "        sum_raw_prob=('raw_prob', 'sum'),\n",
    "        reference_prob=('ref_raw_prob', 'max')\n",
    "    )\n",
    "    .assign(\n",
    "        phrases_kept=lambda d: d['num_phrases'],\n",
    "        pmf_no_context=lambda d: d['reference_prob'].div(d['sum_raw_prob']),\n",
    "        llr_no_context=lambda d: np.where(d['pmf_no_context'] > 0, -np.log10(d['pmf_no_context']), 0.0)\n",
    "    )\n",
    "    .drop(columns=['sum_raw_prob', 'reference_prob'])\n",
    ")\n",
    "\n",
    "# 2) Known phrase stats\n",
    "known_phrase_stats = (\n",
    "    known\n",
    "    .assign(ref_raw_prob=known['raw_prob']\n",
    "        .where(known['phrase_type'].eq('reference')))\n",
    "    .groupby(['phrase_num', 'phrase_occurence'], dropna=False)\n",
    "    .agg(\n",
    "        sum_raw_prob=('raw_prob', 'sum'),\n",
    "        reference_prob=('ref_raw_prob', 'max')\n",
    "    )\n",
    "    .assign(\n",
    "        pmf_known=lambda d: d['reference_prob'].div(d['sum_raw_prob']),\n",
    "        llr_known=lambda d: np.where(d['pmf_known'] > 0, -np.log10(d['pmf_known']), 0.0)\n",
    "    )\n",
    "    .drop(columns=['sum_raw_prob', 'reference_prob'])\n",
    ")\n",
    "\n",
    "# 3) Unknown phrase stats\n",
    "unknown_phrase_stats = (\n",
    "    unknown\n",
    "      .assign(ref_raw_prob=unknown['raw_prob']\n",
    "              .where(unknown['phrase_type'].eq('reference')))\n",
    "      .groupby(['phrase_num', 'phrase_occurence'], dropna=False)\n",
    "      .agg(\n",
    "          sum_raw_prob=('raw_prob', 'sum'),\n",
    "          reference_prob=('ref_raw_prob', 'max')\n",
    "      )\n",
    "      .assign(\n",
    "          pmf_unknown=lambda d: d['reference_prob'].div(d['sum_raw_prob']),\n",
    "          llr_unknown=lambda d: np.where(d['pmf_unknown'] > 0, -np.log10(d['pmf_unknown']), 0.0)\n",
    "      )\n",
    "      .drop(columns=['sum_raw_prob', 'reference_prob'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7a47adf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join\n",
    "LLR = (\n",
    "    llr_base\n",
    "      .assign(\n",
    "          phrase_num=llr_base['phrase_num'].astype('string'),\n",
    "          phrase_occurence=pd.to_numeric(llr_base['phrase_occurence'], errors='coerce').astype('Int64')\n",
    "      )\n",
    "      .join(no_context_phrase_stats, on='phrase_num', how='left')\n",
    "      .join(known_phrase_stats, on=['phrase_num','phrase_occurence'], how='left')\n",
    "      .join(unknown_phrase_stats, on=['phrase_num','phrase_occurence'], how='left')\n",
    ")\n",
    "\n",
    "LLR = LLR[['phrase_num', 'phrase_occurence', 'original_phrase', 'num_phrases', 'phrases_kept',\n",
    "           'pmf_no_context', 'pmf_known', 'pmf_unknown', 'llr_no_context', 'llr_known', 'llr_unknown']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ca16e19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase_num</th>\n",
       "      <th>phrase_occurence</th>\n",
       "      <th>original_phrase</th>\n",
       "      <th>num_phrases</th>\n",
       "      <th>phrases_kept</th>\n",
       "      <th>pmf_no_context</th>\n",
       "      <th>pmf_known</th>\n",
       "      <th>pmf_unknown</th>\n",
       "      <th>llr_no_context</th>\n",
       "      <th>llr_known</th>\n",
       "      <th>llr_unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phrase_01</td>\n",
       "      <td>1</td>\n",
       "      <td>, this is not</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>0.488617</td>\n",
       "      <td>0.284798</td>\n",
       "      <td>0.314008</td>\n",
       "      <td>0.311031</td>\n",
       "      <td>0.545463</td>\n",
       "      <td>0.503060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phrase_02</td>\n",
       "      <td>1</td>\n",
       "      <td>, but this</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>0.012607</td>\n",
       "      <td>0.020277</td>\n",
       "      <td>0.024125</td>\n",
       "      <td>1.899381</td>\n",
       "      <td>1.693002</td>\n",
       "      <td>1.617525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phrase_02</td>\n",
       "      <td>2</td>\n",
       "      <td>, but this</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>0.012607</td>\n",
       "      <td>0.030038</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.899381</td>\n",
       "      <td>1.522332</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phrase_03</td>\n",
       "      <td>1</td>\n",
       "      <td>, you are</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>0.716338</td>\n",
       "      <td>0.914467</td>\n",
       "      <td>0.915033</td>\n",
       "      <td>0.144882</td>\n",
       "      <td>0.038832</td>\n",
       "      <td>0.038563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phrase_04</td>\n",
       "      <td>1</td>\n",
       "      <td>do not have</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>0.614058</td>\n",
       "      <td>0.291087</td>\n",
       "      <td>3.135517</td>\n",
       "      <td>0.211790</td>\n",
       "      <td>0.535977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>phrase_05</td>\n",
       "      <td>1</td>\n",
       "      <td>one of the</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0.881184</td>\n",
       "      <td>0.970453</td>\n",
       "      <td>0.959643</td>\n",
       "      <td>0.054933</td>\n",
       "      <td>0.013026</td>\n",
       "      <td>0.017890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>phrase_06</td>\n",
       "      <td>1</td>\n",
       "      <td>welcome to improve</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>0.254981</td>\n",
       "      <td>0.083369</td>\n",
       "      <td>0.032135</td>\n",
       "      <td>0.593492</td>\n",
       "      <td>1.078997</td>\n",
       "      <td>1.493017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>phrase_07</td>\n",
       "      <td>1</td>\n",
       "      <td>you do not</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.059834</td>\n",
       "      <td>0.217250</td>\n",
       "      <td>0.377068</td>\n",
       "      <td>1.223055</td>\n",
       "      <td>0.663041</td>\n",
       "      <td>0.423580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>phrase_08</td>\n",
       "      <td>1</td>\n",
       "      <td>about this</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>0.894526</td>\n",
       "      <td>0.662013</td>\n",
       "      <td>0.058029</td>\n",
       "      <td>0.048407</td>\n",
       "      <td>0.179133</td>\n",
       "      <td>1.236355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>phrase_09</td>\n",
       "      <td>1</td>\n",
       "      <td>articles on</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>0.033955</td>\n",
       "      <td>0.397028</td>\n",
       "      <td>0.425706</td>\n",
       "      <td>1.469096</td>\n",
       "      <td>0.401179</td>\n",
       "      <td>0.370891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>phrase_10</td>\n",
       "      <td>1</td>\n",
       "      <td>because the</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>0.015668</td>\n",
       "      <td>0.014886</td>\n",
       "      <td>0.216434</td>\n",
       "      <td>1.804979</td>\n",
       "      <td>1.827233</td>\n",
       "      <td>0.664674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>phrase_10</td>\n",
       "      <td>2</td>\n",
       "      <td>because the</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>0.015668</td>\n",
       "      <td>0.931699</td>\n",
       "      <td>0.301239</td>\n",
       "      <td>1.804979</td>\n",
       "      <td>0.030724</td>\n",
       "      <td>0.521089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>phrase_11</td>\n",
       "      <td>1</td>\n",
       "      <td>the subject</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>0.004709</td>\n",
       "      <td>0.082468</td>\n",
       "      <td>0.032023</td>\n",
       "      <td>2.327088</td>\n",
       "      <td>1.083715</td>\n",
       "      <td>1.494538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>phrase_12</td>\n",
       "      <td>1</td>\n",
       "      <td>this article</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>0.011344</td>\n",
       "      <td>0.073519</td>\n",
       "      <td>0.142304</td>\n",
       "      <td>1.945226</td>\n",
       "      <td>1.133602</td>\n",
       "      <td>0.846783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   phrase_num  phrase_occurence  ... llr_known  llr_unknown\n",
       "0   phrase_01                 1  ...  0.545463     0.503060\n",
       "1   phrase_02                 1  ...  1.693002     1.617525\n",
       "2   phrase_02                 2  ...  1.522332          NaN\n",
       "3   phrase_03                 1  ...  0.038832     0.038563\n",
       "4   phrase_04                 1  ...  0.211790     0.535977\n",
       "5   phrase_05                 1  ...  0.013026     0.017890\n",
       "6   phrase_06                 1  ...  1.078997     1.493017\n",
       "7   phrase_07                 1  ...  0.663041     0.423580\n",
       "8   phrase_08                 1  ...  0.179133     1.236355\n",
       "9   phrase_09                 1  ...  0.401179     0.370891\n",
       "10  phrase_10                 1  ...  1.827233     0.664674\n",
       "11  phrase_10                 2  ...  0.030724     0.521089\n",
       "12  phrase_11                 1  ...  1.083715     1.494538\n",
       "13  phrase_12                 1  ...  1.133602     0.846783\n",
       "\n",
       "[14 rows x 11 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "97ec58e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLR_summary = pd.DataFrame([{\n",
    "    'num_phrases': LLR['phrase_num'].nunique(),\n",
    "    'phrases_kept': LLR.loc[LLR['phrases_kept'] > 0, 'phrase_num'].nunique(),\n",
    "    'llr_no_context': LLR['llr_no_context'].sum(skipna=True),\n",
    "    'llr_known': LLR['llr_known'].sum(skipna=True),\n",
    "    'llr_unknown': LLR['llr_unknown'].sum(skipna=True),\n",
    "}])\n",
    "\n",
    "LLR_summary = LLR_summary.assign(\n",
    "    normalised_llr_no_context=lambda d: d['llr_no_context'] / d['phrases_kept'],\n",
    "    normalised_llr_known=lambda d: d['llr_known'] / d['phrases_kept'],\n",
    "    normalised_llr_unknown=lambda d: d['llr_unknown'] / d['phrases_kept']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3592a0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Drop any overlapping columns in metadata\n",
    "overlapping_cols = LLR_summary.columns.intersection(metadata.columns)\n",
    "metadata = metadata.drop(columns=overlapping_cols, errors='ignore')\n",
    "\n",
    "# 4. Concatenate new values\n",
    "metadata = pd.concat([metadata, LLR_summary], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "45848249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>problem</th>\n",
       "      <th>corpus</th>\n",
       "      <th>known_author</th>\n",
       "      <th>unknown_author</th>\n",
       "      <th>unknown_doc_id</th>\n",
       "      <th>known_doc_id</th>\n",
       "      <th>target</th>\n",
       "      <th>num_phrases</th>\n",
       "      <th>phrases_kept</th>\n",
       "      <th>llr_no_context</th>\n",
       "      <th>llr_known</th>\n",
       "      <th>llr_unknown</th>\n",
       "      <th>normalised_llr_no_context</th>\n",
       "      <th>normalised_llr_known</th>\n",
       "      <th>normalised_llr_unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Hodja_Nasreddin vs Hodja_Nasreddin</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>Hodja_Nasreddin</td>\n",
       "      <td>Hodja_Nasreddin</td>\n",
       "      <td>hodja_nasreddin_text_3</td>\n",
       "      <td>hodja_nasreddin_text_1</td>\n",
       "      <td>True</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>18.661448</td>\n",
       "      <td>10.42207</td>\n",
       "      <td>9.763942</td>\n",
       "      <td>1.555121</td>\n",
       "      <td>0.868506</td>\n",
       "      <td>0.813662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  sample_id  ... normalised_llr_known normalised_llr_unknown\n",
       "0      0          1  ...             0.868506               0.813662\n",
       "\n",
       "[1 rows x 17 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dff4f503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_doc_pipeline(doc_loc, write_excel=True, save_dir=None, phrase_loc=None):\n",
    "    \"\"\"Pipeline to manually get the results from a document\"\"\"\n",
    "    \n",
    "    doc_name = os.path.basename(doc_loc)\n",
    "    \n",
    "    print(f\"Processing Document: {doc_name}\")\n",
    "    # Read the sheets as dataframes\n",
    "    docs = pd.read_excel(doc_loc, sheet_name=\"docs\")\n",
    "    known = pd.read_excel(doc_loc, sheet_name=\"known\")\n",
    "    unknown = pd.read_excel(doc_loc, sheet_name=\"unknown\")\n",
    "    no_context = pd.read_excel(doc_loc, sheet_name=\"no context\")\n",
    "    metadata = pd.read_excel(doc_loc, sheet_name=\"metadata\")\n",
    "    \n",
    "    # Get phrases to keep\n",
    "    if phrase_loc:\n",
    "        phrase_list = pd.read_excel(phrase_loc)\n",
    "        phrases_to_keep = phrase_list[phrase_list['keep_phrase'] == 1].copy()\n",
    "\n",
    "        # Convert the stringified tuples into actual tuples, then into lists\n",
    "        phrases_to_keep['tokens'] = phrases_to_keep['tokens'].apply(lambda x: list(ast.literal_eval(x)) if isinstance(x, str) else list(x))\n",
    "        phrases_to_keep = phrases_to_keep[['phrase']]\n",
    "        \n",
    "        reference_phrases = no_context[no_context['phrase_type'] == 'reference'].copy()\n",
    "\n",
    "        # Perform the merge using the tuple-based key\n",
    "        merged_phrases = pd.merge(reference_phrases, phrases_to_keep, on='phrase', how='inner')\n",
    "        merged_phrases = merged_phrases[['phrase_num']]\n",
    "\n",
    "        no_context = pd.merge(no_context, merged_phrases, on='phrase_num', how='inner')\n",
    "        known = pd.merge(known, merged_phrases, on='phrase_num', how='inner')\n",
    "        unknown= pd.merge(unknown, merged_phrases, on='phrase_num', how='inner')\n",
    "        \n",
    "    # Get the base LLR table\n",
    "    cols = ['phrase_num', 'phrase_occurence', 'original_phrase']\n",
    "    llr_base = (\n",
    "        pd.concat([known[cols], unknown[cols]], ignore_index=True)\n",
    "        .drop_duplicates()\n",
    "        .sort_values(cols, ascending=[True, True, True])  # explicit\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    # Now get the phrase statistics\n",
    "    # 1) No context phrase stats\n",
    "    no_context_phrase_stats = (\n",
    "        no_context\n",
    "        .assign(ref_raw_prob=no_context['raw_prob']\n",
    "            .where(no_context['phrase_type'].eq('reference')))\n",
    "        .groupby('phrase_num', dropna=False)\n",
    "        .agg(\n",
    "            num_phrases=('phrase_num', 'size'),\n",
    "            sum_raw_prob=('raw_prob', 'sum'),\n",
    "            reference_prob=('ref_raw_prob', 'max')\n",
    "        )\n",
    "        .assign(\n",
    "            phrases_kept=lambda d: d['num_phrases'],\n",
    "            pmf_no_context=lambda d: d['reference_prob'].div(d['sum_raw_prob']),\n",
    "            llr_no_context=lambda d: np.where(d['pmf_no_context'] > 0, -np.log10(d['pmf_no_context']), 0.0)\n",
    "        )\n",
    "        .drop(columns=['sum_raw_prob', 'reference_prob'])\n",
    "    )\n",
    "\n",
    "    # 2) Known phrase stats\n",
    "    known_phrase_stats = (\n",
    "        known\n",
    "        .assign(ref_raw_prob=known['raw_prob']\n",
    "            .where(known['phrase_type'].eq('reference')))\n",
    "        .groupby(['phrase_num', 'phrase_occurence'], dropna=False)\n",
    "        .agg(\n",
    "            sum_raw_prob=('raw_prob', 'sum'),\n",
    "            reference_prob=('ref_raw_prob', 'max')\n",
    "        )\n",
    "        .assign(\n",
    "            pmf_known=lambda d: d['reference_prob'].div(d['sum_raw_prob']),\n",
    "            llr_known=lambda d: np.where(d['pmf_known'] > 0, -np.log10(d['pmf_known']), 0.0)\n",
    "        )\n",
    "        .drop(columns=['sum_raw_prob', 'reference_prob'])\n",
    "    )\n",
    "\n",
    "    # 3) Unknown phrase stats\n",
    "    unknown_phrase_stats = (\n",
    "        unknown\n",
    "        .assign(ref_raw_prob=unknown['raw_prob']\n",
    "                .where(unknown['phrase_type'].eq('reference')))\n",
    "        .groupby(['phrase_num', 'phrase_occurence'], dropna=False)\n",
    "        .agg(\n",
    "            sum_raw_prob=('raw_prob', 'sum'),\n",
    "            reference_prob=('ref_raw_prob', 'max')\n",
    "        )\n",
    "        .assign(\n",
    "            pmf_unknown=lambda d: d['reference_prob'].div(d['sum_raw_prob']),\n",
    "            llr_unknown=lambda d: np.where(d['pmf_unknown'] > 0, -np.log10(d['pmf_unknown']), 0.0)\n",
    "        )\n",
    "        .drop(columns=['sum_raw_prob', 'reference_prob'])\n",
    "    )\n",
    "    \n",
    "    # Create final LLR table\n",
    "    LLR = (\n",
    "        llr_base\n",
    "        .assign(\n",
    "            phrase_num=llr_base['phrase_num'].astype('string'),\n",
    "            phrase_occurence=pd.to_numeric(llr_base['phrase_occurence'], errors='coerce').astype('Int64')\n",
    "        )\n",
    "        .join(no_context_phrase_stats, on='phrase_num', how='left')\n",
    "        .join(known_phrase_stats, on=['phrase_num','phrase_occurence'], how='left').fillna(0)\n",
    "        .join(unknown_phrase_stats, on=['phrase_num','phrase_occurence'], how='left').fillna(0)\n",
    "    )\n",
    "\n",
    "    LLR = LLR[['phrase_num', 'phrase_occurence', 'original_phrase', 'num_phrases', 'phrases_kept',\n",
    "            'pmf_no_context', 'pmf_known', 'pmf_unknown', 'llr_no_context', 'llr_known', 'llr_unknown']]\n",
    "    \n",
    "    # Summarise the LLR table for the metadata\n",
    "    LLR_summary = pd.DataFrame([{\n",
    "    'num_phrases': LLR['phrase_num'].nunique(),\n",
    "    'phrases_kept': LLR.loc[LLR['phrases_kept'] > 0, 'phrase_num'].nunique(),\n",
    "    'llr_no_context': LLR['llr_no_context'].sum(skipna=True),\n",
    "    'llr_known': LLR['llr_known'].sum(skipna=True),\n",
    "    'llr_unknown': LLR['llr_unknown'].sum(skipna=True),\n",
    "    }])\n",
    "\n",
    "    LLR_summary = LLR_summary.assign(\n",
    "        normalised_llr_no_context=lambda d: d['llr_no_context'] / d['phrases_kept'],\n",
    "        normalised_llr_known=lambda d: d['llr_known'] / d['phrases_kept'],\n",
    "        normalised_llr_unknown=lambda d: d['llr_unknown'] / d['phrases_kept']\n",
    "    )\n",
    "    \n",
    "    # Create final metadata table\n",
    "    # 1. Drop any overlapping columns in metadata\n",
    "    overlapping_cols = LLR_summary.columns.intersection(metadata.columns)\n",
    "    metadata_final = metadata.drop(columns=overlapping_cols, errors='ignore')\n",
    "\n",
    "    # 2. Concatenate new values\n",
    "    metadata_final = pd.concat([metadata_final, LLR_summary], axis=1)\n",
    "    \n",
    "    if write_excel:\n",
    "        \n",
    "        print(\"Writing file\")\n",
    "        path = Path(save_dir + '/' + doc_name)\n",
    "        \n",
    "        # Choose writer mode safely\n",
    "        writer_mode = \"a\" if path.exists() else \"w\"\n",
    "        writer_kwargs = {\"engine\": \"openpyxl\", \"mode\": writer_mode}\n",
    "        if writer_mode == \"a\":\n",
    "            writer_kwargs[\"if_sheet_exists\"] = \"replace\"  # only valid in append mode\n",
    "        \n",
    "\n",
    "        with pd.ExcelWriter(path, **writer_kwargs) as writer:\n",
    "            # Write sheets\n",
    "            docs.to_excel(writer, index=False, sheet_name=\"docs\")\n",
    "            known.to_excel(writer, index=False, sheet_name=\"known\")\n",
    "            unknown.to_excel(writer, index=False, sheet_name=\"unknown\")\n",
    "            no_context.to_excel(writer, index=False, sheet_name=\"no context\")\n",
    "            LLR.to_excel(writer, index=False, sheet_name=\"LLR\")\n",
    "            metadata_final.to_excel(writer, index=False, sheet_name=\"metadata\")\n",
    "            \n",
    "            # wb = writer.book\n",
    "            # wb._sheets = [\"docs\", \"metadata\", \"no context\", \"known\", \"unknown\", \"LLR\"]\n",
    "    \n",
    "    return metadata_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9ce25a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completing file 1 out of 186\n",
      "Processing Document: honestopl_text_3 vs hootmag_text_13.xlsx\n",
      "Writing file\n",
      "Completing file 2 out of 186\n",
      "Processing Document: hodja_nasreddin_text_1 vs honestopl_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 3 out of 186\n",
      "Processing Document: honestopl_text_3 vs honestopl_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 4 out of 186\n",
      "Processing Document: hodja_nasreddin_text_11 vs honestopl_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 5 out of 186\n",
      "Processing Document: honestopl_text_4 vs honestopl_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 6 out of 186\n",
      "Processing Document: honestopl_text_4 vs hootmag_text_13.xlsx\n",
      "Writing file\n",
      "Completing file 7 out of 186\n",
      "Processing Document: honestopl_text_5 vs honestopl_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 8 out of 186\n",
      "Processing Document: honestopl_text_5 vs hootmag_text_13.xlsx\n",
      "Writing file\n",
      "Completing file 9 out of 186\n",
      "Processing Document: hootmag_text_1 vs hootmag_text_13.xlsx\n",
      "Writing file\n",
      "Completing file 10 out of 186\n",
      "Processing Document: hootmag_text_1 vs iain99_text_5.xlsx\n",
      "Writing file\n",
      "Completing file 11 out of 186\n",
      "Processing Document: hootmag_text_10 vs hootmag_text_13.xlsx\n",
      "Writing file\n",
      "Completing file 12 out of 186\n",
      "Processing Document: hootmag_text_10 vs iain99_text_5.xlsx\n",
      "Writing file\n",
      "Completing file 13 out of 186\n",
      "Processing Document: icarus3_text_3 vs icarus3_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 14 out of 186\n",
      "Processing Document: irvine22_text_1 vs irvine22_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 15 out of 186\n",
      "Processing Document: irvine22_text_4 vs irvine22_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 16 out of 186\n",
      "Processing Document: itub_text_3 vs ivoshandor_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 17 out of 186\n",
      "Processing Document: ivoshandor_text_2 vs jasper_deng_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 18 out of 186\n",
      "Processing Document: hodja_nasreddin_text_1 vs hodja_nasreddin_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 19 out of 186\n",
      "Processing Document: hodja_nasreddin_text_10 vs hodja_nasreddin_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 20 out of 186\n",
      "Processing Document: hootmag_text_12 vs hootmag_text_13.xlsx\n",
      "Writing file\n",
      "Completing file 21 out of 186\n",
      "Processing Document: hodja_nasreddin_text_10 vs honestopl_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 22 out of 186\n",
      "Processing Document: hodja_nasreddin_text_11 vs hodja_nasreddin_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 23 out of 186\n",
      "Processing Document: hootmag_text_12 vs iain99_text_5.xlsx\n",
      "Writing file\n",
      "Completing file 24 out of 186\n",
      "Processing Document: intothefire_text_1 vs irvine22_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 25 out of 186\n",
      "Processing Document: intothefire_text_2 vs intothefire_text_12.xlsx\n",
      "Writing file\n",
      "Completing file 26 out of 186\n",
      "Processing Document: intothefire_text_2 vs irvine22_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 27 out of 186\n",
      "Processing Document: intothefire_text_10 vs intothefire_text_12.xlsx\n",
      "Writing file\n",
      "Completing file 28 out of 186\n",
      "Processing Document: intothefire_text_10 vs irvine22_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 29 out of 186\n",
      "Processing Document: irvine22_text_2 vs irvine22_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 30 out of 186\n",
      "Processing Document: itub_text_3 vs itub_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 31 out of 186\n",
      "Processing Document: ivoshandor_text_5 vs jasper_deng_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 32 out of 186\n",
      "Processing Document: ivoshandor_text_2 vs ivoshandor_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 33 out of 186\n",
      "Processing Document: ivoshandor_text_4 vs jasper_deng_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 34 out of 186\n",
      "Processing Document: jasper_deng_text_1 vs jbmurray_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 35 out of 186\n",
      "Processing Document: jasper_deng_text_3 vs jasper_deng_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 36 out of 186\n",
      "Processing Document: jc37_text_10 vs jeffrey_vernon_merkey_text_10.xlsx\n",
      "Writing file\n",
      "Completing file 37 out of 186\n",
      "Processing Document: iain99_text_2 vs iain99_text_5.xlsx\n",
      "Writing file\n",
      "Completing file 38 out of 186\n",
      "Processing Document: iain99_text_3 vs iain99_text_5.xlsx\n",
      "Writing file\n",
      "Completing file 39 out of 186\n",
      "Processing Document: iain99_text_1 vs iain99_text_5.xlsx\n",
      "Writing file\n",
      "Completing file 40 out of 186\n",
      "Processing Document: iain99_text_2 vs icarus3_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 41 out of 186\n",
      "Processing Document: iain99_text_1 vs icarus3_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 42 out of 186\n",
      "Processing Document: iain99_text_3 vs icarus3_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 43 out of 186\n",
      "Processing Document: icarus3_text_1 vs icarus3_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 44 out of 186\n",
      "Processing Document: icarus3_text_1 vs intangible_text_2.xlsx\n",
      "Writing file\n",
      "Completing file 45 out of 186\n",
      "Processing Document: icarus3_text_2 vs icarus3_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 46 out of 186\n",
      "Processing Document: icarus3_text_2 vs intangible_text_2.xlsx\n",
      "Writing file\n",
      "Completing file 47 out of 186\n",
      "Processing Document: icarus3_text_3 vs intangible_text_2.xlsx\n",
      "Writing file\n",
      "Completing file 48 out of 186\n",
      "Processing Document: intangible_text_1 vs intangible_text_2.xlsx\n",
      "Writing file\n",
      "Completing file 49 out of 186\n",
      "Processing Document: intangible_text_1 vs intothefire_text_12.xlsx\n",
      "Writing file\n",
      "Completing file 50 out of 186\n",
      "Processing Document: intangible_text_3 vs intangible_text_2.xlsx\n",
      "Writing file\n",
      "Completing file 51 out of 186\n",
      "Processing Document: intangible_text_3 vs intothefire_text_12.xlsx\n",
      "Writing file\n",
      "Completing file 52 out of 186\n",
      "Processing Document: intangible_text_5 vs intangible_text_2.xlsx\n",
      "Writing file\n",
      "Completing file 53 out of 186\n",
      "Processing Document: intangible_text_5 vs intothefire_text_12.xlsx\n",
      "Writing file\n",
      "Completing file 54 out of 186\n",
      "Processing Document: intothefire_text_1 vs intothefire_text_12.xlsx\n",
      "Writing file\n",
      "Completing file 55 out of 186\n",
      "Processing Document: irvine22_text_1 vs itub_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 56 out of 186\n",
      "Processing Document: irvine22_text_2 vs itub_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 57 out of 186\n",
      "Processing Document: irvine22_text_4 vs itub_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 58 out of 186\n",
      "Processing Document: itub_text_2 vs itub_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 59 out of 186\n",
      "Processing Document: itub_text_2 vs ivoshandor_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 60 out of 186\n",
      "Processing Document: itub_text_5 vs itub_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 61 out of 186\n",
      "Processing Document: itub_text_5 vs ivoshandor_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 62 out of 186\n",
      "Processing Document: ivoshandor_text_4 vs ivoshandor_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 63 out of 186\n",
      "Processing Document: ivoshandor_text_5 vs ivoshandor_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 64 out of 186\n",
      "Processing Document: jasper_deng_text_1 vs jasper_deng_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 65 out of 186\n",
      "Processing Document: jasper_deng_text_2 vs jasper_deng_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 66 out of 186\n",
      "Processing Document: jasper_deng_text_2 vs jbmurray_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 67 out of 186\n",
      "Processing Document: jasper_deng_text_3 vs jbmurray_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 68 out of 186\n",
      "Processing Document: jbmurray_text_2 vs jbmurray_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 69 out of 186\n",
      "Processing Document: jbmurray_text_2 vs jc37_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 70 out of 186\n",
      "Processing Document: jbmurray_text_4 vs jbmurray_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 71 out of 186\n",
      "Processing Document: jbmurray_text_4 vs jc37_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 72 out of 186\n",
      "Processing Document: jbmurray_text_5 vs jbmurray_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 73 out of 186\n",
      "Processing Document: jbmurray_text_5 vs jc37_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 74 out of 186\n",
      "Processing Document: jc37_text_1 vs jc37_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 75 out of 186\n",
      "Processing Document: jc37_text_1 vs jeffrey_vernon_merkey_text_10.xlsx\n",
      "Writing file\n",
      "Completing file 76 out of 186\n",
      "Processing Document: jc37_text_2 vs jc37_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 77 out of 186\n",
      "Processing Document: jc37_text_2 vs jeffrey_vernon_merkey_text_10.xlsx\n",
      "Writing file\n",
      "Completing file 78 out of 186\n",
      "Processing Document: jc37_text_10 vs jc37_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 79 out of 186\n",
      "Processing Document: jeffrey_vernon_merkey_text_1 vs jeffrey_vernon_merkey_text_10.xlsx\n",
      "Writing file\n",
      "Completing file 80 out of 186\n",
      "Processing Document: jeffrey_vernon_merkey_text_1 vs jerekrischel_text_13.xlsx\n",
      "Writing file\n",
      "Completing file 81 out of 186\n",
      "Processing Document: jeffrey_vernon_merkey_text_3 vs jeffrey_vernon_merkey_text_10.xlsx\n",
      "Writing file\n",
      "Completing file 82 out of 186\n",
      "Processing Document: jeffrey_vernon_merkey_text_3 vs jerekrischel_text_13.xlsx\n",
      "Writing file\n",
      "Completing file 83 out of 186\n",
      "Processing Document: jeffrey_vernon_merkey_text_11 vs jerekrischel_text_13.xlsx\n",
      "Writing file\n",
      "Completing file 84 out of 186\n",
      "Processing Document: jerekrischel_text_10 vs jerekrischel_text_13.xlsx\n",
      "Writing file\n",
      "Completing file 85 out of 186\n",
      "Processing Document: jerekrischel_text_10 vs jerryfriedman_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 86 out of 186\n",
      "Processing Document: jerekrischel_text_11 vs jerryfriedman_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 87 out of 186\n",
      "Processing Document: jerekrischel_text_12 vs jerekrischel_text_13.xlsx\n",
      "Writing file\n",
      "Completing file 88 out of 186\n",
      "Processing Document: jerekrischel_text_12 vs jerryfriedman_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 89 out of 186\n",
      "Processing Document: jerryfriedman_text_1 vs jerryfriedman_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 90 out of 186\n",
      "Processing Document: jerryfriedman_text_1 vs jimharlow99_text_10.xlsx\n",
      "Writing file\n",
      "Completing file 91 out of 186\n",
      "Processing Document: jerryfriedman_text_2 vs jimharlow99_text_10.xlsx\n",
      "Writing file\n",
      "Completing file 92 out of 186\n",
      "Processing Document: jerryfriedman_text_4 vs jerryfriedman_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 93 out of 186\n",
      "Processing Document: jerryfriedman_text_4 vs jimharlow99_text_10.xlsx\n",
      "Writing file\n",
      "Completing file 94 out of 186\n",
      "Processing Document: jéské_couriano_text_1 vs jéské_couriano_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 95 out of 186\n",
      "Processing Document: jéské_couriano_text_1 vs kashmiri_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 96 out of 186\n",
      "Processing Document: jéské_couriano_text_3 vs jéské_couriano_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 97 out of 186\n",
      "Processing Document: jéské_couriano_text_3 vs kashmiri_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 98 out of 186\n",
      "Processing Document: jéské_couriano_text_5 vs kashmiri_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 99 out of 186\n",
      "Processing Document: jim_hardie_text_1 vs jim_hardie_text_2.xlsx\n",
      "Writing file\n",
      "Completing file 100 out of 186\n",
      "Processing Document: jim_hardie_text_1 vs jmrh6_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 101 out of 186\n",
      "Processing Document: jim_hardie_text_3 vs jim_hardie_text_2.xlsx\n",
      "Writing file\n",
      "Completing file 102 out of 186\n",
      "Processing Document: jim_hardie_text_3 vs jmrh6_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 103 out of 186\n",
      "Processing Document: jim_hardie_text_4 vs jim_hardie_text_2.xlsx\n",
      "Writing file\n",
      "Completing file 104 out of 186\n",
      "Processing Document: jim_hardie_text_4 vs jmrh6_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 105 out of 186\n",
      "Processing Document: jimharlow99_text_11 vs jim_hardie_text_2.xlsx\n",
      "Writing file\n",
      "Completing file 106 out of 186\n",
      "Processing Document: jimharlow99_text_11 vs jimharlow99_text_10.xlsx\n",
      "Writing file\n",
      "Completing file 107 out of 186\n",
      "Processing Document: jimharlow99_text_12 vs jim_hardie_text_2.xlsx\n",
      "Writing file\n",
      "Completing file 108 out of 186\n",
      "Processing Document: jimharlow99_text_12 vs jimharlow99_text_10.xlsx\n",
      "Writing file\n",
      "Completing file 109 out of 186\n",
      "Processing Document: jimharlow99_text_13 vs jim_hardie_text_2.xlsx\n",
      "Writing file\n",
      "Completing file 110 out of 186\n",
      "Processing Document: jimharlow99_text_13 vs jimharlow99_text_10.xlsx\n",
      "Writing file\n",
      "Completing file 111 out of 186\n",
      "Processing Document: jmrh6_text_2 vs jmrh6_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 112 out of 186\n",
      "Processing Document: jmrh6_text_3 vs jmrh6_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 113 out of 186\n",
      "Processing Document: jmrh6_text_4 vs jmrh6_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 114 out of 186\n",
      "Processing Document: jmrh6_text_4 vs johnuniq_text_10.xlsx\n",
      "Writing file\n",
      "Completing file 115 out of 186\n",
      "Processing Document: johnuniq_text_1 vs jwanders_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 116 out of 186\n",
      "Processing Document: johnuniq_text_11 vs jwanders_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 117 out of 186\n",
      "Processing Document: johnuniq_text_11 vs johnuniq_text_10.xlsx\n",
      "Writing file\n",
      "Completing file 118 out of 186\n",
      "Processing Document: jwanders_text_1 vs jéské_couriano_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 119 out of 186\n",
      "Processing Document: jwanders_text_1 vs jwanders_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 120 out of 186\n",
      "Processing Document: jwanders_text_2 vs jéské_couriano_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 121 out of 186\n",
      "Processing Document: jwanders_text_2 vs jwanders_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 122 out of 186\n",
      "Processing Document: jwanders_text_5 vs jéské_couriano_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 123 out of 186\n",
      "Processing Document: jwanders_text_5 vs jwanders_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 124 out of 186\n",
      "Processing Document: kashmiri_text_1 vs kashmiri_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 125 out of 186\n",
      "Processing Document: kashmiri_text_1 vs kblott_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 126 out of 186\n",
      "Processing Document: kashmiri_text_2 vs kashmiri_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 127 out of 186\n",
      "Processing Document: kashmiri_text_2 vs kblott_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 128 out of 186\n",
      "Processing Document: kashmiri_text_4 vs kashmiri_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 129 out of 186\n",
      "Processing Document: kashmiri_text_4 vs kblott_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 130 out of 186\n",
      "Processing Document: kblott_text_3 vs kblott_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 131 out of 186\n",
      "Processing Document: kblott_text_3 vs killerchihuahua_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 132 out of 186\n",
      "Processing Document: kblott_text_4 vs kblott_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 133 out of 186\n",
      "Processing Document: kblott_text_4 vs killerchihuahua_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 134 out of 186\n",
      "Processing Document: kblott_text_5 vs kblott_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 135 out of 186\n",
      "Processing Document: kblott_text_5 vs killerchihuahua_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 136 out of 186\n",
      "Processing Document: killerchihuahua_text_1 vs killerchihuahua_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 137 out of 186\n",
      "Processing Document: killerchihuahua_text_1 vs kudzu1_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 138 out of 186\n",
      "Processing Document: killerchihuahua_text_2 vs killerchihuahua_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 139 out of 186\n",
      "Processing Document: killerchihuahua_text_2 vs kudzu1_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 140 out of 186\n",
      "Processing Document: killerchihuahua_text_11 vs killerchihuahua_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 141 out of 186\n",
      "Processing Document: killerchihuahua_text_11 vs kudzu1_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 142 out of 186\n",
      "Processing Document: kudzu1_text_2 vs kudzu1_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 143 out of 186\n",
      "Processing Document: kudzu1_text_2 vs l_tak_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 144 out of 186\n",
      "Processing Document: kudzu1_text_4 vs kudzu1_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 145 out of 186\n",
      "Processing Document: kudzu1_text_4 vs l_tak_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 146 out of 186\n",
      "Processing Document: kudzu1_text_5 vs kudzu1_text_1.xlsx\n",
      "Writing file\n",
      "Completing file 147 out of 186\n",
      "Processing Document: kudzu1_text_5 vs l_tak_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 148 out of 186\n",
      "Processing Document: l_tak_text_1 vs l_tak_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 149 out of 186\n",
      "Processing Document: l_tak_text_1 vs lear_21_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 150 out of 186\n",
      "Processing Document: l_tak_text_2 vs l_tak_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 151 out of 186\n",
      "Processing Document: l_tak_text_2 vs lear_21_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 152 out of 186\n",
      "Processing Document: l_tak_text_10 vs l_tak_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 153 out of 186\n",
      "Processing Document: l_tak_text_10 vs lear_21_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 154 out of 186\n",
      "Processing Document: lear_21_text_1 vs ledenierhomme_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 155 out of 186\n",
      "Processing Document: lear_21_text_2 vs lear_21_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 156 out of 186\n",
      "Processing Document: lear_21_text_2 vs ledenierhomme_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 157 out of 186\n",
      "Processing Document: lear_21_text_5 vs lear_21_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 158 out of 186\n",
      "Processing Document: lear_21_text_5 vs ledenierhomme_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 159 out of 186\n",
      "Processing Document: ledenierhomme_text_1 vs ledenierhomme_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 160 out of 186\n",
      "Processing Document: ledenierhomme_text_1 vs legolas2186_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 161 out of 186\n",
      "Processing Document: ledenierhomme_text_2 vs ledenierhomme_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 162 out of 186\n",
      "Processing Document: ledenierhomme_text_2 vs legolas2186_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 163 out of 186\n",
      "Processing Document: ledenierhomme_text_4 vs ledenierhomme_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 164 out of 186\n",
      "Processing Document: ledenierhomme_text_4 vs legolas2186_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 165 out of 186\n",
      "Processing Document: legolas2186_text_2 vs livelikemusic_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 166 out of 186\n",
      "Processing Document: livelikemusic_text_2 vs livelikemusic_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 167 out of 186\n",
      "Processing Document: livelikemusic_text_2 vs livitup_text_5.xlsx\n",
      "Writing file\n",
      "Completing file 168 out of 186\n",
      "Processing Document: livelikemusic_text_4 vs livelikemusic_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 169 out of 186\n",
      "Processing Document: livitup_text_1 vs machine_elf_1735_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 170 out of 186\n",
      "Processing Document: livitup_text_3 vs livitup_text_5.xlsx\n",
      "Writing file\n",
      "Completing file 171 out of 186\n",
      "Processing Document: livitup_text_3 vs machine_elf_1735_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 172 out of 186\n",
      "Processing Document: livitup_text_4 vs machine_elf_1735_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 173 out of 186\n",
      "Processing Document: machine_elf_1735_text_2 vs machine_elf_1735_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 174 out of 186\n",
      "Processing Document: machine_elf_1735_text_2 vs magog_the_ogre_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 175 out of 186\n",
      "Processing Document: machine_elf_1735_text_4 vs machine_elf_1735_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 176 out of 186\n",
      "Processing Document: machine_elf_1735_text_4 vs magog_the_ogre_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 177 out of 186\n",
      "Processing Document: machine_elf_1735_text_5 vs machine_elf_1735_text_3.xlsx\n",
      "Writing file\n",
      "Completing file 178 out of 186\n",
      "Processing Document: machine_elf_1735_text_5 vs magog_the_ogre_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 179 out of 186\n",
      "Processing Document: mareklug_text_1 vs marine_69_71_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 180 out of 186\n",
      "Processing Document: mareklug_text_3 vs mareklug_text_5.xlsx\n",
      "Writing file\n",
      "Completing file 181 out of 186\n",
      "Processing Document: mareklug_text_3 vs marine_69_71_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 182 out of 186\n",
      "Processing Document: mareklug_text_4 vs marine_69_71_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 183 out of 186\n",
      "Processing Document: marine_69_71_text_3 vs mathsci_text_10.xlsx\n",
      "Writing file\n",
      "Completing file 184 out of 186\n",
      "Processing Document: marine_69_71_text_5 vs marine_69_71_text_4.xlsx\n",
      "Writing file\n",
      "Completing file 185 out of 186\n",
      "Processing Document: marine_69_71_text_5 vs mathsci_text_10.xlsx\n",
      "Writing file\n",
      "Completing file 186 out of 186\n",
      "Processing Document: mathsci_text_3 vs mathsci_text_10.xlsx\n",
      "Writing file\n",
      "All files complete\n"
     ]
    }
   ],
   "source": [
    "orig_dir = \"/Volumes/BCross/paraphrase examples slurm/gpt2 results/Wiki-test-gpt2-raw\"\n",
    "save_dir = \"/Volumes/BCross/paraphrase examples slurm/gpt2 results/Wiki-test-gpt2-filtered\"\n",
    "phrase_loc = '/Volumes/BCross/paraphrase examples slurm/wiki-phrase-list-reviewed.xlsx'\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Get all .xlsx files from the original directory\n",
    "xlsx_files = glob.glob(os.path.join(orig_dir, \"*.xlsx\"))\n",
    "\n",
    "all_metadata = []\n",
    "\n",
    "for i, file_path in enumerate(xlsx_files, start=1):\n",
    "    print(f\"Completing file {i} out of {len(xlsx_files)}\")\n",
    "    \n",
    "    try:\n",
    "        metadata = create_results_doc_pipeline(file_path, write_excel=True, save_dir=save_dir, phrase_loc=phrase_loc)\n",
    "        all_metadata.append(metadata)\n",
    "    except Exception as e:\n",
    "        print(f\"File failed: {file_path}\\nError: {e}\")\n",
    "        continue\n",
    "\n",
    "# Combine all metadata after processing\n",
    "if all_metadata:\n",
    "    full_metadata = pd.concat(all_metadata, ignore_index=True)\n",
    "    # You can optionally save full_metadata here\n",
    "else:\n",
    "    full_metadata = pd.DataFrame()\n",
    "\n",
    "print(\"All files complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "48681998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>problem</th>\n",
       "      <th>corpus</th>\n",
       "      <th>known_author</th>\n",
       "      <th>unknown_author</th>\n",
       "      <th>unknown_doc_id</th>\n",
       "      <th>known_doc_id</th>\n",
       "      <th>target</th>\n",
       "      <th>num_phrases</th>\n",
       "      <th>phrases_kept</th>\n",
       "      <th>llr_no_context</th>\n",
       "      <th>llr_known</th>\n",
       "      <th>llr_unknown</th>\n",
       "      <th>normalised_llr_no_context</th>\n",
       "      <th>normalised_llr_known</th>\n",
       "      <th>normalised_llr_unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Hodja_Nasreddin vs Hodja_Nasreddin</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>Hodja_Nasreddin</td>\n",
       "      <td>Hodja_Nasreddin</td>\n",
       "      <td>hodja_nasreddin_text_3</td>\n",
       "      <td>hodja_nasreddin_text_1</td>\n",
       "      <td>True</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>14.496452</td>\n",
       "      <td>9.601831</td>\n",
       "      <td>8.983871</td>\n",
       "      <td>1.208038</td>\n",
       "      <td>0.800153</td>\n",
       "      <td>0.748656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Hodja_Nasreddin vs Hodja_Nasreddin</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>Hodja_Nasreddin</td>\n",
       "      <td>Hodja_Nasreddin</td>\n",
       "      <td>hodja_nasreddin_text_3</td>\n",
       "      <td>hodja_nasreddin_text_10</td>\n",
       "      <td>True</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>19.065389</td>\n",
       "      <td>5.264463</td>\n",
       "      <td>6.480075</td>\n",
       "      <td>1.733217</td>\n",
       "      <td>0.478588</td>\n",
       "      <td>0.589098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Hodja_Nasreddin vs Hodja_Nasreddin</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>Hodja_Nasreddin</td>\n",
       "      <td>Hodja_Nasreddin</td>\n",
       "      <td>hodja_nasreddin_text_3</td>\n",
       "      <td>hodja_nasreddin_text_11</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>10.347628</td>\n",
       "      <td>4.811538</td>\n",
       "      <td>5.554616</td>\n",
       "      <td>1.293453</td>\n",
       "      <td>0.601442</td>\n",
       "      <td>0.694327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Hodja_Nasreddin vs HonestopL</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>Hodja_Nasreddin</td>\n",
       "      <td>HonestopL</td>\n",
       "      <td>honestopl_text_1</td>\n",
       "      <td>hodja_nasreddin_text_1</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>17.638238</td>\n",
       "      <td>9.362668</td>\n",
       "      <td>5.072673</td>\n",
       "      <td>2.204780</td>\n",
       "      <td>1.170333</td>\n",
       "      <td>0.634084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Hodja_Nasreddin vs HonestopL</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>Hodja_Nasreddin</td>\n",
       "      <td>HonestopL</td>\n",
       "      <td>honestopl_text_1</td>\n",
       "      <td>hodja_nasreddin_text_10</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>7.835679</td>\n",
       "      <td>3.294278</td>\n",
       "      <td>2.046288</td>\n",
       "      <td>0.979460</td>\n",
       "      <td>0.411785</td>\n",
       "      <td>0.255786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>215</td>\n",
       "      <td>216</td>\n",
       "      <td>Mareklug vs Marine_69-71</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>Mareklug</td>\n",
       "      <td>Marine_69-71</td>\n",
       "      <td>marine_69_71_text_4</td>\n",
       "      <td>mareklug_text_4</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>16.939512</td>\n",
       "      <td>7.046125</td>\n",
       "      <td>6.928849</td>\n",
       "      <td>1.411626</td>\n",
       "      <td>0.587177</td>\n",
       "      <td>0.577404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>218</td>\n",
       "      <td>219</td>\n",
       "      <td>Marine_69-71 vs Marine_69-71</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>Marine_69-71</td>\n",
       "      <td>Marine_69-71</td>\n",
       "      <td>marine_69_71_text_4</td>\n",
       "      <td>marine_69_71_text_5</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>17.199060</td>\n",
       "      <td>7.942716</td>\n",
       "      <td>6.263631</td>\n",
       "      <td>1.911007</td>\n",
       "      <td>0.882524</td>\n",
       "      <td>0.695959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>220</td>\n",
       "      <td>221</td>\n",
       "      <td>Marine_69-71 vs Mathsci</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>Marine_69-71</td>\n",
       "      <td>Mathsci</td>\n",
       "      <td>mathsci_text_10</td>\n",
       "      <td>marine_69_71_text_3</td>\n",
       "      <td>False</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>12.369134</td>\n",
       "      <td>4.982426</td>\n",
       "      <td>3.083010</td>\n",
       "      <td>0.824609</td>\n",
       "      <td>0.332162</td>\n",
       "      <td>0.205534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>221</td>\n",
       "      <td>222</td>\n",
       "      <td>Marine_69-71 vs Mathsci</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>Marine_69-71</td>\n",
       "      <td>Mathsci</td>\n",
       "      <td>mathsci_text_10</td>\n",
       "      <td>marine_69_71_text_5</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>11.246587</td>\n",
       "      <td>4.930518</td>\n",
       "      <td>2.026293</td>\n",
       "      <td>1.405823</td>\n",
       "      <td>0.616315</td>\n",
       "      <td>0.253287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>224</td>\n",
       "      <td>225</td>\n",
       "      <td>Mathsci vs Mathsci</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>Mathsci</td>\n",
       "      <td>Mathsci</td>\n",
       "      <td>mathsci_text_10</td>\n",
       "      <td>mathsci_text_3</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9.648973</td>\n",
       "      <td>5.278513</td>\n",
       "      <td>2.452191</td>\n",
       "      <td>1.072108</td>\n",
       "      <td>0.586501</td>\n",
       "      <td>0.272466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>186 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  sample_id  ... normalised_llr_known normalised_llr_unknown\n",
       "0        0          1  ...             0.800153               0.748656\n",
       "1        1          2  ...             0.478588               0.589098\n",
       "2        2          3  ...             0.601442               0.694327\n",
       "3        3          4  ...             1.170333               0.634084\n",
       "4        4          5  ...             0.411785               0.255786\n",
       "..     ...        ...  ...                  ...                    ...\n",
       "181    215        216  ...             0.587177               0.577404\n",
       "182    218        219  ...             0.882524               0.695959\n",
       "183    220        221  ...             0.332162               0.205534\n",
       "184    221        222  ...             0.616315               0.253287\n",
       "185    224        225  ...             0.586501               0.272466\n",
       "\n",
       "[186 rows x 17 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_metadata = full_metadata.sort_values(by=\"index\").reset_index(drop=True)\n",
    "full_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "49e16488",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_save_loc = '/Volumes/BCross/paraphrase examples slurm/gpt2 results/wiki-test-gpt2-filtered-results.xlsx'\n",
    "\n",
    "full_metadata.to_excel(result_save_loc, index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
