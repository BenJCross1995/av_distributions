{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd79e6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f23990a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('../../src'))\n",
    "\n",
    "from read_and_write_docs import read_xml, read_jsonl\n",
    "from tokenize_and_score import load_model, compute_log_probs_with_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e2597c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_text = read_xml(\"/Volumes/BCross/datasets/test_sms/adjusted/SMS_ENG_20110818.0003.conv.xml\")\n",
    "unknown_text = read_xml(\"/Volumes/BCross/datasets/test_sms/adjusted/SMS_ENG_20110901.0001.conv.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "602418b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_text = read_jsonl(\"/Volumes/BCross/datasets/author_verification/training/Wiki/known_corpus_split/classicjupiter2_text_4.jsonl\").loc[0, 'text']\n",
    "unknown_text = read_jsonl(\"/Volumes/BCross/datasets/author_verification/training/Wiki/known_corpus_split/habap_text_3.jsonl\").loc[0, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f419ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "{2: {('a', 'note'),\n",
    "  ('drop', 'you'),\n",
    "  ('i', 'just'),\n",
    "  ('i', 'was'),\n",
    "  ('i', 'will'),\n",
    "  ('if', 'you'),\n",
    "  ('in', 'the'),\n",
    "  ('is', 'a'),\n",
    "  ('is', 'also'),\n",
    "  ('is', 'not'),\n",
    "  ('just', 'wanted'),\n",
    "  ('of', 'the'),\n",
    "  ('on', 'the'),\n",
    "  ('talk', 'page'),\n",
    "  ('that', 'there'),\n",
    "  ('the', 'article'),\n",
    "  ('the', 'discussion'),\n",
    "  ('the', 'page'),\n",
    "  ('there', 'is'),\n",
    "  ('to', 'be'),\n",
    "  ('to', 'drop'),\n",
    "  ('to', 'the'),\n",
    "  ('wanted', 'to'),\n",
    "  ('was', 'only'),\n",
    "  ('you', 'a'),\n",
    "  ('you', 'can')},\n",
    " 3: {('drop', 'you', 'a'),\n",
    "  ('just', 'wanted', 'to'),\n",
    "  ('there', 'is', 'a'),\n",
    "  ('to', 'drop', 'you'),\n",
    "  ('wanted', 'to', 'drop'),\n",
    "  ('you', 'a', 'note')},\n",
    " 4: {('drop', 'you', 'a', 'note'),\n",
    "  ('just', 'wanted', 'to', 'drop'),\n",
    "  ('to', 'drop', 'you', 'a'),\n",
    "  ('wanted', 'to', 'drop', 'you')},\n",
    " 5: {('just', 'wanted', 'to', 'drop', 'you'),\n",
    "  ('to', 'drop', 'you', 'a', 'note'),\n",
    "  ('wanted', 'to', 'drop', 'you', 'a')},\n",
    " 6: {('just', 'wanted', 'to', 'drop', 'you', 'a'),\n",
    "  ('wanted', 'to', 'drop', 'you', 'a', 'note')},\n",
    " 7: {('just', 'wanted', 'to', 'drop', 'you', 'a', 'note')}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f3040f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_phrase = \"bell me wen u\"\n",
    "base_phrase = \"just wanted to drop you a note\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1d5b3611",
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrase_list = [\n",
    "    \"bel me wen u\",\n",
    "    \"bel me wen ya\",\n",
    "    \"bel me wen yu\",\n",
    "    \"bel me whn u\",\n",
    "    \"bell me wen ya\",\n",
    "    \"bell me wen yu\",\n",
    "    \"bell me whn u\",\n",
    "    \"bell me when u\",\n",
    "    \"bell me when you\",\n",
    "    \"ring me wen u\",\n",
    "    \"ring me wen ya\",\n",
    "    \"ring me wen yu\",\n",
    "    \"ring me whn u\",\n",
    "    \"ring me when u\",\n",
    "    \"ring me when you\",\n",
    "    \"call me wen u\",\n",
    "    \"call me wen ya\",\n",
    "    \"call me wen yu\",\n",
    "    \"call me whn u\",\n",
    "    \"call me when u\",\n",
    "    \"call me when you\",\n",
    "    \"buzz me wen u\",\n",
    "    \"buzz me wen ya\",\n",
    "    \"buzz me wen yu\",\n",
    "    \"buzz me whn u\",\n",
    "    \"buzz me when u\",\n",
    "    \"buzz me when you\",\n",
    "    \"give me a bell wen u\",\n",
    "    \"give me a ring wen u\",\n",
    "    \"give me a call wen u\",\n",
    "    \"gimme a bell wen u\",\n",
    "    \"gimme a ring wen u\",\n",
    "    \"gimme a call wen u\",\n",
    "    \"hit me wen u\",\n",
    "    \"hit me wen ya\",\n",
    "    \"hit me wen yu\",\n",
    "    \"hit me whn u\",\n",
    "    \"hit me when u\",\n",
    "    \"hit me when you\",\n",
    "    \"phone me wen u\",\n",
    "    \"phone me wen ya\",\n",
    "    \"phone me wen yu\",\n",
    "    \"phone me whn u\",\n",
    "    \"phone me when u\",\n",
    "    \"phone me when you\",\n",
    "    \"holla wen u\",\n",
    "    \"holla at me wen u\",\n",
    "    \"holla when u\",\n",
    "    \"holler wen u\",\n",
    "    \"holler when u\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "74bfef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrase_list = [\n",
    "    \"just wanted to drop a note to you\",\n",
    "    \"just wanted to drop you a message\",\n",
    "    \"just wanted to drop a message to you\",\n",
    "    \"just wanted to drop you a line\",\n",
    "    \"just wanted to drop a line to you\",\n",
    "    \"just wanted to send you a note\",\n",
    "    \"just wanted to send a note to you\",\n",
    "    \"just wanted to send you a message\",\n",
    "    \"just wanted to send a message to you\",\n",
    "    \"just wanted to send you a line\",\n",
    "    \"just wanted to send a line to you\",\n",
    "    \"just wanted to leave you a note\",\n",
    "    \"just wanted to leave a note for you\",\n",
    "    \"just wanted to leave you a message\",\n",
    "    \"just wanted to leave a message for you\",\n",
    "    \"just wanted to give you a note\",\n",
    "    \"just wanted to give a note to you\",\n",
    "    \"just wanted to give you a message\",\n",
    "    \"just wanted to give a message to you\",\n",
    "    \"just wanted to pass you a note\",\n",
    "    \"just wanted to pass a note to you\",\n",
    "    \"just wanted to pass a message to you\", \n",
    "    \"just wanted to drop ya a note\",\n",
    "    \"just wanted to drop a note to ya\",\n",
    "    \"just wanted to send ya a note\",\n",
    "    \"just wanted to send a note to ya\",\n",
    "    \"just wanted to leave ya a note\",\n",
    "    \"just wanted to leave a note for ya\",\n",
    "    \"just wanted to give ya a note\",\n",
    "    \"just wanted to give a note to ya\",\n",
    "    \"just wanted to pass ya a note\",\n",
    "    \"just wanted to pass a note to ya\",\n",
    "    \"only wanted to drop you a note\",\n",
    "    \"simply wanted to drop you a note\",\n",
    "    \"only wanted to send you a note\",\n",
    "    \"simply wanted to send you a note\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9e5b39a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_before_phrase(text: str, phrase: str, case_insensitive: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Return everything in `text` before the first occurrence of `phrase`.\n",
    "    If `phrase` isn’t found, returns the entire `text`.\n",
    "\n",
    "    :param text:       The full string you want to trim.\n",
    "    :param phrase:     The substring (phrase) you want to stop at.\n",
    "    :param case_insensitive:  If True, match phrase ignoring case.\n",
    "    :return:           The portion of `text` before `phrase`.\n",
    "    \"\"\"\n",
    "    if case_insensitive:\n",
    "        idx = text.lower().find(phrase.lower())\n",
    "    else:\n",
    "        idx = text.find(phrase)\n",
    "\n",
    "    return text[:idx] if idx != -1 else text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "af93aeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_known_text = keep_before_phrase(known_text, base_phrase)\n",
    "base_unknown_text = keep_before_phrase(unknown_text, base_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f0747a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"International Surrealism was only recognized by the public and the historians and scholars between the years, 1924 to 1969.\\nFrom the late 1960's to 2002's SURREALISM DESIRE UNBOUND Retrospective, the public acknowledges that International Surrealism is a retrospective movement in the arts and literature composed of notable figures who came and went, in and out of the International Surrealist Movement, like Aragon, Dali, Ernst, Matta, etc.\\nBetween the years, 2003 to today, there is a surrealist presence online from artists and others that claim to be surrealist, and in fairness to the everyone that claims the surrealist label, its the public that is the final judge.\\nWikipedia is an online encyclopedia, and it should not be used for self-promotion of non-notable groups that claim to be surrealists, just because they say so.\\nThe Internet has literally hundreds of surreal and surrealist related material, art, etc, and we would open the floodgates for misinformation on this encyclopedia.\\nThe information on their online blogs is so vague and misleading, that its just a case of any creative person using a BLOG or MYSPACE to draw attention to themselves, but Wikipedia is not to be used as such, its just not right.\\nThe preceding Jam, thanks so much for contacting me on my discussion page.\\nDaniel, I hope that you can understand this, I was wrong about the validity of the information in the Surrealism article.\\nIf you intend to re-edit the article, I will support your edits.\\nThe article really needs credible reference material\\nThere is the user SUPERTHIKE who is also the same user as HydroSony, Jacques Stenzack, etc, using the Discussion page for flamebait.\\nThey are using the page to start up flamebait and disrupting the discussion, HydroSony in particular keeps asking for personal contact info.\\nThanks for getting back to me on my talk page.\\nCan you please investigate the IP's of the following users, who I believe is one person using sockpuppets, Hello Naconkantari.\\nI\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_known_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0646b8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = load_model(\"/Volumes/BCross/models/Qwen 2.5/Qwen2.5-0.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2f561d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def run_over_phrases(\n",
    "    base_known: str,\n",
    "    phrases: List[str],\n",
    "    tokenizer,\n",
    "    model\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    For each phrase in `phrases`, prepend base_known, run compute_log_probs_with_median,\n",
    "    and store tokens, log_probs, median_logprobs, and sum_log_probs.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    total = len(phrases)\n",
    "    for idx, phrase in enumerate(phrases, start=1):\n",
    "        print(f\"Scoring phrase {idx} out of {total}: {phrase}\")\n",
    "        text = base_known + phrase\n",
    "        tokens, log_probs, median_logprobs = compute_log_probs_with_median(text, tokenizer, model)\n",
    "        results[phrase] = {\n",
    "            \"text\": text,\n",
    "            \"tokens\":            tokens,\n",
    "            \"log_probs\":         log_probs,\n",
    "            \"median_logprobs\":   median_logprobs,\n",
    "            \"sum_log_probs\":     sum(log_probs),\n",
    "        }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f9ebaf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = run_over_phrases(base_known_text, paraphrase_list, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e2c8a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "def score_phrases(\n",
    "    base_text: str,\n",
    "    ref_phrase: str,\n",
    "    paraphrases: List[str],\n",
    "    tokenizer,\n",
    "    model\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    - Scores base_text alone → base_total and n_base_tokens\n",
    "    - Then scores base_text + ref_phrase and each base_text + paraphrase,\n",
    "      printing:\n",
    "        • full sequence sum (`sum_before`)\n",
    "        • phrase-only sum (`phrase_total`)\n",
    "        • the phrase text\n",
    "        • `difference`\n",
    "    - Trims off the base_text tokens to isolate phrase-only tokens/log-probs.\n",
    "    - Returns a DataFrame with columns:\n",
    "        phrase_type, phrase, tokens, base_total, sum_before,\n",
    "        log_probs, phrase_total, difference\n",
    "    \"\"\"\n",
    "    # 1) score base_text alone\n",
    "    print(\"Scoring base_text alone...\")\n",
    "    _, log_probs_base, _ = compute_log_probs_with_median(base_text, tokenizer, model)\n",
    "    base_total    = sum(log_probs_base)\n",
    "    n_base_tokens = len(log_probs_base)\n",
    "\n",
    "    # prepare items\n",
    "    all_items = [(\"reference\", ref_phrase)] + [(\"paraphrase\", p) for p in paraphrases]\n",
    "    total = len(all_items)\n",
    "    rows = []\n",
    "\n",
    "    for idx, (ptype, phrase) in enumerate(all_items, start=1):\n",
    "        print(f\"Scoring {ptype} {idx}/{total}…\")\n",
    "        text = base_text + phrase\n",
    "        tokens_all, log_probs_all, _ = compute_log_probs_with_median(text, tokenizer, model)\n",
    "\n",
    "        sum_before       = sum(log_probs_all)\n",
    "        # isolate phrase-only\n",
    "        phrase_tokens    = tokens_all[n_base_tokens:]\n",
    "        phrase_log_probs = log_probs_all[n_base_tokens:]\n",
    "        phrase_total     = sum(phrase_log_probs)\n",
    "        difference       = base_total - sum_before\n",
    "\n",
    "        rows.append({\n",
    "            \"phrase_type\":  ptype,\n",
    "            \"phrase\":       phrase,\n",
    "            \"tokens\":       phrase_tokens,\n",
    "            \"base_total\":   base_total,\n",
    "            \"sum_before\":   sum_before,\n",
    "            \"log_probs\":    phrase_log_probs,\n",
    "            \"phrase_total\": phrase_total,\n",
    "            \"difference\":   difference\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\n",
    "        \"phrase_type\",\n",
    "        \"phrase\",\n",
    "        \"tokens\",\n",
    "        \"base_total\",\n",
    "        \"sum_before\",\n",
    "        \"log_probs\",\n",
    "        \"phrase_total\",\n",
    "        \"difference\"\n",
    "    ])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c161c132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "def compute_log_probs_with_median(text: str, tokenizer, model):\n",
    "    \"\"\"\n",
    "    For each token (including the first), returns:\n",
    "      - tokens: list of tokenizer.convert_ids_to_tokens\n",
    "      - log_probs: list of log-probs for each token\n",
    "      - median_logprobs: median log-prob of the distribution at each step\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"]         # shape [1, seq_len]\n",
    "    # --- ALIGN TOKENS CORRECTLY HERE ---\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    logits = outputs.logits                 # [1, seq_len, vocab_size]\n",
    "\n",
    "    log_probs = []\n",
    "    median_logprobs = []\n",
    "    # for each position i, look at logits from i-1 (or the BOS for i=0)\n",
    "    for i in range(input_ids.size(1)):\n",
    "        prev_idx = 0 if i == 0 else i - 1\n",
    "        dist = torch.log_softmax(logits[0, prev_idx], dim=-1)\n",
    "        log_prob = dist[input_ids[0, i]].item()\n",
    "        median_lp = float(dist.median().item())\n",
    "        log_probs.append(log_prob)\n",
    "        median_logprobs.append(median_lp)\n",
    "\n",
    "    return tokens, log_probs, median_logprobs\n",
    "\n",
    "def score_phrases(\n",
    "    base_text: str,\n",
    "    ref_phrase: str,\n",
    "    paraphrases: List[str],\n",
    "    tokenizer,\n",
    "    model\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    1) Score base_text alone → base_total\n",
    "    2) For each phrase (reference + paraphrases):\n",
    "         a) Get its token count by scoring phrase alone\n",
    "         b) Score base_text + phrase → full tokens & log_probs\n",
    "         c) sum_before = sum(full log_probs)\n",
    "         d) phrase_tokens    = last n_phrase tokens of full tokens\n",
    "         e) phrase_log_probs = last n_phrase values of full log_probs\n",
    "         f) phrase_total     = sum(phrase_log_probs)\n",
    "         g) difference       = base_total - sum_before\n",
    "         h) APPEND row\n",
    "    3) Return DataFrame with columns:\n",
    "       phrase_type, phrase, tokens, base_total, sum_before,\n",
    "       log_probs, phrase_total, difference\n",
    "    \"\"\"\n",
    "    # 1) score base_text\n",
    "    print(\"→ Scoring base_text alone…\")\n",
    "    _, log_probs_base, _ = compute_log_probs_with_median(base_text.strip(), tokenizer, model)\n",
    "    base_total = sum(log_probs_base)\n",
    "    print(f\"   base_total = {base_total:.4f}\\n\")\n",
    "\n",
    "    items = [(\"reference\", ref_phrase)] + [(\"paraphrase\", p) for p in paraphrases]\n",
    "    rows = []\n",
    "\n",
    "    for idx, (ptype, phrase) in enumerate(items, start=1):\n",
    "        print(f\"→ [{idx}/{len(items)}] Processing {ptype}…\")\n",
    "\n",
    "        # a) phrase alone → get token count\n",
    "        tokens_phrase, log_probs_phrase, _ = compute_log_probs_with_median(phrase, tokenizer, model)\n",
    "        n_phrase_tokens = len(tokens_phrase)\n",
    "        # b) full sequence\n",
    "        full_text = base_text + phrase\n",
    "        tokens_full, log_probs_full, _ = compute_log_probs_with_median(full_text, tokenizer, model)\n",
    "        # c) full sum\n",
    "        sum_before = sum(log_probs_full)\n",
    "        # d/e) slice last n_phrase_tokens\n",
    "        phrase_tokens    = tokens_full[-n_phrase_tokens:]\n",
    "        phrase_log_probs = log_probs_full[-n_phrase_tokens:]\n",
    "        # f/g) compute sums\n",
    "        phrase_total = sum(phrase_log_probs)\n",
    "        difference   = base_total - sum_before\n",
    "        # h) collect\n",
    "        rows.append({\n",
    "            \"phrase_type\":  ptype,\n",
    "            \"phrase\":       phrase,\n",
    "            \"tokens\":       phrase_tokens,\n",
    "            \"sum_log_probs_base\":   base_total,\n",
    "            \"sum_log_probs_inc_phrase\":   sum_before,\n",
    "            \"difference\":   difference,\n",
    "            \"phrase_log_probs\":    phrase_log_probs,\n",
    "            \"sum_log_probs_phrase\": phrase_total,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\n",
    "        \"phrase_type\", \"phrase\", \"tokens\",\n",
    "        \"sum_log_probs_base\", \"sum_log_probs_inc_phrase\",\n",
    "        \"difference\", \"phrase_log_probs\", \"sum_log_probs_phrase\",\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "74753335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import pandas as pd\n",
    "# from typing import List\n",
    "\n",
    "# def compute_log_probs_with_median(text: str, tokenizer, model):\n",
    "#     \"\"\"\n",
    "#     For each token (including the first), returns:\n",
    "#       - tokens: list of tokenizer.convert_ids_to_tokens\n",
    "#       - log_probs: list of log-probs for each token\n",
    "#       - median_logprobs: median log-prob of the distribution at each step\n",
    "#     \"\"\"\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "#     input_ids = inputs[\"input_ids\"]         # shape [1, seq_len]\n",
    "#     # --- ALIGN TOKENS CORRECTLY HERE ---\n",
    "#     tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(input_ids)\n",
    "#     logits = outputs.logits                 # [1, seq_len, vocab_size]\n",
    "\n",
    "#     log_probs = []\n",
    "#     median_logprobs = []\n",
    "#     # for each position i, look at logits from i-1 (or the BOS for i=0)\n",
    "#     for i in range(input_ids.size(1)):\n",
    "#         prev_idx = 0 if i == 0 else i - 1\n",
    "#         dist = torch.log_softmax(logits[0, prev_idx], dim=-1)\n",
    "#         log_prob = dist[input_ids[0, i]].item()\n",
    "#         median_lp = float(dist.median().item())\n",
    "#         log_probs.append(log_prob)\n",
    "#         median_logprobs.append(median_lp)\n",
    "\n",
    "#     return tokens, log_probs, median_logprobs\n",
    "\n",
    "\n",
    "# def score_phrases(\n",
    "#     base_text: str,\n",
    "#     ref_phrase: str,\n",
    "#     paraphrases: List[str],\n",
    "#     tokenizer,\n",
    "#     model\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     1) Score base_text → base_total & n_base_tokens\n",
    "#     2) For ref_phrase + each paraphrase:\n",
    "#         • Score base_text+phrase → tokens_all, log_probs_all\n",
    "#         • sum_before = sum(log_probs_all)\n",
    "#         • phrase_tokens = tokens_all[n_base_tokens:]\n",
    "#         • phrase_log_probs = log_probs_all[n_base_tokens:]\n",
    "#         • phrase_total = sum(phrase_log_probs)\n",
    "#         • difference   = base_total - sum_before\n",
    "#         • PRINT sum_before, phrase_total, phrase, difference\n",
    "#         • APPEND to rows\n",
    "#     3) Return DataFrame with columns:\n",
    "#        phrase_type, phrase, tokens, base_total, sum_before,\n",
    "#        log_probs, phrase_total, difference\n",
    "#     \"\"\"\n",
    "#     # 1) score base_text alone\n",
    "#     print(\"→ Scoring base_text alone…\")\n",
    "#     tokens_base, log_probs_base, _ = compute_log_probs_with_median(base_text, tokenizer, model)\n",
    "#     base_total    = sum(log_probs_base)\n",
    "#     n_base_tokens = len(tokens_base)\n",
    "#     print(f\"   base_total = {base_total:.4f} over {n_base_tokens} tokens\\n\")\n",
    "\n",
    "#     all_items = [(\"reference\", ref_phrase)] + [(\"paraphrase\", p) for p in paraphrases]\n",
    "#     rows = []\n",
    "#     total = len(all_items)\n",
    "\n",
    "#     for idx, (ptype, phrase) in enumerate(all_items, start=1):\n",
    "#         print(f\"→ [{idx}/{total}] Scoring {ptype}…\")\n",
    "#         text = base_text + phrase\n",
    "#         tokens_all, log_probs_all, _ = compute_log_probs_with_median(text, tokenizer, model)\n",
    "\n",
    "#         sum_before       = sum(log_probs_all)\n",
    "#         # **NOW** this slice actually lines up\n",
    "#         phrase_tokens    = tokens_all[n_base_tokens-1:]\n",
    "#         phrase_log_probs = log_probs_all[n_base_tokens-1:]\n",
    "#         phrase_total     = sum(phrase_log_probs)\n",
    "#         difference       = base_total - sum_before\n",
    "\n",
    "#         rows.append({\n",
    "#             \"phrase_type\":  ptype,\n",
    "#             \"phrase\":       phrase,\n",
    "#             \"tokens\":       phrase_tokens,\n",
    "#             \"base_total\":   base_total,\n",
    "#             \"sum_before\":   sum_before,\n",
    "#             \"log_probs\":    phrase_log_probs,\n",
    "#             \"phrase_total\": phrase_total,\n",
    "#             \"difference\":   difference\n",
    "#         })\n",
    "\n",
    "#     df = pd.DataFrame(rows, columns=[\n",
    "#         \"phrase_type\", \"phrase\", \"tokens\",\n",
    "#         \"base_total\", \"sum_before\", \"log_probs\",\n",
    "#         \"phrase_total\", \"difference\"\n",
    "#     ])\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9c56433a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Scoring base_text alone…\n",
      "   base_total = -1583.6236\n",
      "\n",
      "→ [1/37] Processing reference…\n",
      "→ [2/37] Processing paraphrase…\n",
      "→ [3/37] Processing paraphrase…\n",
      "→ [4/37] Processing paraphrase…\n",
      "→ [5/37] Processing paraphrase…\n",
      "→ [6/37] Processing paraphrase…\n",
      "→ [7/37] Processing paraphrase…\n",
      "→ [8/37] Processing paraphrase…\n",
      "→ [9/37] Processing paraphrase…\n",
      "→ [10/37] Processing paraphrase…\n",
      "→ [11/37] Processing paraphrase…\n",
      "→ [12/37] Processing paraphrase…\n",
      "→ [13/37] Processing paraphrase…\n",
      "→ [14/37] Processing paraphrase…\n",
      "→ [15/37] Processing paraphrase…\n",
      "→ [16/37] Processing paraphrase…\n",
      "→ [17/37] Processing paraphrase…\n",
      "→ [18/37] Processing paraphrase…\n",
      "→ [19/37] Processing paraphrase…\n",
      "→ [20/37] Processing paraphrase…\n",
      "→ [21/37] Processing paraphrase…\n",
      "→ [22/37] Processing paraphrase…\n",
      "→ [23/37] Processing paraphrase…\n",
      "→ [24/37] Processing paraphrase…\n",
      "→ [25/37] Processing paraphrase…\n",
      "→ [26/37] Processing paraphrase…\n",
      "→ [27/37] Processing paraphrase…\n",
      "→ [28/37] Processing paraphrase…\n",
      "→ [29/37] Processing paraphrase…\n",
      "→ [30/37] Processing paraphrase…\n",
      "→ [31/37] Processing paraphrase…\n",
      "→ [32/37] Processing paraphrase…\n",
      "→ [33/37] Processing paraphrase…\n",
      "→ [34/37] Processing paraphrase…\n",
      "→ [35/37] Processing paraphrase…\n",
      "→ [36/37] Processing paraphrase…\n",
      "→ [37/37] Processing paraphrase…\n"
     ]
    }
   ],
   "source": [
    "results_known = score_phrases(base_known_text, base_phrase, paraphrase_list, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "28d724d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase_type</th>\n",
       "      <th>phrase</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sum_log_probs_base</th>\n",
       "      <th>sum_log_probs_inc_phrase</th>\n",
       "      <th>difference</th>\n",
       "      <th>phrase_log_probs</th>\n",
       "      <th>sum_log_probs_phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to send you a message</td>\n",
       "      <td>[Ġjust, Ġwanted, Ġto, Ġsend, Ġyou, Ġa, Ġmessage]</td>\n",
       "      <td>-1583.623627</td>\n",
       "      <td>-1600.023247</td>\n",
       "      <td>16.399620</td>\n",
       "      <td>[-5.656452178955078, -2.6228346824645996, -0.0...</td>\n",
       "      <td>-16.399620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to drop you a line</td>\n",
       "      <td>[Ġjust, Ġwanted, Ġto, Ġdrop, Ġyou, Ġa, Ġline]</td>\n",
       "      <td>-1583.623627</td>\n",
       "      <td>-1601.297384</td>\n",
       "      <td>17.673757</td>\n",
       "      <td>[-5.656452178955078, -2.6228346824645996, -0.0...</td>\n",
       "      <td>-17.673757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  phrase_type                             phrase  \\\n",
       "8  paraphrase  just wanted to send you a message   \n",
       "4  paraphrase     just wanted to drop you a line   \n",
       "\n",
       "                                             tokens  sum_log_probs_base  \\\n",
       "8  [Ġjust, Ġwanted, Ġto, Ġsend, Ġyou, Ġa, Ġmessage]        -1583.623627   \n",
       "4     [Ġjust, Ġwanted, Ġto, Ġdrop, Ġyou, Ġa, Ġline]        -1583.623627   \n",
       "\n",
       "   sum_log_probs_inc_phrase  difference  \\\n",
       "8              -1600.023247   16.399620   \n",
       "4              -1601.297384   17.673757   \n",
       "\n",
       "                                    phrase_log_probs  sum_log_probs_phrase  \n",
       "8  [-5.656452178955078, -2.6228346824645996, -0.0...            -16.399620  \n",
       "4  [-5.656452178955078, -2.6228346824645996, -0.0...            -17.673757  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_known.sort_values(by='difference', inplace=True)\n",
    "results_known.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "63b766ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "results_known.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b1eb1d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def logsumexp(xs):\n",
    "    m = max(xs)\n",
    "    return m + math.log(sum(math.exp(x - m) for x in xs))\n",
    "\n",
    "def p_ref_among_options(logprob_lists, priors=None, ref_index=0):\n",
    "    L = [sum(lp) for lp in logprob_lists]\n",
    "    if priors is not None:\n",
    "        L = [Li + math.log(pi) for Li, pi in zip(L, priors)]\n",
    "    return math.exp(L[ref_index] - logsumexp(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "31b81bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def logsumexp(xs):\n",
    "    m = max(xs)\n",
    "    return m + math.log(sum(math.exp(x - m) for x in xs))\n",
    "\n",
    "def ref_metrics(token_logprob_lists, priors=None, ref_index=0):\n",
    "    # sequence log-likelihoods\n",
    "    L = [sum(lst) for lst in token_logprob_lists]\n",
    "    if priors is not None:\n",
    "        L = [Li + math.log(pi) for Li, pi in zip(L, priors)]\n",
    "\n",
    "    logZ = logsumexp(L)\n",
    "    p_ref = math.exp(L[ref_index] - logZ)\n",
    "\n",
    "    others = [L[i] for i in range(len(L)) if i != ref_index]\n",
    "    log_den_others = logsumexp(others)\n",
    "    llr_ref_vs_rest = L[ref_index] - log_den_others     # = logit(p_ref)\n",
    "    odds_ref = math.exp(llr_ref_vs_rest)\n",
    "\n",
    "    pmf = [math.exp(Li - logZ) for Li in L]            # full normalized PMF\n",
    "\n",
    "    return {\n",
    "        \"p_ref\": p_ref,\n",
    "        \"pmf\": pmf,\n",
    "        \"log_den\": logZ,            # numerically stable\n",
    "        \"den\": math.exp(logZ),      # may overflow/underflow\n",
    "        \"llr_ref_vs_rest\": llr_ref_vs_rest,\n",
    "        \"odds_ref\": odds_ref\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "52272141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12267416407896513"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_known.sort_values(by='phrase_type', ascending=False, inplace=True)\n",
    "\n",
    "p_ref_among_options(results_known['phrase_log_probs'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d5f5a572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'p_ref': 0.12267416407896513,\n",
       " 'pmf': [0.12267416407896513,\n",
       "  0.0017798566555540494,\n",
       "  0.0125798948455079,\n",
       "  0.12657870523569342,\n",
       "  0.06738147396391961,\n",
       "  0.018400070914360057,\n",
       "  0.014097329110571225,\n",
       "  0.013937170982132843,\n",
       "  0.012674830969522089,\n",
       "  0.006810618166642372,\n",
       "  0.0017889109618277436,\n",
       "  0.0062875900431353824,\n",
       "  0.0007158860450894309,\n",
       "  0.004117177263267585,\n",
       "  0.000434469539497419,\n",
       "  0.0028527199424338507,\n",
       "  0.0026874769234762367,\n",
       "  0.12691055836512496,\n",
       "  0.000895849705025534,\n",
       "  0.4537831787918548,\n",
       "  7.693809488150951e-07,\n",
       "  2.992009824435929e-05,\n",
       "  0.000626254290402676,\n",
       "  0.00029521942142091335,\n",
       "  0.00023608819831597998,\n",
       "  0.00015680944972811312,\n",
       "  0.00012433316791233835,\n",
       "  0.00010197514528528433,\n",
       "  3.969938516975028e-05,\n",
       "  2.8915014733866484e-05,\n",
       "  0.0009289088465866804,\n",
       "  2.6158133359340097e-05,\n",
       "  8.432866211287504e-06,\n",
       "  3.666536183207843e-06,\n",
       "  3.538822827199206e-06,\n",
       "  1.1761084605139176e-06,\n",
       "  2.0263060763071356e-07],\n",
       " 'log_den': -15.609484244127495,\n",
       " 'den': 1.6629803877247642e-07,\n",
       " 'llr_ref_vs_rest': -1.9673466902158605,\n",
       " 'odds_ref': 0.13982736978237878}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_metrics(results_known['phrase_log_probs'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1df89ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-849.7581649534404"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(results_known['sum_log_probs_phrase'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c7caa2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Scoring base_text alone…\n",
      "   base_total = -2347.6514\n",
      "\n",
      "→ [1/37] Processing reference…\n",
      "→ [2/37] Processing paraphrase…\n",
      "→ [3/37] Processing paraphrase…\n",
      "→ [4/37] Processing paraphrase…\n",
      "→ [5/37] Processing paraphrase…\n",
      "→ [6/37] Processing paraphrase…\n",
      "→ [7/37] Processing paraphrase…\n",
      "→ [8/37] Processing paraphrase…\n",
      "→ [9/37] Processing paraphrase…\n",
      "→ [10/37] Processing paraphrase…\n",
      "→ [11/37] Processing paraphrase…\n",
      "→ [12/37] Processing paraphrase…\n",
      "→ [13/37] Processing paraphrase…\n",
      "→ [14/37] Processing paraphrase…\n",
      "→ [15/37] Processing paraphrase…\n",
      "→ [16/37] Processing paraphrase…\n",
      "→ [17/37] Processing paraphrase…\n",
      "→ [18/37] Processing paraphrase…\n",
      "→ [19/37] Processing paraphrase…\n",
      "→ [20/37] Processing paraphrase…\n",
      "→ [21/37] Processing paraphrase…\n",
      "→ [22/37] Processing paraphrase…\n",
      "→ [23/37] Processing paraphrase…\n",
      "→ [24/37] Processing paraphrase…\n",
      "→ [25/37] Processing paraphrase…\n",
      "→ [26/37] Processing paraphrase…\n",
      "→ [27/37] Processing paraphrase…\n",
      "→ [28/37] Processing paraphrase…\n",
      "→ [29/37] Processing paraphrase…\n",
      "→ [30/37] Processing paraphrase…\n",
      "→ [31/37] Processing paraphrase…\n",
      "→ [32/37] Processing paraphrase…\n",
      "→ [33/37] Processing paraphrase…\n",
      "→ [34/37] Processing paraphrase…\n",
      "→ [35/37] Processing paraphrase…\n",
      "→ [36/37] Processing paraphrase…\n",
      "→ [37/37] Processing paraphrase…\n"
     ]
    }
   ],
   "source": [
    "results_unknown = score_phrases(base_unknown_text, base_phrase, paraphrase_list, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3e5fee08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase_type</th>\n",
       "      <th>phrase</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sum_log_probs_base</th>\n",
       "      <th>sum_log_probs_inc_phrase</th>\n",
       "      <th>difference</th>\n",
       "      <th>phrase_log_probs</th>\n",
       "      <th>sum_log_probs_phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reference</td>\n",
       "      <td>just wanted to drop you a note</td>\n",
       "      <td>[.just, Ġwanted, Ġto, Ġdrop, Ġyou, Ġa, Ġnote]</td>\n",
       "      <td>-2347.651417</td>\n",
       "      <td>-2366.505260</td>\n",
       "      <td>18.853843</td>\n",
       "      <td>[-18.542102813720703, -1.2133077383041382, -0....</td>\n",
       "      <td>-21.993745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to send you a note</td>\n",
       "      <td>[.just, Ġwanted, Ġto, Ġsend, Ġyou, Ġa, Ġnote]</td>\n",
       "      <td>-2347.651417</td>\n",
       "      <td>-2370.240882</td>\n",
       "      <td>22.589465</td>\n",
       "      <td>[-18.542102813720703, -1.2133077383041382, -0....</td>\n",
       "      <td>-25.729366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>only wanted to drop you a note</td>\n",
       "      <td>[.only, Ġwanted, Ġto, Ġdrop, Ġyou, Ġa, Ġnote]</td>\n",
       "      <td>-2347.651417</td>\n",
       "      <td>-2370.678002</td>\n",
       "      <td>23.026585</td>\n",
       "      <td>[-20.74664306640625, -2.797487258911133, -0.02...</td>\n",
       "      <td>-26.166487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>simply wanted to drop you a note</td>\n",
       "      <td>[.sim, ply, Ġwanted, Ġto, Ġdrop, Ġyou, Ġa, Ġnote]</td>\n",
       "      <td>-2347.651417</td>\n",
       "      <td>-2370.883709</td>\n",
       "      <td>23.232292</td>\n",
       "      <td>[-19.088476181030273, -0.2531338334083557, -3....</td>\n",
       "      <td>-26.372194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to drop you a message</td>\n",
       "      <td>[.just, Ġwanted, Ġto, Ġdrop, Ġyou, Ġa, Ġmessage]</td>\n",
       "      <td>-2347.651417</td>\n",
       "      <td>-2371.118758</td>\n",
       "      <td>23.467341</td>\n",
       "      <td>[-18.542102813720703, -1.2133077383041382, -0....</td>\n",
       "      <td>-26.607242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   phrase_type                             phrase  \\\n",
       "0    reference     just wanted to drop you a note   \n",
       "6   paraphrase     just wanted to send you a note   \n",
       "33  paraphrase     only wanted to drop you a note   \n",
       "34  paraphrase   simply wanted to drop you a note   \n",
       "2   paraphrase  just wanted to drop you a message   \n",
       "\n",
       "                                               tokens  sum_log_probs_base  \\\n",
       "0       [.just, Ġwanted, Ġto, Ġdrop, Ġyou, Ġa, Ġnote]        -2347.651417   \n",
       "6       [.just, Ġwanted, Ġto, Ġsend, Ġyou, Ġa, Ġnote]        -2347.651417   \n",
       "33      [.only, Ġwanted, Ġto, Ġdrop, Ġyou, Ġa, Ġnote]        -2347.651417   \n",
       "34  [.sim, ply, Ġwanted, Ġto, Ġdrop, Ġyou, Ġa, Ġnote]        -2347.651417   \n",
       "2    [.just, Ġwanted, Ġto, Ġdrop, Ġyou, Ġa, Ġmessage]        -2347.651417   \n",
       "\n",
       "    sum_log_probs_inc_phrase  difference  \\\n",
       "0               -2366.505260   18.853843   \n",
       "6               -2370.240882   22.589465   \n",
       "33              -2370.678002   23.026585   \n",
       "34              -2370.883709   23.232292   \n",
       "2               -2371.118758   23.467341   \n",
       "\n",
       "                                     phrase_log_probs  sum_log_probs_phrase  \n",
       "0   [-18.542102813720703, -1.2133077383041382, -0....            -21.993745  \n",
       "6   [-18.542102813720703, -1.2133077383041382, -0....            -25.729366  \n",
       "33  [-20.74664306640625, -2.797487258911133, -0.02...            -26.166487  \n",
       "34  [-19.088476181030273, -0.2531338334083557, -3....            -26.372194  \n",
       "2   [-18.542102813720703, -1.2133077383041382, -0....            -26.607242  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_unknown.sort_values(by='difference', inplace=True)\n",
    "results_unknown.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ebf213ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "results_unknown.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c35076cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9140128938128587"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_unknown.sort_values(by='phrase_type', ascending=False, inplace=True)\n",
    "\n",
    "p_ref_among_options(results_unknown['phrase_log_probs'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "501ac05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_metrics = ref_metrics(results_unknown['phrase_log_probs'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "677e3f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'p_ref': 0.9140128938128587,\n",
       " 'pmf': [0.9140128938128587,\n",
       "  0.00016217135470962,\n",
       "  4.784973369683106e-05,\n",
       "  3.571054297152586e-05,\n",
       "  2.1443270550867535e-05,\n",
       "  1.2708328473742358e-05,\n",
       "  7.457871773571731e-06,\n",
       "  5.912474607660524e-06,\n",
       "  3.987683067853873e-06,\n",
       "  2.4226698073304474e-06,\n",
       "  1.8995599646531103e-06,\n",
       "  1.8479749160098645e-06,\n",
       "  1.020550925898058e-06,\n",
       "  7.595025255933304e-07,\n",
       "  3.6509056688697477e-07,\n",
       "  3.408860488113954e-07,\n",
       "  6.500583407921985e-08,\n",
       "  0.0001244046886345057,\n",
       "  0.00017975772915759886,\n",
       "  0.02180682103856921,\n",
       "  0.00018677745182850718,\n",
       "  0.014084886187533164,\n",
       "  0.011466106350459549,\n",
       "  0.009064328332442115,\n",
       "  0.00669993353248597,\n",
       "  0.00638473242985564,\n",
       "  0.005240339306690617,\n",
       "  0.0032569309362411336,\n",
       "  0.0019164574733504629,\n",
       "  0.0014093162653898397,\n",
       "  0.001211408831988399,\n",
       "  0.0006841709377676138,\n",
       "  0.0006740666049030073,\n",
       "  0.0005588811416520615,\n",
       "  0.000384417909883419,\n",
       "  0.0003473590114705051,\n",
       "  4.7526398237230654e-08],\n",
       " 'log_den': -21.903833932897783,\n",
       " 'den': 3.0710422436889723e-10,\n",
       " 'llr_ref_vs_rest': 2.363647321414213,\n",
       " 'odds_ref': 10.629650587654375}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "80c332f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.806970898748768e-10"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(-21.993745)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1aaf3714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0710422436889754e-10"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(math.exp(lp) for lp in results_unknown['sum_log_probs_phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "83d3ceae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.806970898748768e-10"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(-21.993745)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7d4faf1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9140124674341824"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.806970898748768e-10 / 3.0710422436889754e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "fd5dd6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.008312444037906"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log(0.12267416407896513) - math.log(0.9140124674341824)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2ccb21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['just', 'Ġwanted', 'Ġto', 'Ġdrop', 'Ġyou', 'Ġa', 'Ġnote'],\n",
       " [-13.547368049621582,\n",
       "  -11.230600357055664,\n",
       "  -0.07694364339113235,\n",
       "  -5.435577392578125,\n",
       "  -3.012918472290039,\n",
       "  -0.5511003136634827,\n",
       "  -2.96950101852417],\n",
       " [-16.516681671142578,\n",
       "  -16.516681671142578,\n",
       "  -22.0695858001709,\n",
       "  -21.389036178588867,\n",
       "  -20.315900802612305,\n",
       "  -20.781362533569336,\n",
       "  -21.7308349609375])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = compute_log_probs_with_median(base_phrase, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "39260d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_phrases_no_context(\n",
    "    ref_phrase: str,\n",
    "    paraphrases: List[str],\n",
    "    tokenizer,\n",
    "    model\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    1) Score base_text alone → base_total\n",
    "    2) For each phrase (reference + paraphrases):\n",
    "         a) Get its token count by scoring phrase alone\n",
    "         b) Score base_text + phrase → full tokens & log_probs\n",
    "         c) sum_before = sum(full log_probs)\n",
    "         d) phrase_tokens    = last n_phrase tokens of full tokens\n",
    "         e) phrase_log_probs = last n_phrase values of full log_probs\n",
    "         f) phrase_total     = sum(phrase_log_probs)\n",
    "         g) difference       = base_total - sum_before\n",
    "         h) APPEND row\n",
    "    3) Return DataFrame with columns:\n",
    "       phrase_type, phrase, tokens, base_total, sum_before,\n",
    "       log_probs, phrase_total, difference\n",
    "    \"\"\"\n",
    "    # 1) score base_text\n",
    "    items = [(\"reference\", ref_phrase)] + [(\"paraphrase\", p) for p in paraphrases]\n",
    "    rows = []\n",
    "\n",
    "    for idx, (ptype, phrase) in enumerate(items, start=1):\n",
    "        print(f\"→ [{idx}/{len(items)}] Processing {ptype}…\")\n",
    "\n",
    "        # a) phrase alone → get token count\n",
    "        tokens_phrase, log_probs_phrase, _ = compute_log_probs_with_median(phrase, tokenizer, model)\n",
    "        # b) compute sum\n",
    "        phrase_total = sum(log_probs_phrase)\n",
    "        # h) collect\n",
    "        rows.append({\n",
    "            \"phrase_type\":  ptype,\n",
    "            \"phrase\":       phrase,\n",
    "            \"tokens\":       tokens_phrase,\n",
    "            \"log_probs\":    log_probs_phrase,\n",
    "            \"sum_log_probs\": phrase_total,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\n",
    "        \"phrase_type\", \"phrase\", \"tokens\",\n",
    "        \"log_probs\", \"sum_log_probs\",\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "71e2307f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ [1/37] Processing reference…\n",
      "→ [2/37] Processing paraphrase…\n",
      "→ [3/37] Processing paraphrase…\n",
      "→ [4/37] Processing paraphrase…\n",
      "→ [5/37] Processing paraphrase…\n",
      "→ [6/37] Processing paraphrase…\n",
      "→ [7/37] Processing paraphrase…\n",
      "→ [8/37] Processing paraphrase…\n",
      "→ [9/37] Processing paraphrase…\n",
      "→ [10/37] Processing paraphrase…\n",
      "→ [11/37] Processing paraphrase…\n",
      "→ [12/37] Processing paraphrase…\n",
      "→ [13/37] Processing paraphrase…\n",
      "→ [14/37] Processing paraphrase…\n",
      "→ [15/37] Processing paraphrase…\n",
      "→ [16/37] Processing paraphrase…\n",
      "→ [17/37] Processing paraphrase…\n",
      "→ [18/37] Processing paraphrase…\n",
      "→ [19/37] Processing paraphrase…\n",
      "→ [20/37] Processing paraphrase…\n",
      "→ [21/37] Processing paraphrase…\n",
      "→ [22/37] Processing paraphrase…\n",
      "→ [23/37] Processing paraphrase…\n",
      "→ [24/37] Processing paraphrase…\n",
      "→ [25/37] Processing paraphrase…\n",
      "→ [26/37] Processing paraphrase…\n",
      "→ [27/37] Processing paraphrase…\n",
      "→ [28/37] Processing paraphrase…\n",
      "→ [29/37] Processing paraphrase…\n",
      "→ [30/37] Processing paraphrase…\n",
      "→ [31/37] Processing paraphrase…\n",
      "→ [32/37] Processing paraphrase…\n",
      "→ [33/37] Processing paraphrase…\n",
      "→ [34/37] Processing paraphrase…\n",
      "→ [35/37] Processing paraphrase…\n",
      "→ [36/37] Processing paraphrase…\n",
      "→ [37/37] Processing paraphrase…\n"
     ]
    }
   ],
   "source": [
    "phrase_no_context_results = score_phrases_no_context(base_phrase, paraphrase_list, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "de720cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase_type</th>\n",
       "      <th>phrase</th>\n",
       "      <th>tokens</th>\n",
       "      <th>log_probs</th>\n",
       "      <th>sum_log_probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reference</td>\n",
       "      <td>just wanted to drop you a note</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġdrop, Ġyou, Ġa, Ġnote]</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-36.824009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to drop a note to you</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġdrop, Ġa, Ġnote, Ġto, Ġyou]</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-35.761074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to drop you a message</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġdrop, Ġyou, Ġa, Ġmessage]</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-39.778634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to drop a message to you</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġdrop, Ġa, Ġmessage, Ġto,...</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-37.104399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to drop you a line</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġdrop, Ġyou, Ġa, Ġline]</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-34.001307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to drop a line to you</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġdrop, Ġa, Ġline, Ġto, Ġyou]</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-38.315746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to send you a note</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġsend, Ġyou, Ġa, Ġnote]</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-36.229169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to send a note to you</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġsend, Ġa, Ġnote, Ġto, Ġyou]</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-38.441291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to send you a message</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġsend, Ġyou, Ġa, Ġmessage]</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-34.957332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to send a message to you</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġsend, Ġa, Ġmessage, Ġto,...</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-36.873977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to send you a line</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġsend, Ġyou, Ġa, Ġline]</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-41.211228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to send a line to you</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġsend, Ġa, Ġline, Ġto, Ġyou]</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-47.928334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to leave you a note</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġleave, Ġyou, Ġa, Ġnote]</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-37.398837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to leave a note for you</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġleave, Ġa, Ġnote, Ġfor, ...</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-37.701949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to leave you a message</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġleave, Ġyou, Ġa, Ġmessage]</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-37.468703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to leave a message for you</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġleave, Ġa, Ġmessage, Ġfo...</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-37.449453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to give you a note</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġgive, Ġyou, Ġa, Ġnote]</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-37.434040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to give a note to you</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġgive, Ġa, Ġnote, Ġto, Ġyou]</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-39.546929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to give you a message</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġgive, Ġyou, Ġa, Ġmessage]</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-39.499083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to give a message to you</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġgive, Ġa, Ġmessage, Ġto,...</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-41.382180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to pass you a note</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġpass, Ġyou, Ġa, Ġnote]</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-40.869490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to pass a note to you</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġpass, Ġa, Ġnote, Ġto, Ġyou]</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-40.457631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to pass a message to you</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġpass, Ġa, Ġmessage, Ġto,...</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-41.917173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to drop ya a note</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġdrop, Ġya, Ġa, Ġnote]</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-42.407314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to drop a note to ya</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġdrop, Ġa, Ġnote, Ġto, Ġya]</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-42.006555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to send ya a note</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġsend, Ġya, Ġa, Ġnote]</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-43.879005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to send a note to ya</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġsend, Ġa, Ġnote, Ġto, Ġya]</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-44.622148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to leave ya a note</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġleave, Ġya, Ġa, Ġnote]</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-43.533917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to leave a note for ya</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġleave, Ġa, Ġnote, Ġfor, ...</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-43.273208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to give ya a note</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġgive, Ġya, Ġa, Ġnote]</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-44.844115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to give a note to ya</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġgive, Ġa, Ġnote, Ġto, Ġya]</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-44.735384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to pass ya a note</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġpass, Ġya, Ġa, Ġnote]</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-49.065433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>just wanted to pass a note to ya</td>\n",
       "      <td>[just, Ġwanted, Ġto, Ġpass, Ġa, Ġnote, Ġto, Ġya]</td>\n",
       "      <td>[-13.547368049621582, -11.230600357055664, -0....</td>\n",
       "      <td>-46.829541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>only wanted to drop you a note</td>\n",
       "      <td>[only, Ġwanted, Ġto, Ġdrop, Ġyou, Ġa, Ġnote]</td>\n",
       "      <td>[-15.629056930541992, -13.394790649414062, -0....</td>\n",
       "      <td>-56.300932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>simply wanted to drop you a note</td>\n",
       "      <td>[sim, ply, Ġwanted, Ġto, Ġdrop, Ġyou, Ġa, Ġnote]</td>\n",
       "      <td>[-14.350906372070312, -14.789237976074219, -8....</td>\n",
       "      <td>-63.827073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>only wanted to send you a note</td>\n",
       "      <td>[only, Ġwanted, Ġto, Ġsend, Ġyou, Ġa, Ġnote]</td>\n",
       "      <td>[-15.629056930541992, -13.394790649414062, -0....</td>\n",
       "      <td>-46.332183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>paraphrase</td>\n",
       "      <td>simply wanted to send you a note</td>\n",
       "      <td>[sim, ply, Ġwanted, Ġto, Ġsend, Ġyou, Ġa, Ġnote]</td>\n",
       "      <td>[-14.350906372070312, -14.789237976074219, -8....</td>\n",
       "      <td>-51.408504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   phrase_type                                  phrase  \\\n",
       "0    reference          just wanted to drop you a note   \n",
       "1   paraphrase       just wanted to drop a note to you   \n",
       "2   paraphrase       just wanted to drop you a message   \n",
       "3   paraphrase    just wanted to drop a message to you   \n",
       "4   paraphrase          just wanted to drop you a line   \n",
       "5   paraphrase       just wanted to drop a line to you   \n",
       "6   paraphrase          just wanted to send you a note   \n",
       "7   paraphrase       just wanted to send a note to you   \n",
       "8   paraphrase       just wanted to send you a message   \n",
       "9   paraphrase    just wanted to send a message to you   \n",
       "10  paraphrase          just wanted to send you a line   \n",
       "11  paraphrase       just wanted to send a line to you   \n",
       "12  paraphrase         just wanted to leave you a note   \n",
       "13  paraphrase     just wanted to leave a note for you   \n",
       "14  paraphrase      just wanted to leave you a message   \n",
       "15  paraphrase  just wanted to leave a message for you   \n",
       "16  paraphrase          just wanted to give you a note   \n",
       "17  paraphrase       just wanted to give a note to you   \n",
       "18  paraphrase       just wanted to give you a message   \n",
       "19  paraphrase    just wanted to give a message to you   \n",
       "20  paraphrase          just wanted to pass you a note   \n",
       "21  paraphrase       just wanted to pass a note to you   \n",
       "22  paraphrase    just wanted to pass a message to you   \n",
       "23  paraphrase           just wanted to drop ya a note   \n",
       "24  paraphrase        just wanted to drop a note to ya   \n",
       "25  paraphrase           just wanted to send ya a note   \n",
       "26  paraphrase        just wanted to send a note to ya   \n",
       "27  paraphrase          just wanted to leave ya a note   \n",
       "28  paraphrase      just wanted to leave a note for ya   \n",
       "29  paraphrase           just wanted to give ya a note   \n",
       "30  paraphrase        just wanted to give a note to ya   \n",
       "31  paraphrase           just wanted to pass ya a note   \n",
       "32  paraphrase        just wanted to pass a note to ya   \n",
       "33  paraphrase          only wanted to drop you a note   \n",
       "34  paraphrase        simply wanted to drop you a note   \n",
       "35  paraphrase          only wanted to send you a note   \n",
       "36  paraphrase        simply wanted to send you a note   \n",
       "\n",
       "                                               tokens  \\\n",
       "0        [just, Ġwanted, Ġto, Ġdrop, Ġyou, Ġa, Ġnote]   \n",
       "1   [just, Ġwanted, Ġto, Ġdrop, Ġa, Ġnote, Ġto, Ġyou]   \n",
       "2     [just, Ġwanted, Ġto, Ġdrop, Ġyou, Ġa, Ġmessage]   \n",
       "3   [just, Ġwanted, Ġto, Ġdrop, Ġa, Ġmessage, Ġto,...   \n",
       "4        [just, Ġwanted, Ġto, Ġdrop, Ġyou, Ġa, Ġline]   \n",
       "5   [just, Ġwanted, Ġto, Ġdrop, Ġa, Ġline, Ġto, Ġyou]   \n",
       "6        [just, Ġwanted, Ġto, Ġsend, Ġyou, Ġa, Ġnote]   \n",
       "7   [just, Ġwanted, Ġto, Ġsend, Ġa, Ġnote, Ġto, Ġyou]   \n",
       "8     [just, Ġwanted, Ġto, Ġsend, Ġyou, Ġa, Ġmessage]   \n",
       "9   [just, Ġwanted, Ġto, Ġsend, Ġa, Ġmessage, Ġto,...   \n",
       "10       [just, Ġwanted, Ġto, Ġsend, Ġyou, Ġa, Ġline]   \n",
       "11  [just, Ġwanted, Ġto, Ġsend, Ġa, Ġline, Ġto, Ġyou]   \n",
       "12      [just, Ġwanted, Ġto, Ġleave, Ġyou, Ġa, Ġnote]   \n",
       "13  [just, Ġwanted, Ġto, Ġleave, Ġa, Ġnote, Ġfor, ...   \n",
       "14   [just, Ġwanted, Ġto, Ġleave, Ġyou, Ġa, Ġmessage]   \n",
       "15  [just, Ġwanted, Ġto, Ġleave, Ġa, Ġmessage, Ġfo...   \n",
       "16       [just, Ġwanted, Ġto, Ġgive, Ġyou, Ġa, Ġnote]   \n",
       "17  [just, Ġwanted, Ġto, Ġgive, Ġa, Ġnote, Ġto, Ġyou]   \n",
       "18    [just, Ġwanted, Ġto, Ġgive, Ġyou, Ġa, Ġmessage]   \n",
       "19  [just, Ġwanted, Ġto, Ġgive, Ġa, Ġmessage, Ġto,...   \n",
       "20       [just, Ġwanted, Ġto, Ġpass, Ġyou, Ġa, Ġnote]   \n",
       "21  [just, Ġwanted, Ġto, Ġpass, Ġa, Ġnote, Ġto, Ġyou]   \n",
       "22  [just, Ġwanted, Ġto, Ġpass, Ġa, Ġmessage, Ġto,...   \n",
       "23        [just, Ġwanted, Ġto, Ġdrop, Ġya, Ġa, Ġnote]   \n",
       "24   [just, Ġwanted, Ġto, Ġdrop, Ġa, Ġnote, Ġto, Ġya]   \n",
       "25        [just, Ġwanted, Ġto, Ġsend, Ġya, Ġa, Ġnote]   \n",
       "26   [just, Ġwanted, Ġto, Ġsend, Ġa, Ġnote, Ġto, Ġya]   \n",
       "27       [just, Ġwanted, Ġto, Ġleave, Ġya, Ġa, Ġnote]   \n",
       "28  [just, Ġwanted, Ġto, Ġleave, Ġa, Ġnote, Ġfor, ...   \n",
       "29        [just, Ġwanted, Ġto, Ġgive, Ġya, Ġa, Ġnote]   \n",
       "30   [just, Ġwanted, Ġto, Ġgive, Ġa, Ġnote, Ġto, Ġya]   \n",
       "31        [just, Ġwanted, Ġto, Ġpass, Ġya, Ġa, Ġnote]   \n",
       "32   [just, Ġwanted, Ġto, Ġpass, Ġa, Ġnote, Ġto, Ġya]   \n",
       "33       [only, Ġwanted, Ġto, Ġdrop, Ġyou, Ġa, Ġnote]   \n",
       "34   [sim, ply, Ġwanted, Ġto, Ġdrop, Ġyou, Ġa, Ġnote]   \n",
       "35       [only, Ġwanted, Ġto, Ġsend, Ġyou, Ġa, Ġnote]   \n",
       "36   [sim, ply, Ġwanted, Ġto, Ġsend, Ġyou, Ġa, Ġnote]   \n",
       "\n",
       "                                            log_probs  sum_log_probs  \n",
       "0   [-13.547368049621582, -11.230600357055664, -0....     -36.824009  \n",
       "1   [-13.547368049621582, -11.230600357055664, -0....     -35.761074  \n",
       "2   [-13.547368049621582, -11.230600357055664, -0....     -39.778634  \n",
       "3   [-13.547368049621582, -11.230600357055664, -0....     -37.104399  \n",
       "4   [-13.547368049621582, -11.230600357055664, -0....     -34.001307  \n",
       "5   [-13.547368049621582, -11.230600357055664, -0....     -38.315746  \n",
       "6   [-13.547368049621582, -11.230600357055664, -0....     -36.229169  \n",
       "7   [-13.547368049621582, -11.230600357055664, -0....     -38.441291  \n",
       "8   [-13.547368049621582, -11.230600357055664, -0....     -34.957332  \n",
       "9   [-13.547368049621582, -11.230600357055664, -0....     -36.873977  \n",
       "10  [-13.547368049621582, -11.230600357055664, -0....     -41.211228  \n",
       "11  [-13.547368049621582, -11.230600357055664, -0....     -47.928334  \n",
       "12  [-13.547368049621582, -11.230600357055664, -0....     -37.398837  \n",
       "13  [-13.547368049621582, -11.230600357055664, -0....     -37.701949  \n",
       "14  [-13.547368049621582, -11.230600357055664, -0....     -37.468703  \n",
       "15  [-13.547368049621582, -11.230600357055664, -0....     -37.449453  \n",
       "16  [-13.547368049621582, -11.230600357055664, -0....     -37.434040  \n",
       "17  [-13.547368049621582, -11.230600357055664, -0....     -39.546929  \n",
       "18  [-13.547368049621582, -11.230600357055664, -0....     -39.499083  \n",
       "19  [-13.547368049621582, -11.230600357055664, -0....     -41.382180  \n",
       "20  [-13.547368049621582, -11.230600357055664, -0....     -40.869490  \n",
       "21  [-13.547368049621582, -11.230600357055664, -0....     -40.457631  \n",
       "22  [-13.547368049621582, -11.230600357055664, -0....     -41.917173  \n",
       "23  [-13.547368049621582, -11.230600357055664, -0....     -42.407314  \n",
       "24  [-13.547368049621582, -11.230600357055664, -0....     -42.006555  \n",
       "25  [-13.547368049621582, -11.230600357055664, -0....     -43.879005  \n",
       "26  [-13.547368049621582, -11.230600357055664, -0....     -44.622148  \n",
       "27  [-13.547368049621582, -11.230600357055664, -0....     -43.533917  \n",
       "28  [-13.547368049621582, -11.230600357055664, -0....     -43.273208  \n",
       "29  [-13.547368049621582, -11.230600357055664, -0....     -44.844115  \n",
       "30  [-13.547368049621582, -11.230600357055664, -0....     -44.735384  \n",
       "31  [-13.547368049621582, -11.230600357055664, -0....     -49.065433  \n",
       "32  [-13.547368049621582, -11.230600357055664, -0....     -46.829541  \n",
       "33  [-15.629056930541992, -13.394790649414062, -0....     -56.300932  \n",
       "34  [-14.350906372070312, -14.789237976074219, -8....     -63.827073  \n",
       "35  [-15.629056930541992, -13.394790649414062, -0....     -46.332183  \n",
       "36  [-14.350906372070312, -14.789237976074219, -8....     -51.408504  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_no_context_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c5e7a5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "phrase_no_context_results.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "eebe9711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'p_ref': 0.02942703608435714,\n",
       " 'pmf': [0.02942703608435714,\n",
       "  0.08518705975877248,\n",
       "  0.00153309522815474,\n",
       "  0.02223181571976897,\n",
       "  0.49502898376941395,\n",
       "  0.006620540822840208,\n",
       "  0.05334360039450061,\n",
       "  0.005839424315377672,\n",
       "  0.19029799722300392,\n",
       "  0.0279927686630204,\n",
       "  0.00036593318764960883,\n",
       "  4.4279190953408884e-07,\n",
       "  0.016561591261916014,\n",
       "  0.012231002196408955,\n",
       "  0.015443991910654193,\n",
       "  0.015744169946309096,\n",
       "  0.01598870792759976,\n",
       "  0.0019328462398086792,\n",
       "  0.00202757444268726,\n",
       "  0.00030843112278552016,\n",
       "  0.0005150109174092671,\n",
       "  0.0007774717419987056,\n",
       "  0.00018063978406505414,\n",
       "  0.00011064914097456609,\n",
       "  0.00016519444097009707,\n",
       "  2.539807869046445e-05,\n",
       "  1.2079753541652437e-05,\n",
       "  3.586496373663749e-05,\n",
       "  4.6547386795370134e-05,\n",
       "  9.675178073893586e-06,\n",
       "  1.0786488684721038e-05,\n",
       "  1.4202459703150708e-07,\n",
       "  1.32861536726391e-06,\n",
       "  1.0233554992817302e-10,\n",
       "  5.5139786685459756e-14,\n",
       "  2.1847368120972857e-06,\n",
       "  1.3638956227979283e-08],\n",
       " 'log_den': -33.29816781462455,\n",
       " 'den': 3.4577171230513927e-15,\n",
       " 'llr_ref_vs_rest': -3.4959727350786665,\n",
       " 'odds_ref': 0.03031924149796808}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_metrics(phrase_no_context_results['log_probs'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4895eff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
