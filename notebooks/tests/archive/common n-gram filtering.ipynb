{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb7b54e2",
   "metadata": {},
   "source": [
    "# Common n-gram filtering test\n",
    "\n",
    "From first exploration some 2-grams contain only punctuation and a single word. Want to test how to remove this automatically before sending to the LLM ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d11c4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from from_root import from_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66e44005",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure we can import from src/\n",
    "sys.path.insert(0, str(from_root(\"src\")))\n",
    "\n",
    "from read_and_write_docs import read_jsonl\n",
    "from utils import apply_temp_doc_id\n",
    "from n_gram_functions import (\n",
    "    common_ngrams,\n",
    "    filter_ngrams,\n",
    "    pretty_print_common_ngrams\n",
    ")\n",
    "from model_loading import load_model, distinct_special_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5592b2",
   "metadata": {},
   "source": [
    "## Load model and get special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81ca4a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = load_model(\"/Users/user/Documents/models/Qwen2.5-0.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7647dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = distinct_special_chars(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5ba0c8",
   "metadata": {},
   "source": [
    "## Load texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d291d4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"Wiki\"\n",
    "data_type = \"test\"\n",
    "\n",
    "known_loc = f\"/Volumes/BCross/datasets/author_verification/{data_type}/{corpus}/known_raw.jsonl\"\n",
    "known_loc = f\"/Users/user/Documents/test_data/{corpus}/known_raw.jsonl\"\n",
    "known = read_jsonl(known_loc)\n",
    "known = apply_temp_doc_id(known)\n",
    "\n",
    "unknown_loc = f\"/Volumes/BCross/datasets/author_verification/{data_type}/{corpus}/unknown_raw.jsonl\"\n",
    "unknown_loc = f\"/Users/user/Documents/test_data/{corpus}/unknown_raw.jsonl\"\n",
    "unknown = read_jsonl(unknown_loc)\n",
    "unknown_df = apply_temp_doc_id(unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef77d8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = known[known['doc_id'] == 'hootmag_text_1'].reset_index().loc[0, 'text']\n",
    "t2 = unknown[unknown['doc_id'] == 'hootmag_text_13'].reset_index().loc[0, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6efd8c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article indicates that the HDI data are estimates for 2012.\n",
      "I answered all of your questions - even though I thought they were irrelevant to this discussion, and that's why I expect you to answer all of my questions - even though you think they are irrelevant to this discussion.\n",
      "I understand your position, so you don't have to repeat it.\n",
      "I agree with you that if userSilvertrial has external sources only, then that won't be sufficient.\n",
      "However, what if userSilvertrial has 'more than just external' sources?\n"
     ]
    }
   ],
   "source": [
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eabf8466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indeed, this is pure logic Just think about the following hypothetical case On Saturday, the first user posts a hidden post, which is going to be unhidden on Tuesday, automatically.\n",
      "In your view, this case involves a prohibitted kind of changing the first user's post, because it's a hidden post on Sunday when it has already been responded to, and it's an automatically unhidden post on Tuseday.\n",
      "Now, if a third party un-hides according to your suggestion the first user's post - before Tuesday, say on Monday, then we get to a second case being very similar to the first prohibited one it's a hidden post on Sunday when it has already been responded to, and it's an unhidden post on Monday after it has already been un-hidden by the third party, who has done that according to your suggestion.\n",
      "Logically, one must infer the following just as in your opinion the first case is prohibited because it involves a change between Sunday when the post is hidden and Tuesday when it has already been unhidden automatically, so the second case must be prohibited because it involves a change between Sunday when the post is hidden and Monday when it has already been unhidden by the third party.\n",
      "To sum up, both cases involves a prohibited act of changing a post after the post has already been responded to, although the second act was recommended by you.\n",
      "The only solution for this kind of cases, is to replace the time-delays - by another mechanis, which will make the post hidden for ever, so that its status never changes and by the way Wikipedia has such a mechanism.\n",
      "I thought that your opinion was different from how you present it now, because you indicated that Wikipedia permitted no user to change e.g. to make a dated-reveal their own post after it had been responded to, so I inferred that you couldn't think that any third party was permitted to change other users' posts after they had been responded to this is pure logic.\n",
      "Had you been aware of the very basic historical facts which I explained thoroughly, you wouldn't have said that, as an honest person and you 'are' an honest person. 'don't have' to find such a person, because nothing concrete has came out - or may come out - of this discussion as far as my case is concerned  The time delays which were not my initiative remained there untill\n"
     ]
    }
   ],
   "source": [
    "print(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28443610",
   "metadata": {},
   "source": [
    "## Get common n-grams between the two texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03253b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "common = common_ngrams(t1, t2, 2, model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1e9030c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: {(\"'t\", 'Ġhave'),\n",
       "  (',', 'Ġand'),\n",
       "  (',', 'Ġso'),\n",
       "  (',', 'Ġthen'),\n",
       "  ('.Ċ', 'i'),\n",
       "  ('Ġthis', 'Ġdiscussion'),\n",
       "  ('Ġto', 'Ġthis')}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36a7dfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = filter_ngrams(common, special_tokens=special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "226d9380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: {(\"'t\", 'Ġhave'), ('Ġthis', 'Ġdiscussion'), ('Ġto', 'Ġthis')}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e20af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Set, Tuple, List, Union\n",
    "import string\n",
    "\n",
    "def pretty_print_common_ngrams(\n",
    "    common: Dict[int, Set[Tuple[Any, ...]]],\n",
    "    sep: str = \" \",\n",
    "    order: str = \"count_desc\",      # \"count_desc\" | \"len_asc\" | \"len_desc\"\n",
    "    tokenizer=None,                 # Optional HuggingFace tokenizer\n",
    "    return_format: str = \"print\",   # \"print\" | \"flat\" | \"grouped\"\n",
    "    show_raw: bool = False          # If True, include raw token forms\n",
    ") -> Union[None, List[Union[str, Tuple[str, str]]], Dict[int, List[Union[str, Tuple[str, str]]]]]:\n",
    "    \"\"\"\n",
    "    Pretty-print or return shared n-grams, optionally paired with raw form.\n",
    "    If show_raw=True, each output element is a tuple (pretty_str, raw_repr).\n",
    "    raw_repr is a string representing the token tuple, e.g. \"('Ġhello', 'Ġworld')\".\n",
    "    \"\"\"\n",
    "\n",
    "    if not common:\n",
    "        if return_format == \"print\":\n",
    "            print(\"{}\")\n",
    "            return None\n",
    "        return [] if return_format == \"flat\" else {}\n",
    "\n",
    "    def stringify_ngram(ngram: Tuple[Any, ...]) -> str:\n",
    "        \"\"\"Convert to human-readable text, using tokenizer if available.\"\"\"\n",
    "        if tokenizer is None:\n",
    "            return sep.join(map(str, ngram))\n",
    "\n",
    "        toks = list(ngram)\n",
    "        # If all ints, decode in one shot\n",
    "        if all(isinstance(t, int) for t in toks):\n",
    "            return tokenizer.decode(\n",
    "                toks,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=False,\n",
    "            )\n",
    "\n",
    "        # Otherwise, convert each token (id or str) to string form\n",
    "        specials = set(getattr(tokenizer, \"all_special_tokens\", []))\n",
    "        norm_tokens: List[str] = []\n",
    "        for t in toks:\n",
    "            if isinstance(t, int):\n",
    "                norm_tokens.append(tokenizer.convert_ids_to_tokens(t))\n",
    "            else:\n",
    "                norm_tokens.append(str(t))\n",
    "\n",
    "        # Filter out special tokens like <s>, </s>\n",
    "        norm_tokens = [t for t in norm_tokens if t not in specials]\n",
    "\n",
    "        return tokenizer.convert_tokens_to_string(norm_tokens)\n",
    "\n",
    "    def raw_repr_of_ngram(ngram: Tuple[Any, ...]) -> str:\n",
    "        \"\"\"Return a string showing the raw token tuple, as tokens or ints.\"\"\"\n",
    "        # We want something like \"('Ġhello', 'Ġworld')\" or \"(12, 34, 56)\" or mixed\n",
    "        return \"(\" + \", \".join(repr(tok) for tok in ngram) + \")\"\n",
    "\n",
    "    # Build grouped mapping: for each n, a list of pretty or (pretty, raw)\n",
    "    grouped: Dict[int, List[Union[str, Tuple[str, str]]]] = {}\n",
    "    for n, grams in common.items():\n",
    "        out_list: List[Union[str, Tuple[str, str]]] = []\n",
    "        for g in sorted(grams):\n",
    "            pretty = stringify_ngram(g)\n",
    "            if show_raw:\n",
    "                raw = raw_repr_of_ngram(g)\n",
    "                out_list.append( (pretty, raw) )\n",
    "            else:\n",
    "                out_list.append(pretty)\n",
    "        grouped[n] = out_list\n",
    "\n",
    "    # Order the groups by your `order`\n",
    "    if order == \"count_desc\":\n",
    "        items = sorted(grouped.items(), key=lambda kv: (-len(kv[1]), kv[0]))\n",
    "    elif order == \"len_asc\":\n",
    "        items = sorted(grouped.items(), key=lambda kv: kv[0])\n",
    "    elif order == \"len_desc\":\n",
    "        items = sorted(grouped.items(), key=lambda kv: -kv[0])\n",
    "    else:\n",
    "        items = sorted(grouped.items(), key=lambda kv: (-len(kv[1]), kv[0]))\n",
    "\n",
    "    if return_format == \"flat\":\n",
    "        flat: List[Union[str, Tuple[str, str]]] = []\n",
    "        for _, lst in items:\n",
    "            flat.extend(lst)\n",
    "        return flat\n",
    "\n",
    "    if return_format == \"grouped\":\n",
    "        return grouped\n",
    "\n",
    "    # print mode\n",
    "    for n, lst in items:\n",
    "        if show_raw:\n",
    "            # print each as \"pretty (raw: …)\"\n",
    "            pretty_with_raw = [f\"{p}  [raw: {r}]\" for (p, r) in lst]\n",
    "            print(f\"{n}-grams ({len(lst)}): {pretty_with_raw}\")\n",
    "        else:\n",
    "            print(f\"{n}-grams ({len(lst)}): {lst}\")\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "956be848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'t have\", ' this discussion', ' to this']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretty_print_common_ngrams(filtered, tokenizer=tokenizer, order='len_desc', return_format='flat', show_raw=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc09d0af",
   "metadata": {},
   "source": [
    "## Trial with some edge cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b514a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_case_ngrams = {\n",
    "    2: {\n",
    "        # ---------- your originals ----------\n",
    "        # Word + Punctuation — should be filtered\n",
    "        (\"Ġhello\", \"!\"),\n",
    "        # Punctuation + Word — should be filtered\n",
    "        (\",\", \"Ġhowever\"),\n",
    "        # Word + no space — should be filtered (single word)\n",
    "        (\"Ġwe\", \"'re\"),\n",
    "        # Both punctuation — should be filtered\n",
    "        (\"!\", \"?\"),\n",
    "        # Two real words — should be kept\n",
    "        (\"Ġmachine\", \"Ġlearning\"),\n",
    "        # Both tokens without space — should be filtered (likely one word)\n",
    "        (\"un\", \"breakable\"),\n",
    "        # First token has no special char, second does — should be kept\n",
    "        (\"Super\", \"Ġfragile\"),\n",
    "        # First token is special (e.g., newline), second is word — may be kept or filtered\n",
    "        (\"Ċ\", \"Ġstart\"),\n",
    "        # Two formatting characters — should be filtered\n",
    "        (\"Ċ\", \"Ċ\"),\n",
    "        # Long word, subword chunks — should be filtered as single word\n",
    "        (\".Ċ\", \"pneu\"),\n",
    "        # Word with trailing punctuation inside same token — should be kept\n",
    "        (\"Ġhello!\", \"Ġworld\"),\n",
    "        # Word with attached contraction — likely one word\n",
    "        (\"Ġi\", \"'m\"),\n",
    "\n",
    "        # ---------- additions: punctuation + word / word + punctuation ----------\n",
    "        # Word + punctuation — should be filtered\n",
    "        (\"Ġword\", \".\"),\n",
    "        # Punctuation + word — should be filtered\n",
    "        (\"(\", \"Ġparenthetical\"),\n",
    "        # Word + punctuation — should be filtered\n",
    "        (\"Ġend\", \")\"),\n",
    "        # Punctuation + word (quote) — should be filtered\n",
    "        ('\"', \"Ġquoted\"),\n",
    "        # Word + Unicode dash — should be filtered\n",
    "        (\"Ġdash\", \"—\"),\n",
    "\n",
    "        # ---------- additions: explicit whitespace markers before/after words ----------\n",
    "        # Newline marker + word — may be kept or filtered per your policy\n",
    "        (\"Ċ\", \"ĠTitle\"),\n",
    "        # Word + newline marker — may be kept or filtered\n",
    "        (\"Ġtrail\", \"Ċ\"),\n",
    "        # Tab/VT/FF/CR markers + word — usually filtered as formatting\n",
    "        (\"ĉ\", \"Ġtabbed\"),          # tab\n",
    "        (\"ċ\", \"Ġvtabbed\"),         # vertical tab\n",
    "        (\"Č\", \"Ġformfeed\"),        # form feed\n",
    "        (\"č\", \"Ġcarriage\"),        # carriage return\n",
    "\n",
    "        # ---------- additions: NBSP remap glyphs (byte-level) ----------\n",
    "        # Two visible glyphs (NBSP bytes) — should be filtered (all-special)\n",
    "        (\"Â\", \"Ơ\"),\n",
    "        # NBSP pair then a word — should be filtered (special + word)\n",
    "        (\"ÂƠ\", \"Ġword\"),\n",
    "        # Word then NBSP pair — should be filtered (word + special)\n",
    "        (\"Ġword\", \"ÂƠ\"),\n",
    "\n",
    "        # ---------- additions: SentencePiece-style space marker ----------\n",
    "        # Bare SP space marker + word — should be filtered (special + word)\n",
    "        (\"▁\", \"Hello\"),\n",
    "        # SP \"<unk>\"-like token + word — should be filtered (special-ish + word)\n",
    "        (\"▁<unk>\", \"world\"),\n",
    "        # Two SP-prefixed words (both with spaces inside token) — should be kept\n",
    "        (\"▁we\", \"▁test\"),\n",
    "\n",
    "        # ---------- additions: pure-special bigrams ----------\n",
    "        # Newline + tab — should be filtered\n",
    "        (\"Ċ\", \"ĉ\"),\n",
    "        # Two SP markers — should be filtered\n",
    "        (\"▁\", \"▁\"),\n",
    "    },\n",
    "\n",
    "    3: {\n",
    "        # ---------- your originals ----------\n",
    "        # Subword sequence from a single long word\n",
    "        (\"Ġsuper\", \"cal\", \"ifragilistic\"),\n",
    "        # Punctuation + subword + word — likely formatting garbage\n",
    "        (\"Ċ\", \"i\", \"'m\"),\n",
    "        # Real phrase with spacing — should be kept\n",
    "        (\"Ġnatural\", \"Ġlanguage\", \"Ġprocessing\"),\n",
    "        # Repeated punctuation — should be filtered\n",
    "        (\"Ċ\", \"Ċ\", \"Ċ\"),\n",
    "        # A word broken badly into subtokens without spacing — should be filtered\n",
    "        (\"bio\", \"tech\", \"nology\"),\n",
    "        # First token has no space, rest do — should be kept\n",
    "        (\"Bio\", \"Ġtech\", \"Ġboom\"),\n",
    "        # Two contractions — likely one semantic word\n",
    "        (\"Ġthey\", \"'d\", \"Ġ've\"),\n",
    "\n",
    "        # ---------- additions: punctuation + word + punctuation ----------\n",
    "        # Punct + word + punct — should be filtered\n",
    "        (\"(\", \"Ġaside\", \")\"),\n",
    "        # Word + punct + newline — should be filtered\n",
    "        (\"Ġhello\", \"!\", \"Ċ\"),\n",
    "        # ASCII punct around word — should be filtered\n",
    "        (\"-\", \"Ġbreak\", \"-\"),\n",
    "\n",
    "        # ---------- additions: whitespace markers across positions ----------\n",
    "        # Newline + words — may be kept or filtered per rule\n",
    "        (\"Ċ\", \"Ġnew\", \"Ġparagraph\"),\n",
    "        # Tab + words — should be filtered (formatting)\n",
    "        (\"ĉ\", \"Ġtabbed\", \"Ġline\"),\n",
    "        # Form feed + words — should be filtered (formatting)\n",
    "        (\"Č\", \"Ġform\", \"Ġfeed\"),\n",
    "        # Carriage return + words — should be filtered (formatting)\n",
    "        (\"č\", \"Ġcarriage\", \"Ġreturn\"),\n",
    "\n",
    "        # ---------- additions: NBSP glyphs around words ----------\n",
    "        # NBSP pair then a word — should be filtered (mostly special)\n",
    "        (\"Â\", \"Ơ\", \"Ġspace\"),\n",
    "        # Word + NBSP pair + word — may be filtered (special sandwiched)\n",
    "        (\"Ġbefore\", \"ÂƠ\", \"Ġafter\"),\n",
    "\n",
    "        # ---------- additions: SentencePiece mixes ----------\n",
    "        # SP marker + word + punct — should be filtered\n",
    "        (\"▁\", \"Hello\", \",\"),\n",
    "        # SP \"<unk>\" + word + newline — should be filtered\n",
    "        (\"▁<unk>\", \"Ġtoken\", \"Ċ\"),\n",
    "        # Three SP-prefixed words — should be kept\n",
    "        (\"▁we\", \"▁are\", \"▁testing\"),\n",
    "        # Two SP markers then a word — should be filtered\n",
    "        (\"▁\", \"▁\", \"word\"),\n",
    "\n",
    "        # ---------- additions: mostly/all special ----------\n",
    "        # Double newline then a word — may be filtered\n",
    "        (\"Ċ\", \"Ċ\", \"Ġstart\"),\n",
    "        # SP marker + newline + SP marker — should be filtered\n",
    "        (\"▁\", \"Ċ\", \"▁\"),\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bfbe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_case_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb019cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_ngrams(edge_case_ngrams, special_tokens=special_tokens)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
