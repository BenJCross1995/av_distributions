{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d0722c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Any, Dict, List, Sequence, Set, Tuple, Optional, Iterable\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "866529ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('../../src'))\n",
    "\n",
    "from read_and_write_docs import read_jsonl, read_rds, write_jsonl\n",
    "from tokenize_and_score import load_model\n",
    "from utils import apply_temp_doc_id, build_metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b168c327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_ngrams(\n",
    "    text1: str,\n",
    "    text2: str,\n",
    "    n: int,\n",
    "    model: Any = None,\n",
    "    tokenizer: Any = None,\n",
    "    include_subgrams: bool = False,\n",
    "    lowercase: bool = True,\n",
    ") -> Dict[int, Set[Tuple[Any, ...]]]:\n",
    "    \"\"\"\n",
    "    Return shared n-grams of length >= n between two texts.\n",
    "\n",
    "    If include_subgrams is False (default), remove any shared n-gram that is a\n",
    "    contiguous subspan of a longer shared n-gram. (So a 5-gram that’s part of a\n",
    "    shared 6-gram is excluded; unrelated 5-grams remain.)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lowercase : bool, default True\n",
    "        If True, normalize text using str.casefold() before tokenization.\n",
    "        Applies to both the simple regex tokenization path and the Hugging Face\n",
    "        tokenizer path (by case-folding the raw text before calling the tokenizer).\n",
    "    \"\"\"\n",
    "    if n < 1:\n",
    "        raise ValueError(\"n must be >= 1\")\n",
    "\n",
    "    def _word_tokens(s: str) -> List[str]:\n",
    "        s2 = s.casefold() if lowercase else s\n",
    "        return re.findall(r\"\\w+\", s2)\n",
    "\n",
    "    def _hf_tokens(txt: str) -> List[Any]:\n",
    "        src = txt.casefold() if lowercase else txt\n",
    "        if hasattr(tokenizer, \"tokenize\"):\n",
    "            return list(tokenizer.tokenize(src))\n",
    "        enc = tokenizer(\n",
    "            src,\n",
    "            add_special_tokens=False,\n",
    "            return_attention_mask=False,\n",
    "            return_token_type_ids=False,\n",
    "        )\n",
    "        input_ids = enc.get(\"input_ids\", [])\n",
    "        if input_ids and isinstance(input_ids[0], (list, tuple)):\n",
    "            input_ids = input_ids[0]\n",
    "        if hasattr(tokenizer, \"convert_ids_to_tokens\"):\n",
    "            return tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        return input_ids\n",
    "\n",
    "    def _ngrams_by_len(seq: Sequence[Any], min_n: int) -> Dict[int, Set[Tuple[Any, ...]]]:\n",
    "        out: Dict[int, Set[Tuple[Any, ...]]] = {}\n",
    "        L = len(seq)\n",
    "        for k in range(min_n, L + 1):\n",
    "            s: Set[Tuple[Any, ...]] = set()\n",
    "            for i in range(0, L - k + 1):\n",
    "                s.add(tuple(seq[i : i + k]))\n",
    "            if s:\n",
    "                out[k] = s\n",
    "        return out\n",
    "\n",
    "    token_mode = (model is not None) and (tokenizer is not None)\n",
    "    seq1 = _hf_tokens(text1) if token_mode else _word_tokens(text1)\n",
    "    seq2 = _hf_tokens(text2) if token_mode else _word_tokens(text2)\n",
    "\n",
    "    ngrams1 = _ngrams_by_len(seq1, n)\n",
    "    ngrams2 = _ngrams_by_len(seq2, n)\n",
    "\n",
    "    common: Dict[int, Set[Tuple[Any, ...]]] = {}\n",
    "    for k in set(ngrams1.keys()).intersection(ngrams2.keys()):\n",
    "        inter = ngrams1[k] & ngrams2[k]\n",
    "        if inter:\n",
    "            common[k] = inter\n",
    "\n",
    "    if include_subgrams or not common:\n",
    "        return common\n",
    "\n",
    "    # Remove n-grams that are contiguous subspans of any longer shared n-gram\n",
    "    to_remove: Dict[int, Set[Tuple[Any, ...]]] = defaultdict(set)\n",
    "    lengths = sorted(common.keys())\n",
    "    for k in lengths:\n",
    "        # For each longer length, generate all contiguous subspans down to n\n",
    "        for longer_k in [L for L in lengths if L > k]:\n",
    "            for g in common[longer_k]:\n",
    "                # produce all subspans of length k from g\n",
    "                for i in range(0, longer_k - k + 1):\n",
    "                    to_remove[k].add(g[i : i + k])\n",
    "\n",
    "    # Apply removals\n",
    "    for k, rem in to_remove.items():\n",
    "        if k in common:\n",
    "            common[k] = {g for g in common[k] if g not in rem}\n",
    "            if not common[k]:\n",
    "                del common[k]\n",
    "\n",
    "    return common\n",
    "\n",
    "def pretty_print_common_ngrams(\n",
    "    common: Dict[int, Set[Tuple[Any, ...]]],\n",
    "    sep: str = \" \",\n",
    "    order: str = \"count_desc\",  # \"count_desc\" | \"len_asc\" | \"len_desc\"\n",
    "    tokenizer=None,             # Optional HuggingFace tokenizer\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Pretty-print shared n-grams.\n",
    "\n",
    "    - Groups by n (the integer length).\n",
    "    - If `tokenizer` is None: converts each n-gram tuple into a string joined by `sep` (original behavior).\n",
    "    - If `tokenizer` is provided: decodes token ids/strings to readable text (special tokens removed).\n",
    "    - Prints lists, ordered by the number of n-grams per length (descending) by default.\n",
    "    \"\"\"\n",
    "    if not common:\n",
    "        print(\"{}\")\n",
    "        return\n",
    "\n",
    "    def stringify_ngram(ngram: Tuple[Any, ...]) -> str:\n",
    "        # Original behavior (no tokenizer): join items with sep\n",
    "        if tokenizer is None:\n",
    "            return sep.join(map(str, ngram))\n",
    "\n",
    "        # With tokenizer: decode to human-readable text\n",
    "        toks = list(ngram)\n",
    "\n",
    "        # If everything is ids, use fast decode\n",
    "        if all(isinstance(t, int) for t in toks):\n",
    "            return tokenizer.decode(\n",
    "                toks,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=False,\n",
    "            )\n",
    "\n",
    "        # Otherwise, we may have token *strings* or a mix of ids & strings\n",
    "        specials = set(getattr(tokenizer, \"all_special_tokens\", []))\n",
    "        norm_tokens: List[str] = []\n",
    "        for t in toks:\n",
    "            if isinstance(t, int):\n",
    "                # convert id -> token string\n",
    "                norm_tokens.append(tokenizer.convert_ids_to_tokens(t))\n",
    "            else:\n",
    "                norm_tokens.append(str(t))\n",
    "\n",
    "        # Drop special tokens (e.g., <s>, </s>)\n",
    "        norm_tokens = [t for t in norm_tokens if t not in specials]\n",
    "\n",
    "        # Let the tokenizer handle spacing/newlines between tokens\n",
    "        return tokenizer.convert_tokens_to_string(norm_tokens)\n",
    "\n",
    "    # Convert tuples to strings per length key\n",
    "    grouped: Dict[int, List[str]] = {\n",
    "        n: sorted(stringify_ngram(g) for g in grams)\n",
    "        for n, grams in common.items()\n",
    "    }\n",
    "\n",
    "    # Choose group ordering\n",
    "    if order == \"count_desc\":\n",
    "        items = sorted(grouped.items(), key=lambda kv: (-len(kv[1]), kv[0]))\n",
    "    elif order == \"len_asc\":\n",
    "        items = sorted(grouped.items(), key=lambda kv: kv[0])\n",
    "    elif order == \"len_desc\":\n",
    "        items = sorted(grouped.items(), key=lambda kv: -kv[0])\n",
    "    else:\n",
    "        items = sorted(grouped.items(), key=lambda kv: (-len(kv[1]), kv[0]))\n",
    "\n",
    "    # Print: e.g., \"3-grams (5): ['a b c', 'd e f', ...]\"\n",
    "    for n, strings in items:\n",
    "        print(f\"{n}-grams ({len(strings)}): {strings}\")\n",
    "        \n",
    "def highest_common(common: Dict[int, Set[Tuple[Any, ...]]]) -> Tuple[int, Set[Tuple[Any, ...]]]:\n",
    "    \"\"\"\n",
    "    Given the dict returned by `common_ngrams`, return (max_n, ngrams_at_max).\n",
    "    If there are none, returns (0, empty set).\n",
    "    \"\"\"\n",
    "    if not common:\n",
    "        return 0, set()\n",
    "    max_k = max(common.keys())\n",
    "    return max_k, common[max_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "878ef607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def largest_common_ngram_problems(\n",
    "    metadata: pd.DataFrame,\n",
    "    known: pd.DataFrame,\n",
    "    unknown: pd.DataFrame,\n",
    "    n: int,\n",
    "    model: Any = None,\n",
    "    tokenizer: Any = None,\n",
    "    print_progress: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each metadata row (keeps, problem, known_author, unknown_author, known_doc_id, unknown_doc_id),\n",
    "    filter `known`/`unknown` by doc_id, take text via .reset_index().loc[0, 'text'],\n",
    "    compute common_ngrams(...) then highest_common(common), and extract:\n",
    "      - highes_common_count: the number (first element)\n",
    "      - highes_common_ngram: the n-gram as a space-joined string\n",
    "\n",
    "    Returns columns:\n",
    "      ['keeps','problem','known_author','unknown_author','known_doc_id','unknown_doc_id',\n",
    "       'highes_common_count','highes_common_ngram']\n",
    "    \"\"\"\n",
    "    required_meta_cols = [\n",
    "        \"problem\", \"known_author\", \"unknown_author\",\n",
    "        \"known_doc_id\", \"unknown_doc_id\",\n",
    "    ]\n",
    "    missing_meta = [c for c in required_meta_cols if c not in metadata.columns]\n",
    "    if missing_meta:\n",
    "        raise ValueError(f\"metadata missing columns: {missing_meta}\")\n",
    "\n",
    "    for df_name, df in [(\"known\", known), (\"unknown\", unknown)]:\n",
    "        if \"doc_id\" not in df.columns:\n",
    "            raise ValueError(f\"'{df_name}' is missing required column 'doc_id'\")\n",
    "        if \"text\" not in df.columns:\n",
    "            raise ValueError(f\"'{df_name}' is missing required column 'text'\")\n",
    "\n",
    "    def _pick_ngram_string(ngrams: Any) -> str:\n",
    "        \"\"\"\n",
    "        Accepts one of:\n",
    "          - a tuple/list of tokens (single n-gram),\n",
    "          - a set/list of n-gram tuples (choose deterministic first),\n",
    "          - an already-joined string.\n",
    "        Returns a single space-joined n-gram string.\n",
    "        \"\"\"\n",
    "        # If we received a collection of n-grams, pick a deterministic one\n",
    "        if isinstance(ngrams, (set, list, tuple)) and ngrams and isinstance(next(iter(ngrams)), (tuple, list, str)):\n",
    "            # If it's a set/list of tuples/lists/strings, sort deterministically\n",
    "            if isinstance(ngrams, (set, list)) and ngrams and not isinstance(ngrams, str):\n",
    "                try:\n",
    "                    candidate = sorted(ngrams)[0]\n",
    "                except Exception:\n",
    "                    candidate = next(iter(ngrams))\n",
    "            else:\n",
    "                candidate = ngrams  # already a single n-gram tuple/list/str\n",
    "        else:\n",
    "            candidate = ngrams\n",
    "\n",
    "        # If candidate is a sequence of tokens, join with spaces; otherwise cast to str\n",
    "        if isinstance(candidate, (tuple, list)):\n",
    "            return \" \".join(map(str, candidate))\n",
    "        return str(candidate)\n",
    "\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    total = len(metadata)\n",
    "    it = metadata[required_meta_cols].itertuples(index=False, name=\"MetaRow\")\n",
    "\n",
    "    for i, row in enumerate(it, 1):\n",
    "        problem, known_author, unknown_author, known_doc_id, unknown_doc_id = row\n",
    "\n",
    "        kdf = known.loc[known[\"doc_id\"] == known_doc_id].reset_index(drop=True)\n",
    "        udf = unknown.loc[unknown[\"doc_id\"] == unknown_doc_id].reset_index(drop=True)\n",
    "\n",
    "        if kdf.empty or udf.empty:\n",
    "            count_val = None\n",
    "            ngram_str = None\n",
    "        else:\n",
    "            text_known = kdf.loc[0, \"text\"]\n",
    "            text_unknown = udf.loc[0, \"text\"]\n",
    "\n",
    "            # If your common_ngrams expects n, switch to: common_ngrams(text_known, text_unknown, n)\n",
    "            common = common_ngrams(text_known, text_unknown, n=n, model=model, tokenizer=tokenizer)\n",
    "            hc = highest_common(common)\n",
    "\n",
    "            if hc is None:\n",
    "                count_val = None\n",
    "                ngram_str = None\n",
    "            else:\n",
    "                # Expecting (number, ngrams)\n",
    "                try:\n",
    "                    count_val, ngrams_obj = hc\n",
    "                except Exception:\n",
    "                    # Fallback: treat whole object as the ngram payload and set count None\n",
    "                    count_val = None\n",
    "                    ngrams_obj = hc\n",
    "                ngram_str = _pick_ngram_string(ngrams_obj)\n",
    "\n",
    "        rows.append({\n",
    "            \"problem\": problem,\n",
    "            \"known_author\": known_author,\n",
    "            \"unknown_author\": unknown_author,\n",
    "            \"known_doc_id\": known_doc_id,\n",
    "            \"unknown_doc_id\": unknown_doc_id,\n",
    "            \"highest_common_count\": count_val,      # extracted number\n",
    "            \"highest_common_ngram\": ngram_str,      # tokens joined by ' '\n",
    "        })\n",
    "\n",
    "        if print_progress and total:\n",
    "            if (i % max(1, total // 50) == 0) or (i == total):\n",
    "                pct = int(i * 100 / total)\n",
    "                print(f\"\\rProcessed {i}/{total} ({pct}%)\", end=\"\")\n",
    "\n",
    "    if print_progress:\n",
    "        print()\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeed0874",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"Enron\"\n",
    "data_type = \"training\"\n",
    "\n",
    "known_loc = f\"/Volumes/BCross/datasets/author_verification/{data_type}/{corpus}/known_raw.jsonl\"\n",
    "known = read_jsonl(known_loc)\n",
    "known = apply_temp_doc_id(known)\n",
    "\n",
    "unknown_loc = f\"/Volumes/BCross/datasets/author_verification/{data_type}/{corpus}/unknown_raw.jsonl\"\n",
    "unknown = read_jsonl(unknown_loc)\n",
    "unknown_df = apply_temp_doc_id(unknown)\n",
    "\n",
    "metadata_loc = f\"/Volumes/BCross/datasets/author_verification/{data_type}/metadata.rds\"\n",
    "metadata = read_rds(metadata_loc)\n",
    "filtered_metadata = metadata[metadata['corpus'] == corpus]\n",
    "agg_metadata = build_metadata_df(filtered_metadata, known, unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f7a260b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>corpus</th>\n",
       "      <th>known_author</th>\n",
       "      <th>unknown_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3214</th>\n",
       "      <td>Andy.zipper vs Andy.zipper</td>\n",
       "      <td>Enron</td>\n",
       "      <td>Andy.zipper</td>\n",
       "      <td>Andy.zipper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3215</th>\n",
       "      <td>Andy.zipper vs Barry.tycholiz</td>\n",
       "      <td>Enron</td>\n",
       "      <td>Andy.zipper</td>\n",
       "      <td>Barry.tycholiz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3216</th>\n",
       "      <td>Barry.tycholiz vs Barry.tycholiz</td>\n",
       "      <td>Enron</td>\n",
       "      <td>Barry.tycholiz</td>\n",
       "      <td>Barry.tycholiz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3217</th>\n",
       "      <td>Barry.tycholiz vs Benjamin.rogers</td>\n",
       "      <td>Enron</td>\n",
       "      <td>Barry.tycholiz</td>\n",
       "      <td>Benjamin.rogers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3218</th>\n",
       "      <td>Benjamin.rogers vs Benjamin.rogers</td>\n",
       "      <td>Enron</td>\n",
       "      <td>Benjamin.rogers</td>\n",
       "      <td>Benjamin.rogers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 problem corpus     known_author  \\\n",
       "3214          Andy.zipper vs Andy.zipper  Enron      Andy.zipper   \n",
       "3215       Andy.zipper vs Barry.tycholiz  Enron      Andy.zipper   \n",
       "3216    Barry.tycholiz vs Barry.tycholiz  Enron   Barry.tycholiz   \n",
       "3217   Barry.tycholiz vs Benjamin.rogers  Enron   Barry.tycholiz   \n",
       "3218  Benjamin.rogers vs Benjamin.rogers  Enron  Benjamin.rogers   \n",
       "\n",
       "       unknown_author  \n",
       "3214      Andy.zipper  \n",
       "3215   Barry.tycholiz  \n",
       "3216   Barry.tycholiz  \n",
       "3217  Benjamin.rogers  \n",
       "3218  Benjamin.rogers  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8681cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = load_model(\"/Volumes/BCross/models/Qwen 2.5/Qwen2.5-0.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "648d8882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# problems = largest_common_ngram_problems(\n",
    "#     agg_metadata,\n",
    "#     known,\n",
    "#     unknown,\n",
    "#     n=2,\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer\n",
    "# )\n",
    "\n",
    "# write_jsonl(problems, f\"/Users/user/Documents/test_data/n-gram_tracing/{corpus}_{data_type}_agg.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c31b7214",
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = read_jsonl(f\"/Users/user/Documents/test_data/n-gram_tracing/{corpus}_{data_type}_agg.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f593c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>known_author</th>\n",
       "      <th>unknown_author</th>\n",
       "      <th>known_doc_id</th>\n",
       "      <th>unknown_doc_id</th>\n",
       "      <th>highest_common_count</th>\n",
       "      <th>highest_common_ngram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Andy.zipper vs Andy.zipper</td>\n",
       "      <td>Andy.zipper</td>\n",
       "      <td>Andy.zipper</td>\n",
       "      <td>andy_zipper_mail_1</td>\n",
       "      <td>andy_zipper_mail_2</td>\n",
       "      <td>5</td>\n",
       "      <td>. Ġi Ġam Ġworking Ġon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andy.zipper vs Andy.zipper</td>\n",
       "      <td>Andy.zipper</td>\n",
       "      <td>Andy.zipper</td>\n",
       "      <td>andy_zipper_mail_3</td>\n",
       "      <td>andy_zipper_mail_2</td>\n",
       "      <td>6</td>\n",
       "      <td>Ġin Ġh ouston , Ġbut Ġi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Andy.zipper vs Andy.zipper</td>\n",
       "      <td>Andy.zipper</td>\n",
       "      <td>Andy.zipper</td>\n",
       "      <td>andy_zipper_mail_4</td>\n",
       "      <td>andy_zipper_mail_2</td>\n",
       "      <td>5</td>\n",
       "      <td>Ġare Ġtrying Ġto Ġaccomplish .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Andy.zipper vs Andy.zipper</td>\n",
       "      <td>Andy.zipper</td>\n",
       "      <td>Andy.zipper</td>\n",
       "      <td>andy_zipper_mail_5</td>\n",
       "      <td>andy_zipper_mail_2</td>\n",
       "      <td>4</td>\n",
       "      <td>. Ġi Ġthink Ġthe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Andy.zipper vs Barry.tycholiz</td>\n",
       "      <td>Andy.zipper</td>\n",
       "      <td>Barry.tycholiz</td>\n",
       "      <td>andy_zipper_mail_1</td>\n",
       "      <td>barry_tycholiz_mail_2</td>\n",
       "      <td>5</td>\n",
       "      <td>. Ġi Ġdon 't Ġreally</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         problem known_author  unknown_author  \\\n",
       "0     Andy.zipper vs Andy.zipper  Andy.zipper     Andy.zipper   \n",
       "1     Andy.zipper vs Andy.zipper  Andy.zipper     Andy.zipper   \n",
       "2     Andy.zipper vs Andy.zipper  Andy.zipper     Andy.zipper   \n",
       "3     Andy.zipper vs Andy.zipper  Andy.zipper     Andy.zipper   \n",
       "4  Andy.zipper vs Barry.tycholiz  Andy.zipper  Barry.tycholiz   \n",
       "\n",
       "         known_doc_id         unknown_doc_id  highest_common_count  \\\n",
       "0  andy_zipper_mail_1     andy_zipper_mail_2                     5   \n",
       "1  andy_zipper_mail_3     andy_zipper_mail_2                     6   \n",
       "2  andy_zipper_mail_4     andy_zipper_mail_2                     5   \n",
       "3  andy_zipper_mail_5     andy_zipper_mail_2                     4   \n",
       "4  andy_zipper_mail_1  barry_tycholiz_mail_2                     5   \n",
       "\n",
       "             highest_common_ngram  \n",
       "0           . Ġi Ġam Ġworking Ġon  \n",
       "1         Ġin Ġh ouston , Ġbut Ġi  \n",
       "2  Ġare Ġtrying Ġto Ġaccomplish .  \n",
       "3                . Ġi Ġthink Ġthe  \n",
       "4            . Ġi Ġdon 't Ġreally  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problems.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03b2acef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>known_author</th>\n",
       "      <th>unknown_author</th>\n",
       "      <th>known_doc_id</th>\n",
       "      <th>unknown_doc_id</th>\n",
       "      <th>highest_common_count</th>\n",
       "      <th>highest_common_ngram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Benjamin.rogers vs Benjamin.rogers</td>\n",
       "      <td>Benjamin.rogers</td>\n",
       "      <td>Benjamin.rogers</td>\n",
       "      <td>benjamin_rogers_mail_2</td>\n",
       "      <td>benjamin_rogers_mail_1</td>\n",
       "      <td>13</td>\n",
       "      <td>. Ġplease Ġgive Ġus Ġa Ġcall Ġif Ġyou Ġhave Ġa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Elizabeth.sager vs Elizabeth.sager</td>\n",
       "      <td>Elizabeth.sager</td>\n",
       "      <td>Elizabeth.sager</td>\n",
       "      <td>elizabeth_sager_mail_1</td>\n",
       "      <td>elizabeth_sager_mail_5</td>\n",
       "      <td>13</td>\n",
       "      <td>? Ġhope Ġall Ġis Ġwell Ġand Ġi 'll Ġtalk Ġto Ġ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Jeff.skilling vs Jeff.skilling</td>\n",
       "      <td>Jeff.skilling</td>\n",
       "      <td>Jeff.skilling</td>\n",
       "      <td>jeff_skilling_mail_3</td>\n",
       "      <td>jeff_skilling_mail_1</td>\n",
       "      <td>11</td>\n",
       "      <td>Ġd ottie Ġk err Ġd ottie Ġk err -s olutions .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Benjamin.rogers vs Benjamin.rogers</td>\n",
       "      <td>Benjamin.rogers</td>\n",
       "      <td>Benjamin.rogers</td>\n",
       "      <td>benjamin_rogers_mail_4</td>\n",
       "      <td>benjamin_rogers_mail_1</td>\n",
       "      <td>10</td>\n",
       "      <td>. Ġplease Ġlet Ġme Ġknow Ġif Ġyou Ġhave Ġany Ġ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>Kam.keiser vs Kam.keiser</td>\n",
       "      <td>Kam.keiser</td>\n",
       "      <td>Kam.keiser</td>\n",
       "      <td>kam_keiser_mail_2</td>\n",
       "      <td>kam_keiser_mail_4</td>\n",
       "      <td>9</td>\n",
       "      <td>. Ġlet Ġme Ġknow Ġif Ġthere Ġis Ġanything Ġelse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>D.thomas vs D.thomas</td>\n",
       "      <td>D.thomas</td>\n",
       "      <td>D.thomas</td>\n",
       "      <td>d_thomas_mail_5</td>\n",
       "      <td>d_thomas_mail_3</td>\n",
       "      <td>9</td>\n",
       "      <td>a , Ġhow Ġare Ġyou Ġdoing Ġthis Ġmorning ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>Kam.keiser vs Kam.keiser</td>\n",
       "      <td>Kam.keiser</td>\n",
       "      <td>Kam.keiser</td>\n",
       "      <td>kam_keiser_mail_3</td>\n",
       "      <td>kam_keiser_mail_4</td>\n",
       "      <td>9</td>\n",
       "      <td>Ġbut Ġi Ġwanted Ġto Ġget Ġyou Ġsomething Ġto Ġ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Daren.farmer vs Daren.farmer</td>\n",
       "      <td>Daren.farmer</td>\n",
       "      <td>Daren.farmer</td>\n",
       "      <td>daren_farmer_mail_2</td>\n",
       "      <td>daren_farmer_mail_3</td>\n",
       "      <td>8</td>\n",
       "      <td>Ġhave Ġ 1 0 , 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Errol.mclaughlin vs Errol.mclaughlin</td>\n",
       "      <td>Errol.mclaughlin</td>\n",
       "      <td>Errol.mclaughlin</td>\n",
       "      <td>errol_mclaughlin_mail_1</td>\n",
       "      <td>errol_mclaughlin_mail_3</td>\n",
       "      <td>8</td>\n",
       "      <td>9 , Ġ 2 0 0 1 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Errol.mclaughlin vs Errol.mclaughlin</td>\n",
       "      <td>Errol.mclaughlin</td>\n",
       "      <td>Errol.mclaughlin</td>\n",
       "      <td>errol_mclaughlin_mail_5</td>\n",
       "      <td>errol_mclaughlin_mail_3</td>\n",
       "      <td>8</td>\n",
       "      <td>9 , Ġ 2 0 0 1 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>Kam.keiser vs Kam.keiser</td>\n",
       "      <td>Kam.keiser</td>\n",
       "      <td>Kam.keiser</td>\n",
       "      <td>kam_keiser_mail_1</td>\n",
       "      <td>kam_keiser_mail_4</td>\n",
       "      <td>8</td>\n",
       "      <td>Ġlet Ġme Ġknow Ġif Ġyou Ġhave Ġany Ġquestions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Errol.mclaughlin vs Errol.mclaughlin</td>\n",
       "      <td>Errol.mclaughlin</td>\n",
       "      <td>Errol.mclaughlin</td>\n",
       "      <td>errol_mclaughlin_mail_2</td>\n",
       "      <td>errol_mclaughlin_mail_3</td>\n",
       "      <td>7</td>\n",
       "      <td>, Ġ 2 0 0 1 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>K.allen vs K.allen</td>\n",
       "      <td>K.allen</td>\n",
       "      <td>K.allen</td>\n",
       "      <td>k_allen_mail_3</td>\n",
       "      <td>k_allen_mail_2</td>\n",
       "      <td>7</td>\n",
       "      <td>0 0 , 0 0 0 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Bill.williams vs Bill.williams</td>\n",
       "      <td>Bill.williams</td>\n",
       "      <td>Bill.williams</td>\n",
       "      <td>bill_williams_mail_4</td>\n",
       "      <td>bill_williams_mail_1</td>\n",
       "      <td>7</td>\n",
       "      <td>Ġme Ġwhen Ġyou Ġhave Ġa Ġchance .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Daren.farmer vs Daren.farmer</td>\n",
       "      <td>Daren.farmer</td>\n",
       "      <td>Daren.farmer</td>\n",
       "      <td>daren_farmer_mail_5</td>\n",
       "      <td>daren_farmer_mail_3</td>\n",
       "      <td>6</td>\n",
       "      <td>Ġ 2 0 0 1 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Jane.tholt vs Jane.tholt</td>\n",
       "      <td>Jane.tholt</td>\n",
       "      <td>Jane.tholt</td>\n",
       "      <td>jane_tholt_mail_2</td>\n",
       "      <td>jane_tholt_mail_4</td>\n",
       "      <td>6</td>\n",
       "      <td>Ġa Ġcouple Ġof Ġweeks Ġago .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Darron.giron vs Darron.giron</td>\n",
       "      <td>Darron.giron</td>\n",
       "      <td>Darron.giron</td>\n",
       "      <td>darron_giron_mail_3</td>\n",
       "      <td>darron_giron_mail_1</td>\n",
       "      <td>6</td>\n",
       "      <td>. Ġlet Ġme Ġknow Ġif Ġyou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Darron.giron vs Darron.giron</td>\n",
       "      <td>Darron.giron</td>\n",
       "      <td>Darron.giron</td>\n",
       "      <td>darron_giron_mail_4</td>\n",
       "      <td>darron_giron_mail_1</td>\n",
       "      <td>6</td>\n",
       "      <td>. Ġlet Ġme Ġknow Ġif Ġyou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>D.thomas vs D.thomas</td>\n",
       "      <td>D.thomas</td>\n",
       "      <td>D.thomas</td>\n",
       "      <td>d_thomas_mail_1</td>\n",
       "      <td>d_thomas_mail_3</td>\n",
       "      <td>6</td>\n",
       "      <td>Ġ 2 0 0 1 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Debra.perlingiere vs Debra.perlingiere</td>\n",
       "      <td>Debra.perlingiere</td>\n",
       "      <td>Debra.perlingiere</td>\n",
       "      <td>debra_perlingiere_mail_3</td>\n",
       "      <td>debra_perlingiere_mail_2</td>\n",
       "      <td>6</td>\n",
       "      <td>, Ġeach Ġof Ġwhich Ġshall Ġbe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andy.zipper vs Andy.zipper</td>\n",
       "      <td>Andy.zipper</td>\n",
       "      <td>Andy.zipper</td>\n",
       "      <td>andy_zipper_mail_3</td>\n",
       "      <td>andy_zipper_mail_2</td>\n",
       "      <td>6</td>\n",
       "      <td>Ġin Ġh ouston , Ġbut Ġi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Chris.dorland vs Chris.dorland</td>\n",
       "      <td>Chris.dorland</td>\n",
       "      <td>Chris.dorland</td>\n",
       "      <td>chris_dorland_mail_4</td>\n",
       "      <td>chris_dorland_mail_3</td>\n",
       "      <td>6</td>\n",
       "      <td>Ġwhen Ġyou Ġget Ġa Ġminute .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>Kate.symes vs Kate.symes</td>\n",
       "      <td>Kate.symes</td>\n",
       "      <td>Kate.symes</td>\n",
       "      <td>kate_symes_mail_1</td>\n",
       "      <td>kate_symes_mail_2</td>\n",
       "      <td>6</td>\n",
       "      <td>Ġ 1 0 0 Ġbottles Ġof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Barry.tycholiz vs Barry.tycholiz</td>\n",
       "      <td>Barry.tycholiz</td>\n",
       "      <td>Barry.tycholiz</td>\n",
       "      <td>barry_tycholiz_mail_1</td>\n",
       "      <td>barry_tycholiz_mail_2</td>\n",
       "      <td>6</td>\n",
       "      <td>. Ġat Ġthis Ġtime , Ġi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Barry.tycholiz vs Barry.tycholiz</td>\n",
       "      <td>Barry.tycholiz</td>\n",
       "      <td>Barry.tycholiz</td>\n",
       "      <td>barry_tycholiz_mail_5</td>\n",
       "      <td>barry_tycholiz_mail_2</td>\n",
       "      <td>6</td>\n",
       "      <td>Ġif Ġthere Ġare Ġany Ġquestions ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Carol.clair vs Carol.clair</td>\n",
       "      <td>Carol.clair</td>\n",
       "      <td>Carol.clair</td>\n",
       "      <td>carol_clair_mail_1</td>\n",
       "      <td>carol_clair_mail_2</td>\n",
       "      <td>6</td>\n",
       "      <td>. Ġif Ġyou Ġhave Ġany Ġquestions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>K.allen vs K.allen</td>\n",
       "      <td>K.allen</td>\n",
       "      <td>K.allen</td>\n",
       "      <td>k_allen_mail_1</td>\n",
       "      <td>k_allen_mail_2</td>\n",
       "      <td>6</td>\n",
       "      <td>Ġthe Ġ 1 st Ġlien holders</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    problem       known_author  \\\n",
       "16       Benjamin.rogers vs Benjamin.rogers    Benjamin.rogers   \n",
       "124      Elizabeth.sager vs Elizabeth.sager    Elizabeth.sager   \n",
       "175          Jeff.skilling vs Jeff.skilling      Jeff.skilling   \n",
       "18       Benjamin.rogers vs Benjamin.rogers    Benjamin.rogers   \n",
       "211                Kam.keiser vs Kam.keiser         Kam.keiser   \n",
       "68                     D.thomas vs D.thomas           D.thomas   \n",
       "212                Kam.keiser vs Kam.keiser         Kam.keiser   \n",
       "85             Daren.farmer vs Daren.farmer       Daren.farmer   \n",
       "132    Errol.mclaughlin vs Errol.mclaughlin   Errol.mclaughlin   \n",
       "135    Errol.mclaughlin vs Errol.mclaughlin   Errol.mclaughlin   \n",
       "210                Kam.keiser vs Kam.keiser         Kam.keiser   \n",
       "133    Errol.mclaughlin vs Errol.mclaughlin   Errol.mclaughlin   \n",
       "205                      K.allen vs K.allen            K.allen   \n",
       "24           Bill.williams vs Bill.williams      Bill.williams   \n",
       "87             Daren.farmer vs Daren.farmer       Daren.farmer   \n",
       "155                Jane.tholt vs Jane.tholt         Jane.tholt   \n",
       "99             Darron.giron vs Darron.giron       Darron.giron   \n",
       "100            Darron.giron vs Darron.giron       Darron.giron   \n",
       "66                     D.thomas vs D.thomas           D.thomas   \n",
       "113  Debra.perlingiere vs Debra.perlingiere  Debra.perlingiere   \n",
       "1                Andy.zipper vs Andy.zipper        Andy.zipper   \n",
       "46           Chris.dorland vs Chris.dorland      Chris.dorland   \n",
       "221                Kate.symes vs Kate.symes         Kate.symes   \n",
       "8          Barry.tycholiz vs Barry.tycholiz     Barry.tycholiz   \n",
       "11         Barry.tycholiz vs Barry.tycholiz     Barry.tycholiz   \n",
       "36               Carol.clair vs Carol.clair        Carol.clair   \n",
       "204                      K.allen vs K.allen            K.allen   \n",
       "\n",
       "        unknown_author              known_doc_id            unknown_doc_id  \\\n",
       "16     Benjamin.rogers    benjamin_rogers_mail_2    benjamin_rogers_mail_1   \n",
       "124    Elizabeth.sager    elizabeth_sager_mail_1    elizabeth_sager_mail_5   \n",
       "175      Jeff.skilling      jeff_skilling_mail_3      jeff_skilling_mail_1   \n",
       "18     Benjamin.rogers    benjamin_rogers_mail_4    benjamin_rogers_mail_1   \n",
       "211         Kam.keiser         kam_keiser_mail_2         kam_keiser_mail_4   \n",
       "68            D.thomas           d_thomas_mail_5           d_thomas_mail_3   \n",
       "212         Kam.keiser         kam_keiser_mail_3         kam_keiser_mail_4   \n",
       "85        Daren.farmer       daren_farmer_mail_2       daren_farmer_mail_3   \n",
       "132   Errol.mclaughlin   errol_mclaughlin_mail_1   errol_mclaughlin_mail_3   \n",
       "135   Errol.mclaughlin   errol_mclaughlin_mail_5   errol_mclaughlin_mail_3   \n",
       "210         Kam.keiser         kam_keiser_mail_1         kam_keiser_mail_4   \n",
       "133   Errol.mclaughlin   errol_mclaughlin_mail_2   errol_mclaughlin_mail_3   \n",
       "205            K.allen            k_allen_mail_3            k_allen_mail_2   \n",
       "24       Bill.williams      bill_williams_mail_4      bill_williams_mail_1   \n",
       "87        Daren.farmer       daren_farmer_mail_5       daren_farmer_mail_3   \n",
       "155         Jane.tholt         jane_tholt_mail_2         jane_tholt_mail_4   \n",
       "99        Darron.giron       darron_giron_mail_3       darron_giron_mail_1   \n",
       "100       Darron.giron       darron_giron_mail_4       darron_giron_mail_1   \n",
       "66            D.thomas           d_thomas_mail_1           d_thomas_mail_3   \n",
       "113  Debra.perlingiere  debra_perlingiere_mail_3  debra_perlingiere_mail_2   \n",
       "1          Andy.zipper        andy_zipper_mail_3        andy_zipper_mail_2   \n",
       "46       Chris.dorland      chris_dorland_mail_4      chris_dorland_mail_3   \n",
       "221         Kate.symes         kate_symes_mail_1         kate_symes_mail_2   \n",
       "8       Barry.tycholiz     barry_tycholiz_mail_1     barry_tycholiz_mail_2   \n",
       "11      Barry.tycholiz     barry_tycholiz_mail_5     barry_tycholiz_mail_2   \n",
       "36         Carol.clair        carol_clair_mail_1        carol_clair_mail_2   \n",
       "204            K.allen            k_allen_mail_1            k_allen_mail_2   \n",
       "\n",
       "     highest_common_count                               highest_common_ngram  \n",
       "16                     13  . Ġplease Ġgive Ġus Ġa Ġcall Ġif Ġyou Ġhave Ġa...  \n",
       "124                    13  ? Ġhope Ġall Ġis Ġwell Ġand Ġi 'll Ġtalk Ġto Ġ...  \n",
       "175                    11      Ġd ottie Ġk err Ġd ottie Ġk err -s olutions .  \n",
       "18                     10  . Ġplease Ġlet Ġme Ġknow Ġif Ġyou Ġhave Ġany Ġ...  \n",
       "211                     9    . Ġlet Ġme Ġknow Ġif Ġthere Ġis Ġanything Ġelse  \n",
       "68                      9         a , Ġhow Ġare Ġyou Ġdoing Ġthis Ġmorning ?  \n",
       "212                     9  Ġbut Ġi Ġwanted Ġto Ġget Ġyou Ġsomething Ġto Ġ...  \n",
       "85                      8                                Ġhave Ġ 1 0 , 0 0 0  \n",
       "132                     8                                    9 , Ġ 2 0 0 1 .  \n",
       "135                     8                                    9 , Ġ 2 0 0 1 .  \n",
       "210                     8      Ġlet Ġme Ġknow Ġif Ġyou Ġhave Ġany Ġquestions  \n",
       "133                     7                                      , Ġ 2 0 0 1 .  \n",
       "205                     7                                      0 0 , 0 0 0 .  \n",
       "24                      7                  Ġme Ġwhen Ġyou Ġhave Ġa Ġchance .  \n",
       "87                      6                                        Ġ 2 0 0 1 .  \n",
       "155                     6                       Ġa Ġcouple Ġof Ġweeks Ġago .  \n",
       "99                      6                          . Ġlet Ġme Ġknow Ġif Ġyou  \n",
       "100                     6                          . Ġlet Ġme Ġknow Ġif Ġyou  \n",
       "66                      6                                        Ġ 2 0 0 1 .  \n",
       "113                     6                      , Ġeach Ġof Ġwhich Ġshall Ġbe  \n",
       "1                       6                            Ġin Ġh ouston , Ġbut Ġi  \n",
       "46                      6                       Ġwhen Ġyou Ġget Ġa Ġminute .  \n",
       "221                     6                               Ġ 1 0 0 Ġbottles Ġof  \n",
       "8                       6                             . Ġat Ġthis Ġtime , Ġi  \n",
       "11                      6                  Ġif Ġthere Ġare Ġany Ġquestions ,  \n",
       "36                      6                   . Ġif Ġyou Ġhave Ġany Ġquestions  \n",
       "204                     6                          Ġthe Ġ 1 st Ġlien holders  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_author_problems = problems[problems['known_author'] == problems['unknown_author']].copy()\n",
    "same_author_problems.sort_values([\"highest_common_count\"], ascending=[False], inplace=True)\n",
    "same_author_problems[(same_author_problems['highest_common_count'] > 5) & (same_author_problems['highest_common_count'] <= 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "551b7315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>known_author</th>\n",
       "      <th>unknown_author</th>\n",
       "      <th>known_doc_id</th>\n",
       "      <th>unknown_doc_id</th>\n",
       "      <th>highest_common_count</th>\n",
       "      <th>highest_common_ngram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Jeffrey.shankman vs Joannie.williamson</td>\n",
       "      <td>Jeffrey.shankman</td>\n",
       "      <td>Joannie.williamson</td>\n",
       "      <td>jeffrey_shankman_mail_4</td>\n",
       "      <td>joannie_williamson_mail_4</td>\n",
       "      <td>8</td>\n",
       "      <td>. Ġif Ġyou Ġare Ġnot Ġthe Ġintended Ġrecipient</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Bill.williams vs Cara.semperger</td>\n",
       "      <td>Bill.williams</td>\n",
       "      <td>Cara.semperger</td>\n",
       "      <td>bill_williams_mail_4</td>\n",
       "      <td>cara_semperger_mail_4</td>\n",
       "      <td>8</td>\n",
       "      <td>. Ġplease Ġlet Ġme Ġknow Ġif Ġyou Ġhave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Elizabeth.sager vs Errol.mclaughlin</td>\n",
       "      <td>Elizabeth.sager</td>\n",
       "      <td>Errol.mclaughlin</td>\n",
       "      <td>elizabeth_sager_mail_2</td>\n",
       "      <td>errol_mclaughlin_mail_3</td>\n",
       "      <td>7</td>\n",
       "      <td>Ġi Ġwill Ġbe Ġout Ġof Ġthe Ġoffice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Jeffrey.shankman vs Joannie.williamson</td>\n",
       "      <td>Jeffrey.shankman</td>\n",
       "      <td>Joannie.williamson</td>\n",
       "      <td>jeffrey_shankman_mail_2</td>\n",
       "      <td>joannie_williamson_mail_4</td>\n",
       "      <td>7</td>\n",
       "      <td>Ġplease Ġdo Ġnot Ġhesitate Ġto Ġcontact Ġme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Darron.giron vs David.delainey</td>\n",
       "      <td>Darron.giron</td>\n",
       "      <td>David.delainey</td>\n",
       "      <td>darron_giron_mail_3</td>\n",
       "      <td>david_delainey_mail_3</td>\n",
       "      <td>6</td>\n",
       "      <td>Ġin Ġthe Ġnext Ġcouple Ġof Ġweeks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Cooper.richey vs D.steffes</td>\n",
       "      <td>Cooper.richey</td>\n",
       "      <td>D.steffes</td>\n",
       "      <td>cooper_richey_mail_1</td>\n",
       "      <td>d_steffes_mail_3</td>\n",
       "      <td>6</td>\n",
       "      <td>Ġand Ġi Ġwanted Ġto Ġmake Ġsure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>Kate.symes vs Andy.zipper</td>\n",
       "      <td>Kate.symes</td>\n",
       "      <td>Andy.zipper</td>\n",
       "      <td>kate_symes_mail_3</td>\n",
       "      <td>andy_zipper_mail_2</td>\n",
       "      <td>6</td>\n",
       "      <td>. Ġplease Ġlet Ġme Ġknow Ġif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Benjamin.rogers vs Bill.williams</td>\n",
       "      <td>Benjamin.rogers</td>\n",
       "      <td>Bill.williams</td>\n",
       "      <td>benjamin_rogers_mail_4</td>\n",
       "      <td>bill_williams_mail_1</td>\n",
       "      <td>6</td>\n",
       "      <td>Ġand Ġif Ġyou Ġhave Ġany Ġquestions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Elizabeth.sager vs Errol.mclaughlin</td>\n",
       "      <td>Elizabeth.sager</td>\n",
       "      <td>Errol.mclaughlin</td>\n",
       "      <td>elizabeth_sager_mail_1</td>\n",
       "      <td>errol_mclaughlin_mail_3</td>\n",
       "      <td>6</td>\n",
       "      <td>. Ġif Ġyou Ġhave Ġany Ġquestions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Dan.hyvl vs Dana.davis</td>\n",
       "      <td>Dan.hyvl</td>\n",
       "      <td>Dana.davis</td>\n",
       "      <td>dan_hyvl_mail_4</td>\n",
       "      <td>dana_davis_mail_1</td>\n",
       "      <td>5</td>\n",
       "      <td>Ġlet Ġme Ġknow Ġif Ġthere</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    problem      known_author  \\\n",
       "188  Jeffrey.shankman vs Joannie.williamson  Jeffrey.shankman   \n",
       "27          Bill.williams vs Cara.semperger     Bill.williams   \n",
       "129     Elizabeth.sager vs Errol.mclaughlin   Elizabeth.sager   \n",
       "186  Jeffrey.shankman vs Joannie.williamson  Jeffrey.shankman   \n",
       "103          Darron.giron vs David.delainey      Darron.giron   \n",
       "55               Cooper.richey vs D.steffes     Cooper.richey   \n",
       "219               Kate.symes vs Andy.zipper        Kate.symes   \n",
       "21         Benjamin.rogers vs Bill.williams   Benjamin.rogers   \n",
       "128     Elizabeth.sager vs Errol.mclaughlin   Elizabeth.sager   \n",
       "77                   Dan.hyvl vs Dana.davis          Dan.hyvl   \n",
       "\n",
       "         unknown_author             known_doc_id             unknown_doc_id  \\\n",
       "188  Joannie.williamson  jeffrey_shankman_mail_4  joannie_williamson_mail_4   \n",
       "27       Cara.semperger     bill_williams_mail_4      cara_semperger_mail_4   \n",
       "129    Errol.mclaughlin   elizabeth_sager_mail_2    errol_mclaughlin_mail_3   \n",
       "186  Joannie.williamson  jeffrey_shankman_mail_2  joannie_williamson_mail_4   \n",
       "103      David.delainey      darron_giron_mail_3      david_delainey_mail_3   \n",
       "55            D.steffes     cooper_richey_mail_1           d_steffes_mail_3   \n",
       "219         Andy.zipper        kate_symes_mail_3         andy_zipper_mail_2   \n",
       "21        Bill.williams   benjamin_rogers_mail_4       bill_williams_mail_1   \n",
       "128    Errol.mclaughlin   elizabeth_sager_mail_1    errol_mclaughlin_mail_3   \n",
       "77           Dana.davis          dan_hyvl_mail_4          dana_davis_mail_1   \n",
       "\n",
       "     highest_common_count                            highest_common_ngram  \n",
       "188                     8  . Ġif Ġyou Ġare Ġnot Ġthe Ġintended Ġrecipient  \n",
       "27                      8         . Ġplease Ġlet Ġme Ġknow Ġif Ġyou Ġhave  \n",
       "129                     7              Ġi Ġwill Ġbe Ġout Ġof Ġthe Ġoffice  \n",
       "186                     7     Ġplease Ġdo Ġnot Ġhesitate Ġto Ġcontact Ġme  \n",
       "103                     6               Ġin Ġthe Ġnext Ġcouple Ġof Ġweeks  \n",
       "55                      6                 Ġand Ġi Ġwanted Ġto Ġmake Ġsure  \n",
       "219                     6                    . Ġplease Ġlet Ġme Ġknow Ġif  \n",
       "21                      6             Ġand Ġif Ġyou Ġhave Ġany Ġquestions  \n",
       "128                     6                . Ġif Ġyou Ġhave Ġany Ġquestions  \n",
       "77                      5                       Ġlet Ġme Ġknow Ġif Ġthere  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_author_problems = problems[problems['known_author'] != problems['unknown_author']].copy()\n",
    "diff_author_problems.sort_values([\"highest_common_count\"], ascending=[False], inplace=True)\n",
    "diff_author_problems[(diff_author_problems['highest_common_count'] >= 5) & (diff_author_problems['highest_common_count'] <= 200)].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5304e543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-grams (28): [' a great', ' and other', ' and then', ' back in', ' e-mail', ' for the', ' i believe', ' jeff', ' me to', ' of our', ' of the', ' of this', ' on the', ' the sender', ' to be', ' to contact', ' to the', ' we will', ' week,', ' with the', ' would be', ' you have', ' you on', ', but', ', i', ', please', '. please', '9,']\n",
      "3-grams (6): [' 24', ' and any attachments', ' and will be', ' please do not', ' would like to', '. i will']\n",
      "4-grams (2): [' this message and any', ' this message, and']\n",
      "8-grams (1): ['. if you are not the intended recipient']\n"
     ]
    }
   ],
   "source": [
    "known_doc = 'jeffrey_shankman_mail_4'\n",
    "unknown_doc = 'joannie_williamson_mail_4'\n",
    "\n",
    "known_text = known[known['doc_id'] == known_doc].reset_index().loc[0, 'text']\n",
    "unknown_text = unknown[unknown['doc_id'] == unknown_doc].reset_index().loc[0, 'text']\n",
    "\n",
    "common_ngram_dict = common_ngrams(known_text, unknown_text, n=2, model=model, tokenizer=tokenizer)\n",
    "pretty_print_common_ngrams(common_ngram_dict, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9077394",
   "metadata": {},
   "source": [
    "## Combined Text Option\n",
    "\n",
    "We can also combine the texts by author in the known dataframe, we may just wish to compare these views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dd4c70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Sequence, Union\n",
    "import pandas as pd\n",
    "\n",
    "def concat_text_by_ids(\n",
    "    doc_ids: Iterable,\n",
    "    df: pd.DataFrame,\n",
    "    id_col: str = \"doc_id\",\n",
    "    text_col: str = \"text\",\n",
    "    sep: str = \"\\n\",\n",
    "    unique: bool = False,   # de-dupe doc_ids while preserving order\n",
    "    strict: bool = False,   # raise if any doc_id is missing\n",
    "    dropna: bool = True,    # drop rows where text is NaN\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Return a single string with texts for the given doc_ids concatenated by `sep`.\n",
    "\n",
    "    - Order follows `doc_ids`.\n",
    "    - If a doc_id matches multiple rows, their texts are joined by `sep` first,\n",
    "      then that block is joined into the overall result (also using `sep`).\n",
    "    \"\"\"\n",
    "    # Optionally de-duplicate the provided IDs, preserving order\n",
    "    if unique:\n",
    "        seen = set()\n",
    "        doc_ids = [d for d in doc_ids if not (d in seen or seen.add(d))]\n",
    "\n",
    "    # Optionally drop NaNs in the text column\n",
    "    if dropna:\n",
    "        df = df[df[text_col].notna()].copy()\n",
    "\n",
    "    # Build a mapping: doc_id -> [texts...], preserving row order\n",
    "    grouped = df.groupby(id_col, sort=False)[text_col].apply(list).to_dict()\n",
    "\n",
    "    parts = []\n",
    "    missing = []\n",
    "    for d in doc_ids:\n",
    "        texts = grouped.get(d)\n",
    "        if texts is None:\n",
    "            missing.append(d)\n",
    "            continue\n",
    "        parts.append(sep.join(str(t) for t in texts))\n",
    "\n",
    "    if strict and missing:\n",
    "        raise KeyError(f\"Missing doc_ids in dataframe: {missing}\")\n",
    "\n",
    "    # If not strict, we just skip missing IDs\n",
    "    return sep.join(parts)\n",
    "\n",
    "def concat_text_by_group(\n",
    "    df: pd.DataFrame,\n",
    "    group_cols: Union[str, Sequence[str]] = \"author\",\n",
    "    text_col: str = \"text\",\n",
    "    sep: str = \"\\n\",\n",
    "    dropna: bool = True,\n",
    "    keep_group_order: bool = True,              # keep first-seen group order\n",
    "    keep_row_order_within_group: bool = True,   # keep row order inside each group\n",
    "    output_col: str = \"concat_text\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Group `df` by one or more columns and concatenate each group's `text_col`\n",
    "    joined by `sep`. Returns a DataFrame with the group columns plus `output_col`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    group_cols : str | Sequence[str]\n",
    "        A single column name or a list/tuple of column names to group by.\n",
    "    \"\"\"\n",
    "    # Normalize group_cols to a list\n",
    "    if isinstance(group_cols, str):\n",
    "        group_cols = [group_cols]\n",
    "    else:\n",
    "        group_cols = list(group_cols)\n",
    "\n",
    "    # Validate columns\n",
    "    required = set(group_cols + [text_col])\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing columns in DataFrame: {missing}\")\n",
    "\n",
    "    # Minimal copy\n",
    "    df2 = df[group_cols + [text_col]].copy()\n",
    "\n",
    "    if dropna:\n",
    "        df2 = df2[df2[text_col].notna()]\n",
    "\n",
    "    # Ensure text is string\n",
    "    df2[text_col] = df2[text_col].astype(str)\n",
    "\n",
    "    # Control row order within groups\n",
    "    if not keep_row_order_within_group:\n",
    "        # Basic option: sort texts lexicographically within each group\n",
    "        df2 = df2.sort_values(group_cols + [text_col])\n",
    "\n",
    "    # Group and concatenate\n",
    "    out = (\n",
    "        df2.groupby(group_cols, sort=not keep_group_order)[text_col]\n",
    "           .apply(lambda s: sep.join(s.tolist()))\n",
    "           .reset_index()\n",
    "           .rename(columns={text_col: output_col})\n",
    "    )\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bded0f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_profile = concat_text_by_group(known, group_cols=['corpus', 'author', 'texttype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "278f534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# known_author = \"Jeffrey.shankman\"\n",
    "# unknown_author = \"Joannie.williamson\"\n",
    "\n",
    "# known_docs = known[known['author'] == known_author]['doc_id'].unique().tolist()\n",
    "# known_text = concat_text_by_ids(known_docs, known)\n",
    "\n",
    "# unknown_text = unknown[unknown['author'] == unknown_author].reset_index().loc[0, 'text']\n",
    "\n",
    "# common_ngram_dict = common_ngrams(known_text, unknown_text, n=2, model=model, tokenizer=tokenizer)\n",
    "# pretty_print_common_ngrams(common_ngram_dict, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd430440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "def largest_common_ngram_profile_problems(\n",
    "    metadata: pd.DataFrame,\n",
    "    known: pd.DataFrame,\n",
    "    unknown: pd.DataFrame,\n",
    "    n: int,\n",
    "    model: Any = None,\n",
    "    tokenizer: Any = None,\n",
    "    lowercase: bool = True,         # passed through to common_ngrams\n",
    "    include_subgrams: bool = False, # passed through to common_ngrams if supported\n",
    "    sep: str = \"\\n\",                # newline separator when concatenating\n",
    "    print_progress: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each metadata row (problem, known_author, unknown_author):\n",
    "      1) In `known`, filter to `known_author`, then group by (corpus, author, texttype).\n",
    "         Concatenate each group's texts (preserving row order) with `sep`, then\n",
    "         concatenate those group blobs (also with `sep`) to make ONE big `known_text`.\n",
    "      2) In `unknown`, filter to `unknown_author`. If multiple rows exist, concatenate with `sep`\n",
    "         to make ONE big `unknown_text`.\n",
    "      3) Compute common_ngrams(known_text, unknown_text, n, ...) and take the largest/common\n",
    "         profile via `highest_common`.\n",
    "\n",
    "    Returns columns in this exact order:\n",
    "      ['problem','known_author','unknown_author',\n",
    "       'known_text','unknown_text',\n",
    "       'highest_common_count','highest_common_ngram']\n",
    "    \"\"\"\n",
    "\n",
    "    # --- validations ---\n",
    "    required_meta = [\"problem\", \"known_author\", \"unknown_author\"]\n",
    "    miss_meta = [c for c in required_meta if c not in metadata.columns]\n",
    "    if miss_meta:\n",
    "        raise ValueError(f\"`metadata` missing columns: {miss_meta}\")\n",
    "\n",
    "    for df_name, df, req_cols in [\n",
    "        (\"known\", known, [\"corpus\", \"author\", \"texttype\", \"text\"]),\n",
    "        (\"unknown\", unknown, [\"author\", \"text\"]),\n",
    "    ]:\n",
    "        missing = [c for c in req_cols if c not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"`{df_name}` missing columns: {missing}\")\n",
    "\n",
    "    # --- helpers ---\n",
    "    def _pick_ngram_string(ngrams: Any) -> Optional[str]:\n",
    "        \"\"\"Pick a deterministic n-gram and join tokens with spaces.\"\"\"\n",
    "        if ngrams is None:\n",
    "            return None\n",
    "        candidate = ngrams\n",
    "        if isinstance(ngrams, (set, list)) and ngrams:\n",
    "            try:\n",
    "                candidate = sorted(ngrams)[0]\n",
    "            except Exception:\n",
    "                candidate = next(iter(ngrams))\n",
    "        if isinstance(candidate, (tuple, list)):\n",
    "            return \" \".join(map(str, candidate))\n",
    "        return str(candidate)\n",
    "\n",
    "    def _highest_common_fallback(common: Dict[int, Any]) -> Optional[Tuple[int, Any]]:\n",
    "        \"\"\"Fallback if `highest_common` isn't defined.\"\"\"\n",
    "        if not common:\n",
    "            return None\n",
    "        longest_n = max(common.keys())\n",
    "        grams = common[longest_n]\n",
    "        try:\n",
    "            count = len(grams)\n",
    "        except Exception:\n",
    "            count = None\n",
    "        return (count, grams)\n",
    "\n",
    "    _highest_common = globals().get(\"highest_common\", _highest_common_fallback)\n",
    "\n",
    "    def _concat_known_for_author(author: Any) -> Optional[str]:\n",
    "        ksub = known.loc[known[\"author\"] == author, [\"corpus\", \"author\", \"texttype\", \"text\"]].copy()\n",
    "        if ksub.empty:\n",
    "            return None\n",
    "        ksub = ksub[ksub[\"text\"].notna()]\n",
    "        if ksub.empty:\n",
    "            return None\n",
    "        ksub[\"text\"] = ksub[\"text\"].astype(str)\n",
    "\n",
    "        parts: List[str] = []\n",
    "        # First-seen order for groups and rows within groups\n",
    "        for _, g in ksub.groupby([\"corpus\", \"author\", \"texttype\"], sort=False):\n",
    "            grp_text = sep.join(g[\"text\"].tolist())\n",
    "            if grp_text:\n",
    "                parts.append(grp_text)\n",
    "        return sep.join(parts) if parts else None\n",
    "\n",
    "    def _concat_unknown_for_author(author: Any) -> Optional[str]:\n",
    "        usub = unknown.loc[unknown[\"author\"] == author, [\"author\", \"text\"]].copy()\n",
    "        if usub.empty:\n",
    "            return None\n",
    "        usub = usub[usub[\"text\"].notna()]\n",
    "        if usub.empty:\n",
    "            return None\n",
    "        usub[\"text\"] = usub[\"text\"].astype(str)\n",
    "        return sep.join(usub[\"text\"].tolist())\n",
    "\n",
    "    # --- main loop ---\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    total = len(metadata)\n",
    "    it = metadata[required_meta].itertuples(index=False, name=\"MetaRow\")\n",
    "\n",
    "    for i, meta_row in enumerate(it, 1):\n",
    "        problem, known_author, unknown_author = meta_row\n",
    "\n",
    "        known_text   = _concat_known_for_author(known_author)\n",
    "        unknown_text = _concat_unknown_for_author(unknown_author)\n",
    "\n",
    "        if not known_text or not unknown_text:\n",
    "            count_val = None\n",
    "            ngram_str = None\n",
    "        else:\n",
    "            # Compute common n-grams\n",
    "            try:\n",
    "                common = common_ngrams(\n",
    "                    known_text,\n",
    "                    unknown_text,\n",
    "                    n=n,\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                    include_subgrams=include_subgrams,\n",
    "                    lowercase=lowercase,  # requires your updated common_ngrams\n",
    "                )\n",
    "            except TypeError:\n",
    "                # Backward-compat if your common_ngrams doesn't take lowercase/include_subgrams\n",
    "                common = common_ngrams(\n",
    "                    known_text,\n",
    "                    unknown_text,\n",
    "                    n=n,\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                )\n",
    "\n",
    "            hc = _highest_common(common) if common else None\n",
    "            if hc is None:\n",
    "                count_val = None\n",
    "                ngram_str = None\n",
    "            else:\n",
    "                try:\n",
    "                    count_val, ngrams_obj = hc\n",
    "                except Exception:\n",
    "                    count_val = None\n",
    "                    ngrams_obj = hc\n",
    "                ngram_str = _pick_ngram_string(ngrams_obj)\n",
    "\n",
    "        rows.append({\n",
    "            \"problem\": problem,\n",
    "            \"known_author\": known_author,\n",
    "            \"unknown_author\": unknown_author,\n",
    "            \"known_text\": known_text,\n",
    "            \"unknown_text\": unknown_text,\n",
    "            \"highest_common_count\": count_val,\n",
    "            \"highest_common_ngram\": ngram_str,\n",
    "        })\n",
    "\n",
    "        if print_progress and total:\n",
    "            if (i % max(1, total // 50) == 0) or (i == total):\n",
    "                pct = int(i * 100 / total)\n",
    "                print(f\"\\rProcessed {i}/{total} ({pct}%)\", end=\"\")\n",
    "\n",
    "    if print_progress:\n",
    "        print()\n",
    "\n",
    "    # Ensure column order as requested\n",
    "    cols = [\n",
    "        \"problem\", \"known_author\", \"unknown_author\",\n",
    "        \"known_text\", \"unknown_text\",\n",
    "        \"highest_common_count\", \"highest_common_ngram\",\n",
    "    ]\n",
    "    df_out = pd.DataFrame(rows)\n",
    "    return df_out.reindex(columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443c6c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_problems = largest_common_ngram_profile_problems(\n",
    "    metadata=filtered_metadata,\n",
    "    known=known,\n",
    "    unknown=unknown,\n",
    "    n=2,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    lowercase=True,\n",
    "    include_subgrams=False,\n",
    "    sep=\"\\n\",\n",
    "    print_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66866f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl(profile_problems, f\"/Users/user/Documents/test_data/n-gram_tracing/{corpus}_{data_type}_profile.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77da8091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>known_author</th>\n",
       "      <th>unknown_author</th>\n",
       "      <th>known_text</th>\n",
       "      <th>unknown_text</th>\n",
       "      <th>highest_common_count</th>\n",
       "      <th>highest_common_ngram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>142.196.88.228 vs 142.196.88.228</td>\n",
       "      <td>142.196.88.228</td>\n",
       "      <td>142.196.88.228</td>\n",
       "      <td>The article that is being referred to via the ...</td>\n",
       "      <td>Furthermore, given the nearly parallel emergen...</td>\n",
       "      <td>89</td>\n",
       "      <td>Ġlove Ġthe Ġway Ġthe Ġarticle Ġincludes Ġthe Ġ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>142.196.88.228 vs Aban1313</td>\n",
       "      <td>142.196.88.228</td>\n",
       "      <td>Aban1313</td>\n",
       "      <td>The article that is being referred to via the ...</td>\n",
       "      <td>Leopold Amery also notes in his diary on Aug 4...</td>\n",
       "      <td>3</td>\n",
       "      <td>.Ċ also ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_Man_In_Black vs A_Man_In_Black</td>\n",
       "      <td>A_Man_In_Black</td>\n",
       "      <td>A_Man_In_Black</td>\n",
       "      <td>Nobody's seen fit to comment on these organiza...</td>\n",
       "      <td>These characters frequently wield a large gun ...</td>\n",
       "      <td>52</td>\n",
       "      <td>i Ġthink Ġan Ġimage Ġof Ġa Ġbusty , Ġlong -leg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_Man_In_Black vs Bankhallbretherton</td>\n",
       "      <td>A_Man_In_Black</td>\n",
       "      <td>Bankhallbretherton</td>\n",
       "      <td>Nobody's seen fit to comment on these organiza...</td>\n",
       "      <td>Maybe you need to realise that you need to kno...</td>\n",
       "      <td>3</td>\n",
       "      <td>Ġat Ġall ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aban1313 vs Aban1313</td>\n",
       "      <td>Aban1313</td>\n",
       "      <td>Aban1313</td>\n",
       "      <td>Oh.and enjoy our 10 billion donation to the IM...</td>\n",
       "      <td>Leopold Amery also notes in his diary on Aug 4...</td>\n",
       "      <td>6</td>\n",
       "      <td>.Ċ aban 1 3 1 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Haymaker vs HeadleyDown</td>\n",
       "      <td>Haymaker</td>\n",
       "      <td>HeadleyDown</td>\n",
       "      <td>Even if you choose not to have any faith in th...</td>\n",
       "      <td>I'm just wondering how unique it may or may no...</td>\n",
       "      <td>3</td>\n",
       "      <td>.Ċ i 'm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>HeadleyDown vs HeadleyDown</td>\n",
       "      <td>HeadleyDown</td>\n",
       "      <td>HeadleyDown</td>\n",
       "      <td>I made these points Remove redundancy Meta mod...</td>\n",
       "      <td>I'm just wondering how unique it may or may no...</td>\n",
       "      <td>4</td>\n",
       "      <td>.Ċ the Ġn lp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>HeadleyDown vs Hipocrite</td>\n",
       "      <td>HeadleyDown</td>\n",
       "      <td>Hipocrite</td>\n",
       "      <td>I made these points Remove redundancy Meta mod...</td>\n",
       "      <td>Elect someone who gets all of this to ArbCom.\\...</td>\n",
       "      <td>5</td>\n",
       "      <td>Ġon Ġthis Ġtalk Ġpage .Ċ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Hipocrite vs Hipocrite</td>\n",
       "      <td>Hipocrite</td>\n",
       "      <td>Hipocrite</td>\n",
       "      <td>It appears that reliable sources are using the...</td>\n",
       "      <td>Elect someone who gets all of this to ArbCom.\\...</td>\n",
       "      <td>4</td>\n",
       "      <td>Ġi Ġdon 't Ġsee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Hipocrite vs Hodja_Nasreddin</td>\n",
       "      <td>Hipocrite</td>\n",
       "      <td>Hodja_Nasreddin</td>\n",
       "      <td>It appears that reliable sources are using the...</td>\n",
       "      <td>If you do not like this article, you are welco...</td>\n",
       "      <td>4</td>\n",
       "      <td>.Ċ for Ġexample ,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  problem    known_author      unknown_author  \\\n",
       "0        142.196.88.228 vs 142.196.88.228  142.196.88.228      142.196.88.228   \n",
       "1              142.196.88.228 vs Aban1313  142.196.88.228            Aban1313   \n",
       "2        A_Man_In_Black vs A_Man_In_Black  A_Man_In_Black      A_Man_In_Black   \n",
       "3    A_Man_In_Black vs Bankhallbretherton  A_Man_In_Black  Bankhallbretherton   \n",
       "4                    Aban1313 vs Aban1313        Aban1313            Aban1313   \n",
       "..                                    ...             ...                 ...   \n",
       "145               Haymaker vs HeadleyDown        Haymaker         HeadleyDown   \n",
       "146            HeadleyDown vs HeadleyDown     HeadleyDown         HeadleyDown   \n",
       "147              HeadleyDown vs Hipocrite     HeadleyDown           Hipocrite   \n",
       "148                Hipocrite vs Hipocrite       Hipocrite           Hipocrite   \n",
       "149          Hipocrite vs Hodja_Nasreddin       Hipocrite     Hodja_Nasreddin   \n",
       "\n",
       "                                            known_text  \\\n",
       "0    The article that is being referred to via the ...   \n",
       "1    The article that is being referred to via the ...   \n",
       "2    Nobody's seen fit to comment on these organiza...   \n",
       "3    Nobody's seen fit to comment on these organiza...   \n",
       "4    Oh.and enjoy our 10 billion donation to the IM...   \n",
       "..                                                 ...   \n",
       "145  Even if you choose not to have any faith in th...   \n",
       "146  I made these points Remove redundancy Meta mod...   \n",
       "147  I made these points Remove redundancy Meta mod...   \n",
       "148  It appears that reliable sources are using the...   \n",
       "149  It appears that reliable sources are using the...   \n",
       "\n",
       "                                          unknown_text  highest_common_count  \\\n",
       "0    Furthermore, given the nearly parallel emergen...                    89   \n",
       "1    Leopold Amery also notes in his diary on Aug 4...                     3   \n",
       "2    These characters frequently wield a large gun ...                    52   \n",
       "3    Maybe you need to realise that you need to kno...                     3   \n",
       "4    Leopold Amery also notes in his diary on Aug 4...                     6   \n",
       "..                                                 ...                   ...   \n",
       "145  I'm just wondering how unique it may or may no...                     3   \n",
       "146  I'm just wondering how unique it may or may no...                     4   \n",
       "147  Elect someone who gets all of this to ArbCom.\\...                     5   \n",
       "148  Elect someone who gets all of this to ArbCom.\\...                     4   \n",
       "149  If you do not like this article, you are welco...                     4   \n",
       "\n",
       "                                  highest_common_ngram  \n",
       "0    Ġlove Ġthe Ġway Ġthe Ġarticle Ġincludes Ġthe Ġ...  \n",
       "1                                            .Ċ also ,  \n",
       "2    i Ġthink Ġan Ġimage Ġof Ġa Ġbusty , Ġlong -leg...  \n",
       "3                                           Ġat Ġall ,  \n",
       "4                                      .Ċ aban 1 3 1 3  \n",
       "..                                                 ...  \n",
       "145                                            .Ċ i 'm  \n",
       "146                                       .Ċ the Ġn lp  \n",
       "147                           Ġon Ġthis Ġtalk Ġpage .Ċ  \n",
       "148                                    Ġi Ġdon 't Ġsee  \n",
       "149                                  .Ċ for Ġexample ,  \n",
       "\n",
       "[150 rows x 7 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75661382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>known_author</th>\n",
       "      <th>unknown_author</th>\n",
       "      <th>known_text</th>\n",
       "      <th>unknown_text</th>\n",
       "      <th>highest_common_count</th>\n",
       "      <th>highest_common_ngram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Greg_L vs Greg_L</td>\n",
       "      <td>Greg_L</td>\n",
       "      <td>Greg_L</td>\n",
       "      <td>It amounts to obviousness obscured with talk o...</td>\n",
       "      <td>It s a practice I suspect would benefit Wikipe...</td>\n",
       "      <td>9</td>\n",
       "      <td>, Ġthey Ġshould Ġhave Ġparticipated Ġin Ġthe Ġ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Haymaker vs Haymaker</td>\n",
       "      <td>Haymaker</td>\n",
       "      <td>Haymaker</td>\n",
       "      <td>Even if you choose not to have any faith in th...</td>\n",
       "      <td>See, I can play that game to, consensus is sol...</td>\n",
       "      <td>9</td>\n",
       "      <td>Ġat Ġthe Ġend Ġof Ġthe Ġday , Ġwe 're</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Cla68 vs Cla68</td>\n",
       "      <td>Cla68</td>\n",
       "      <td>Cla68</td>\n",
       "      <td>More insults and confrontational language dire...</td>\n",
       "      <td>Tempers appear to be getting a little short in...</td>\n",
       "      <td>8</td>\n",
       "      <td>i Ġagree Ġwith Ġnorth 8 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Fixentries vs Fixentries</td>\n",
       "      <td>Fixentries</td>\n",
       "      <td>Fixentries</td>\n",
       "      <td>Given that any of it came from electronic medi...</td>\n",
       "      <td>I will help on the article but I haven't been ...</td>\n",
       "      <td>8</td>\n",
       "      <td>Ġthe Ġindividual Ġher it ability Ġof Ġintellig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>DonaNobisPacem vs DonaNobisPacem</td>\n",
       "      <td>DonaNobisPacem</td>\n",
       "      <td>DonaNobisPacem</td>\n",
       "      <td>It points out the medical community does not u...</td>\n",
       "      <td>Agreed that this is crucial to point out - the...</td>\n",
       "      <td>8</td>\n",
       "      <td>Ġafter Ġ 1 8 - 2 0 Ġweeks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Fragments_of_Jade vs Fragments_of_Jade</td>\n",
       "      <td>Fragments_of_Jade</td>\n",
       "      <td>Fragments_of_Jade</td>\n",
       "      <td>You have no room to talk about it, since you'v...</td>\n",
       "      <td>You have been stalking me all over Wikipedia a...</td>\n",
       "      <td>8</td>\n",
       "      <td>Ġme , Ġand Ġit 's Ġgetting Ġold .Ċ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Classicjupiter2 vs Classicjupiter2</td>\n",
       "      <td>Classicjupiter2</td>\n",
       "      <td>Classicjupiter2</td>\n",
       "      <td>I have been editing on here for years, sir, pl...</td>\n",
       "      <td>Its just a trolling attempt at flamebait.\\nI w...</td>\n",
       "      <td>7</td>\n",
       "      <td>Ġon Ġthe Ġsurreal ism Ġdiscussion Ġpage .Ċ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Gwen_Gale vs Gwen_Gale</td>\n",
       "      <td>Gwen_Gale</td>\n",
       "      <td>Gwen_Gale</td>\n",
       "      <td>As an aside, throughout that war, by far most ...</td>\n",
       "      <td>I linked to above, seems off by a few years - ...</td>\n",
       "      <td>6</td>\n",
       "      <td>Ġspeaking Ġonly Ġfor Ġmyself , Ġi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Fipplet vs Fipplet</td>\n",
       "      <td>Fipplet</td>\n",
       "      <td>Fipplet</td>\n",
       "      <td>I don't edit any other articles for the moment...</td>\n",
       "      <td>Some extracts There is not a supermajority vie...</td>\n",
       "      <td>6</td>\n",
       "      <td>Ġdoesn 't Ġmean Ġwe Ġshould Ġstate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>EdJohnston vs EdJohnston</td>\n",
       "      <td>EdJohnston</td>\n",
       "      <td>EdJohnston</td>\n",
       "      <td>Thank you, If you have limited space and you c...</td>\n",
       "      <td>Also, people who are aggrieved want a chance t...</td>\n",
       "      <td>6</td>\n",
       "      <td>Ġdon 't Ġsee Ġany Ġreason Ġto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Harlan_wilkerson vs Harlan_wilkerson</td>\n",
       "      <td>Harlan_wilkerson</td>\n",
       "      <td>Harlan_wilkerson</td>\n",
       "      <td>It reiterated that in the text of the 1988 Dec...</td>\n",
       "      <td>The article already cites Abbas's explanation ...</td>\n",
       "      <td>6</td>\n",
       "      <td>Ġthe Ġstate Ġof Ġpale st ine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>BrotherDarksoul vs BrotherDarksoul</td>\n",
       "      <td>BrotherDarksoul</td>\n",
       "      <td>BrotherDarksoul</td>\n",
       "      <td>Does anyone else think that the page is now wo...</td>\n",
       "      <td>In the meantime this image should stay until a...</td>\n",
       "      <td>6</td>\n",
       "      <td>.Ċ i Ġwill Ġtry Ġand Ġhunt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Ecelan vs Ecelan</td>\n",
       "      <td>Ecelan</td>\n",
       "      <td>Ecelan</td>\n",
       "      <td>You delete a reference by a scholar, one of th...</td>\n",
       "      <td>As conclusion, Mogroviejo states that no gover...</td>\n",
       "      <td>6</td>\n",
       "      <td>, Ġi 'll Ġbe Ġhappy Ġto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Hamiltonstone vs Hamiltonstone</td>\n",
       "      <td>Hamiltonstone</td>\n",
       "      <td>Hamiltonstone</td>\n",
       "      <td>In this edit, This fact about Brown's racial b...</td>\n",
       "      <td>First, I had a problem with the Staal referenc...</td>\n",
       "      <td>6</td>\n",
       "      <td>Ġat Ġthe Ġarticle Ġtalk Ġpage .Ċ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Bfigura vs Bfigura</td>\n",
       "      <td>Bfigura</td>\n",
       "      <td>Bfigura</td>\n",
       "      <td>As stated above, SLS is the common industry na...</td>\n",
       "      <td>Basically, complaints by a minority at an RfC ...</td>\n",
       "      <td>6</td>\n",
       "      <td>, Ġi 'm Ġnot Ġsure Ġthat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Chanakyathegreat vs Chanakyathegreat</td>\n",
       "      <td>Chanakyathegreat</td>\n",
       "      <td>Chanakyathegreat</td>\n",
       "      <td>By78, it was not intentional, just wanted to u...</td>\n",
       "      <td>The report from the government of India about ...</td>\n",
       "      <td>6</td>\n",
       "      <td>.Ċ by 7 8 , Ġi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Antidiskriminator vs Antidiskriminator</td>\n",
       "      <td>Antidiskriminator</td>\n",
       "      <td>Antidiskriminator</td>\n",
       "      <td>I demanded that other editors should start RM ...</td>\n",
       "      <td>I noticed that numerous GA and FA nominations ...</td>\n",
       "      <td>6</td>\n",
       "      <td>Ġthe Ġobject ivity Ġof Ġthe Ġreviews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aban1313 vs Aban1313</td>\n",
       "      <td>Aban1313</td>\n",
       "      <td>Aban1313</td>\n",
       "      <td>Oh.and enjoy our 10 billion donation to the IM...</td>\n",
       "      <td>Leopold Amery also notes in his diary on Aug 4...</td>\n",
       "      <td>6</td>\n",
       "      <td>.Ċ aban 1 3 1 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Fyunck(click) vs Fyunck(click)</td>\n",
       "      <td>Fyunck(click)</td>\n",
       "      <td>Fyunck(click)</td>\n",
       "      <td>But I will always look to make things better f...</td>\n",
       "      <td>A consensus decision is supposed to try and ta...</td>\n",
       "      <td>5</td>\n",
       "      <td>Ġone Ġway Ġor Ġthe Ġother</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>George vs George</td>\n",
       "      <td>George</td>\n",
       "      <td>George</td>\n",
       "      <td>It doesn't matter whether or not editors think...</td>\n",
       "      <td>Eh, I might go to AE for repeat, serial abuser...</td>\n",
       "      <td>5</td>\n",
       "      <td>.Ċ the Ġonly Ġproblem Ġi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    problem       known_author  \\\n",
       "126                        Greg_L vs Greg_L             Greg_L   \n",
       "144                    Haymaker vs Haymaker           Haymaker   \n",
       "56                           Cla68 vs Cla68              Cla68   \n",
       "112                Fixentries vs Fixentries         Fixentries   \n",
       "82         DonaNobisPacem vs DonaNobisPacem     DonaNobisPacem   \n",
       "118  Fragments_of_Jade vs Fragments_of_Jade  Fragments_of_Jade   \n",
       "58       Classicjupiter2 vs Classicjupiter2    Classicjupiter2   \n",
       "130                  Gwen_Gale vs Gwen_Gale          Gwen_Gale   \n",
       "110                      Fipplet vs Fipplet            Fipplet   \n",
       "90                 EdJohnston vs EdJohnston         EdJohnston   \n",
       "142    Harlan_wilkerson vs Harlan_wilkerson   Harlan_wilkerson   \n",
       "46       BrotherDarksoul vs BrotherDarksoul    BrotherDarksoul   \n",
       "88                         Ecelan vs Ecelan             Ecelan   \n",
       "134          Hamiltonstone vs Hamiltonstone      Hamiltonstone   \n",
       "32                       Bfigura vs Bfigura            Bfigura   \n",
       "54     Chanakyathegreat vs Chanakyathegreat   Chanakyathegreat   \n",
       "22   Antidiskriminator vs Antidiskriminator  Antidiskriminator   \n",
       "4                      Aban1313 vs Aban1313           Aban1313   \n",
       "120          Fyunck(click) vs Fyunck(click)      Fyunck(click)   \n",
       "124                        George vs George             George   \n",
       "\n",
       "        unknown_author                                         known_text  \\\n",
       "126             Greg_L  It amounts to obviousness obscured with talk o...   \n",
       "144           Haymaker  Even if you choose not to have any faith in th...   \n",
       "56               Cla68  More insults and confrontational language dire...   \n",
       "112         Fixentries  Given that any of it came from electronic medi...   \n",
       "82      DonaNobisPacem  It points out the medical community does not u...   \n",
       "118  Fragments_of_Jade  You have no room to talk about it, since you'v...   \n",
       "58     Classicjupiter2  I have been editing on here for years, sir, pl...   \n",
       "130          Gwen_Gale  As an aside, throughout that war, by far most ...   \n",
       "110            Fipplet  I don't edit any other articles for the moment...   \n",
       "90          EdJohnston  Thank you, If you have limited space and you c...   \n",
       "142   Harlan_wilkerson  It reiterated that in the text of the 1988 Dec...   \n",
       "46     BrotherDarksoul  Does anyone else think that the page is now wo...   \n",
       "88              Ecelan  You delete a reference by a scholar, one of th...   \n",
       "134      Hamiltonstone  In this edit, This fact about Brown's racial b...   \n",
       "32             Bfigura  As stated above, SLS is the common industry na...   \n",
       "54    Chanakyathegreat  By78, it was not intentional, just wanted to u...   \n",
       "22   Antidiskriminator  I demanded that other editors should start RM ...   \n",
       "4             Aban1313  Oh.and enjoy our 10 billion donation to the IM...   \n",
       "120      Fyunck(click)  But I will always look to make things better f...   \n",
       "124             George  It doesn't matter whether or not editors think...   \n",
       "\n",
       "                                          unknown_text  highest_common_count  \\\n",
       "126  It s a practice I suspect would benefit Wikipe...                     9   \n",
       "144  See, I can play that game to, consensus is sol...                     9   \n",
       "56   Tempers appear to be getting a little short in...                     8   \n",
       "112  I will help on the article but I haven't been ...                     8   \n",
       "82   Agreed that this is crucial to point out - the...                     8   \n",
       "118  You have been stalking me all over Wikipedia a...                     8   \n",
       "58   Its just a trolling attempt at flamebait.\\nI w...                     7   \n",
       "130  I linked to above, seems off by a few years - ...                     6   \n",
       "110  Some extracts There is not a supermajority vie...                     6   \n",
       "90   Also, people who are aggrieved want a chance t...                     6   \n",
       "142  The article already cites Abbas's explanation ...                     6   \n",
       "46   In the meantime this image should stay until a...                     6   \n",
       "88   As conclusion, Mogroviejo states that no gover...                     6   \n",
       "134  First, I had a problem with the Staal referenc...                     6   \n",
       "32   Basically, complaints by a minority at an RfC ...                     6   \n",
       "54   The report from the government of India about ...                     6   \n",
       "22   I noticed that numerous GA and FA nominations ...                     6   \n",
       "4    Leopold Amery also notes in his diary on Aug 4...                     6   \n",
       "120  A consensus decision is supposed to try and ta...                     5   \n",
       "124  Eh, I might go to AE for repeat, serial abuser...                     5   \n",
       "\n",
       "                                  highest_common_ngram  \n",
       "126  , Ġthey Ġshould Ġhave Ġparticipated Ġin Ġthe Ġ...  \n",
       "144              Ġat Ġthe Ġend Ġof Ġthe Ġday , Ġwe 're  \n",
       "56                       i Ġagree Ġwith Ġnorth 8 0 0 0  \n",
       "112  Ġthe Ġindividual Ġher it ability Ġof Ġintellig...  \n",
       "82                           Ġafter Ġ 1 8 - 2 0 Ġweeks  \n",
       "118                 Ġme , Ġand Ġit 's Ġgetting Ġold .Ċ  \n",
       "58          Ġon Ġthe Ġsurreal ism Ġdiscussion Ġpage .Ċ  \n",
       "130                  Ġspeaking Ġonly Ġfor Ġmyself , Ġi  \n",
       "110                 Ġdoesn 't Ġmean Ġwe Ġshould Ġstate  \n",
       "90                       Ġdon 't Ġsee Ġany Ġreason Ġto  \n",
       "142                       Ġthe Ġstate Ġof Ġpale st ine  \n",
       "46                          .Ċ i Ġwill Ġtry Ġand Ġhunt  \n",
       "88                             , Ġi 'll Ġbe Ġhappy Ġto  \n",
       "134                   Ġat Ġthe Ġarticle Ġtalk Ġpage .Ċ  \n",
       "32                            , Ġi 'm Ġnot Ġsure Ġthat  \n",
       "54                                      .Ċ by 7 8 , Ġi  \n",
       "22                Ġthe Ġobject ivity Ġof Ġthe Ġreviews  \n",
       "4                                      .Ċ aban 1 3 1 3  \n",
       "120                          Ġone Ġway Ġor Ġthe Ġother  \n",
       "124                           .Ċ the Ġonly Ġproblem Ġi  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_author_problems = profile_problems[profile_problems['known_author'] == profile_problems['unknown_author']].copy()\n",
    "same_author_problems.sort_values([\"highest_common_count\"], ascending=[False], inplace=True)\n",
    "same_author_problems[(same_author_problems['highest_common_count'] > 3) & (same_author_problems['highest_common_count'] <= 10)].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c079ed3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>known_author</th>\n",
       "      <th>unknown_author</th>\n",
       "      <th>known_text</th>\n",
       "      <th>unknown_text</th>\n",
       "      <th>highest_common_count</th>\n",
       "      <th>highest_common_ngram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>HeadleyDown vs Hipocrite</td>\n",
       "      <td>HeadleyDown</td>\n",
       "      <td>Hipocrite</td>\n",
       "      <td>I made these points Remove redundancy Meta mod...</td>\n",
       "      <td>Elect someone who gets all of this to ArbCom.\\...</td>\n",
       "      <td>5</td>\n",
       "      <td>Ġon Ġthis Ġtalk Ġpage .Ċ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Gwen_Gale vs Habap</td>\n",
       "      <td>Gwen_Gale</td>\n",
       "      <td>Habap</td>\n",
       "      <td>As an aside, throughout that war, by far most ...</td>\n",
       "      <td>I had no idea he'd polled better with the radi...</td>\n",
       "      <td>5</td>\n",
       "      <td>, Ġbut Ġi Ġdon 't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Fyunck(click) vs Garda40</td>\n",
       "      <td>Fyunck(click)</td>\n",
       "      <td>Garda40</td>\n",
       "      <td>But I will always look to make things better f...</td>\n",
       "      <td>Well considering the fact that edit summaries ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Ġ 2 0 1 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Caboga vs Chanakyathegreat</td>\n",
       "      <td>Caboga</td>\n",
       "      <td>Chanakyathegreat</td>\n",
       "      <td>Direktor you might have forgotten this edit la...</td>\n",
       "      <td>The report from the government of India about ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Ġ 2 0 0 8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Dweller vs Ecelan</td>\n",
       "      <td>Dweller</td>\n",
       "      <td>Ecelan</td>\n",
       "      <td>Take your pick from hundreds of non-specialist...</td>\n",
       "      <td>As conclusion, Mogroviejo states that no gover...</td>\n",
       "      <td>5</td>\n",
       "      <td>.Ċ i Ġdon 't Ġthink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Enemesis vs Equanimous1</td>\n",
       "      <td>Enemesis</td>\n",
       "      <td>Equanimous1</td>\n",
       "      <td>That link that siafu left sort of backs up wha...</td>\n",
       "      <td>If you have graduated with a doctorate in heal...</td>\n",
       "      <td>5</td>\n",
       "      <td>Ġthe Ġcurrent Ġstate Ġof Ġthe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>David_Shankbone vs Delicious_carbuncle</td>\n",
       "      <td>David_Shankbone</td>\n",
       "      <td>Delicious_carbuncle</td>\n",
       "      <td>I definitely think the Met Opera one looks muc...</td>\n",
       "      <td>Lord G n, once again, this particular discussi...</td>\n",
       "      <td>4</td>\n",
       "      <td>.Ċ i 'm Ġnot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Ecelan vs EdJohnston</td>\n",
       "      <td>Ecelan</td>\n",
       "      <td>EdJohnston</td>\n",
       "      <td>You delete a reference by a scholar, one of th...</td>\n",
       "      <td>Also, people who are aggrieved want a chance t...</td>\n",
       "      <td>4</td>\n",
       "      <td>.Ċ i Ġdon 't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Cla68 vs Classicjupiter2</td>\n",
       "      <td>Cla68</td>\n",
       "      <td>Classicjupiter2</td>\n",
       "      <td>More insults and confrontational language dire...</td>\n",
       "      <td>Its just a trolling attempt at flamebait.\\nI w...</td>\n",
       "      <td>4</td>\n",
       "      <td>, Ġin Ġorder Ġto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Cptnono vs CyberAnth</td>\n",
       "      <td>Cptnono</td>\n",
       "      <td>CyberAnth</td>\n",
       "      <td>Further edit warring without using the talk pa...</td>\n",
       "      <td>Yes, I can understand how that would feel.\\nIf...</td>\n",
       "      <td>4</td>\n",
       "      <td>Ġto Ġfix Ġit .Ċ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Danlaycock vs David_Shankbone</td>\n",
       "      <td>Danlaycock</td>\n",
       "      <td>David_Shankbone</td>\n",
       "      <td>The scope of the current article is broader th...</td>\n",
       "      <td>We already have a crime section, so the issue ...</td>\n",
       "      <td>4</td>\n",
       "      <td>.Ċ i Ġdon 't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Delicious_carbuncle vs Dennis_Brown</td>\n",
       "      <td>Delicious_carbuncle</td>\n",
       "      <td>Dennis_Brown</td>\n",
       "      <td>I'm trying to defuse this situation before it ...</td>\n",
       "      <td>I am assuming that most submission are by regi...</td>\n",
       "      <td>4</td>\n",
       "      <td>.Ċ i Ġdon 't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>DonaNobisPacem vs Dricherby</td>\n",
       "      <td>DonaNobisPacem</td>\n",
       "      <td>Dricherby</td>\n",
       "      <td>It points out the medical community does not u...</td>\n",
       "      <td>I took a look at both versions, before the rec...</td>\n",
       "      <td>4</td>\n",
       "      <td>Ġand Ġit Ġdoesn 't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Dricherby vs Dweller</td>\n",
       "      <td>Dricherby</td>\n",
       "      <td>Dweller</td>\n",
       "      <td>As I said, the population of Merseyside and th...</td>\n",
       "      <td>Is it because there wasn't much money in the g...</td>\n",
       "      <td>4</td>\n",
       "      <td>i Ġdon 't Ġknow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Falcon9x5 vs Fipplet</td>\n",
       "      <td>Falcon9x5</td>\n",
       "      <td>Fipplet</td>\n",
       "      <td>Ur, when units are a core part 'the' core part...</td>\n",
       "      <td>Some extracts There is not a supermajority vie...</td>\n",
       "      <td>4</td>\n",
       "      <td>.Ċ i Ġdon 't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Essjay vs Exploding_Boy</td>\n",
       "      <td>Essjay</td>\n",
       "      <td>Exploding_Boy</td>\n",
       "      <td>I have received an astounding amount of suppor...</td>\n",
       "      <td>If we have reception and storyline in both Hol...</td>\n",
       "      <td>4</td>\n",
       "      <td>.Ċ i Ġdon 't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Fipplet vs Fixentries</td>\n",
       "      <td>Fipplet</td>\n",
       "      <td>Fixentries</td>\n",
       "      <td>I don't edit any other articles for the moment...</td>\n",
       "      <td>I will help on the article but I haven't been ...</td>\n",
       "      <td>4</td>\n",
       "      <td>.Ċ i Ġdidn 't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Floquenbeam vs Fragments_of_Jade</td>\n",
       "      <td>Floquenbeam</td>\n",
       "      <td>Fragments_of_Jade</td>\n",
       "      <td>It looks like the other editor just got themse...</td>\n",
       "      <td>You have been stalking me all over Wikipedia a...</td>\n",
       "      <td>4</td>\n",
       "      <td>Ġasked Ġyou Ġto Ġstop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Garda40 vs George</td>\n",
       "      <td>Garda40</td>\n",
       "      <td>George</td>\n",
       "      <td>Also the fact that you have to pay to see a sh...</td>\n",
       "      <td>Eh, I might go to AE for repeat, serial abuser...</td>\n",
       "      <td>4</td>\n",
       "      <td>.Ċ i Ġdon 't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Habap vs Hamiltonstone</td>\n",
       "      <td>Habap</td>\n",
       "      <td>Hamiltonstone</td>\n",
       "      <td>It does mention reprisals, but doesn't match o...</td>\n",
       "      <td>First, I had a problem with the Staal referenc...</td>\n",
       "      <td>4</td>\n",
       "      <td>, Ġbut Ġi Ġhave</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    problem         known_author  \\\n",
       "147                HeadleyDown vs Hipocrite          HeadleyDown   \n",
       "131                      Gwen_Gale vs Habap            Gwen_Gale   \n",
       "121                Fyunck(click) vs Garda40        Fyunck(click)   \n",
       "53               Caboga vs Chanakyathegreat               Caboga   \n",
       "87                        Dweller vs Ecelan              Dweller   \n",
       "95                  Enemesis vs Equanimous1             Enemesis   \n",
       "75   David_Shankbone vs Delicious_carbuncle      David_Shankbone   \n",
       "89                     Ecelan vs EdJohnston               Ecelan   \n",
       "57                 Cla68 vs Classicjupiter2                Cla68   \n",
       "67                     Cptnono vs CyberAnth              Cptnono   \n",
       "73            Danlaycock vs David_Shankbone           Danlaycock   \n",
       "77      Delicious_carbuncle vs Dennis_Brown  Delicious_carbuncle   \n",
       "83              DonaNobisPacem vs Dricherby       DonaNobisPacem   \n",
       "85                     Dricherby vs Dweller            Dricherby   \n",
       "109                    Falcon9x5 vs Fipplet            Falcon9x5   \n",
       "103                 Essjay vs Exploding_Boy               Essjay   \n",
       "111                   Fipplet vs Fixentries              Fipplet   \n",
       "117        Floquenbeam vs Fragments_of_Jade          Floquenbeam   \n",
       "123                       Garda40 vs George              Garda40   \n",
       "133                  Habap vs Hamiltonstone                Habap   \n",
       "\n",
       "          unknown_author                                         known_text  \\\n",
       "147            Hipocrite  I made these points Remove redundancy Meta mod...   \n",
       "131                Habap  As an aside, throughout that war, by far most ...   \n",
       "121              Garda40  But I will always look to make things better f...   \n",
       "53      Chanakyathegreat  Direktor you might have forgotten this edit la...   \n",
       "87                Ecelan  Take your pick from hundreds of non-specialist...   \n",
       "95           Equanimous1  That link that siafu left sort of backs up wha...   \n",
       "75   Delicious_carbuncle  I definitely think the Met Opera one looks muc...   \n",
       "89            EdJohnston  You delete a reference by a scholar, one of th...   \n",
       "57       Classicjupiter2  More insults and confrontational language dire...   \n",
       "67             CyberAnth  Further edit warring without using the talk pa...   \n",
       "73       David_Shankbone  The scope of the current article is broader th...   \n",
       "77          Dennis_Brown  I'm trying to defuse this situation before it ...   \n",
       "83             Dricherby  It points out the medical community does not u...   \n",
       "85               Dweller  As I said, the population of Merseyside and th...   \n",
       "109              Fipplet  Ur, when units are a core part 'the' core part...   \n",
       "103        Exploding_Boy  I have received an astounding amount of suppor...   \n",
       "111           Fixentries  I don't edit any other articles for the moment...   \n",
       "117    Fragments_of_Jade  It looks like the other editor just got themse...   \n",
       "123               George  Also the fact that you have to pay to see a sh...   \n",
       "133        Hamiltonstone  It does mention reprisals, but doesn't match o...   \n",
       "\n",
       "                                          unknown_text  highest_common_count  \\\n",
       "147  Elect someone who gets all of this to ArbCom.\\...                     5   \n",
       "131  I had no idea he'd polled better with the radi...                     5   \n",
       "121  Well considering the fact that edit summaries ...                     5   \n",
       "53   The report from the government of India about ...                     5   \n",
       "87   As conclusion, Mogroviejo states that no gover...                     5   \n",
       "95   If you have graduated with a doctorate in heal...                     5   \n",
       "75   Lord G n, once again, this particular discussi...                     4   \n",
       "89   Also, people who are aggrieved want a chance t...                     4   \n",
       "57   Its just a trolling attempt at flamebait.\\nI w...                     4   \n",
       "67   Yes, I can understand how that would feel.\\nIf...                     4   \n",
       "73   We already have a crime section, so the issue ...                     4   \n",
       "77   I am assuming that most submission are by regi...                     4   \n",
       "83   I took a look at both versions, before the rec...                     4   \n",
       "85   Is it because there wasn't much money in the g...                     4   \n",
       "109  Some extracts There is not a supermajority vie...                     4   \n",
       "103  If we have reception and storyline in both Hol...                     4   \n",
       "111  I will help on the article but I haven't been ...                     4   \n",
       "117  You have been stalking me all over Wikipedia a...                     4   \n",
       "123  Eh, I might go to AE for repeat, serial abuser...                     4   \n",
       "133  First, I had a problem with the Staal referenc...                     4   \n",
       "\n",
       "              highest_common_ngram  \n",
       "147       Ġon Ġthis Ġtalk Ġpage .Ċ  \n",
       "131              , Ġbut Ġi Ġdon 't  \n",
       "121                      Ġ 2 0 1 1  \n",
       "53                       Ġ 2 0 0 8  \n",
       "87             .Ċ i Ġdon 't Ġthink  \n",
       "95   Ġthe Ġcurrent Ġstate Ġof Ġthe  \n",
       "75                    .Ċ i 'm Ġnot  \n",
       "89                    .Ċ i Ġdon 't  \n",
       "57                , Ġin Ġorder Ġto  \n",
       "67                 Ġto Ġfix Ġit .Ċ  \n",
       "73                    .Ċ i Ġdon 't  \n",
       "77                    .Ċ i Ġdon 't  \n",
       "83              Ġand Ġit Ġdoesn 't  \n",
       "85                 i Ġdon 't Ġknow  \n",
       "109                   .Ċ i Ġdon 't  \n",
       "103                   .Ċ i Ġdon 't  \n",
       "111                  .Ċ i Ġdidn 't  \n",
       "117          Ġasked Ġyou Ġto Ġstop  \n",
       "123                   .Ċ i Ġdon 't  \n",
       "133                , Ġbut Ġi Ġhave  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_author_problems = profile_problems[profile_problems['known_author'] != profile_problems['unknown_author']].copy()\n",
    "diff_author_problems.sort_values([\"highest_common_count\"], ascending=[False], inplace=True)\n",
    "diff_author_problems[(diff_author_problems['highest_common_count'] > 3) & (diff_author_problems['highest_common_count'] <= 10)].head(20)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
