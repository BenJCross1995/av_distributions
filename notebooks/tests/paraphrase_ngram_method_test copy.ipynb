{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd79e6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, List, Sequence, Set, Tuple, Literal\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f23990a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('../../src'))\n",
    "\n",
    "from read_and_write_docs import read_jsonl, read_rds\n",
    "from tokenize_and_score import load_model, compute_log_probs_with_median\n",
    "from utils import apply_temp_doc_id, build_metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8558c8a7",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "We import the known, unknown and metadata. I have already identified documents of interest for a same_author = True and same_author = False test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76dbfbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_loc = \"/Volumes/BCross/datasets/author_verification/training/Wiki/known_raw.jsonl\"\n",
    "known = read_jsonl(known_loc)\n",
    "known = apply_temp_doc_id(known)\n",
    "\n",
    "unknown_loc = \"/Volumes/BCross/datasets/author_verification/training/Wiki/unknown_raw.jsonl\"\n",
    "unknown = read_jsonl(unknown_loc)\n",
    "unknown_df = apply_temp_doc_id(unknown)\n",
    "\n",
    "metadata_loc = \"/Volumes/BCross/datasets/author_verification/training/metadata.rds\"\n",
    "metadata = read_rds(metadata_loc)\n",
    "filtered_metadata = metadata[metadata['corpus'] == 'Wiki']\n",
    "agg_metadata = build_metadata_df(filtered_metadata, known, unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9095bd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = load_model(\"/Volumes/BCross/models/Qwen 2.5/Qwen2.5-0.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6711a1",
   "metadata": {},
   "source": [
    "## Select Texts\n",
    "\n",
    "Some texts i identified from the Wiki training corpus as having common n-grams within the metadata problem space, while also ensuring that the same author is in both problems are below:\n",
    "\n",
    "* Same Author = True\n",
    "    * fipplet_text_2\n",
    "    * fipplet_text_5\n",
    "* Same Author = False\n",
    "    * falcon9x5_text_3\n",
    "    * fipplet_text_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2597c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_author_known = known[known['doc_id'] == 'greg_l_text_11'].reset_index().loc[0, 'text']\n",
    "same_author_unknown = unknown[unknown['doc_id'] == 'greg_l_text_10'].reset_index().loc[0, 'text']\n",
    "\n",
    "diff_author_known = known[known['doc_id'] == 'britmax_text_1'].reset_index().loc[0, 'text']\n",
    "diff_author_unknown = unknown[unknown['doc_id'] == 'brotherdarksoul_text_4'].reset_index().loc[0, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bf1d401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It s a practice I suspect would benefit Wikipedia if more editors did so.\\nThink of my user page as being a  Well done, Guy, on your exploration into the discipline of TDOTSCIFOGLH.\\nYou seem to have an engineering bent in your makeup as you have properly touched upon real-world shortcomings and omissions in the measure tolerance, calibration, traceability, etc.\\nI 'can' tell you that though American I am all-things-metric and have been for decades.\\nSo I used a tape measure from a major manufacturer that was marked in millimeters, so the value was not the product of a conversion.\\nAs for the 'weight' of the cover, GFHandel, due to your previous prompting, I actually went out there in the middle of the road one day armed with a prying tool and quickly realized it was a monster.\\nOnly recently did I read that manhole covers were disappearing in some U.S. city so they could be cut up and sold to recycling centers.\\nThey apparently weigh nearly a hundred kilos.\\nI had been planning if I could budge the thing to weigh it using a Salter/Brecknell electroSamson, digital hanging scale left over from weighing a lab animal.\\nThat wouldn t have worked and I might have torn a muscle trying.\\nI might jigger things around so the table of contents moves up towards the top as I stare at this in Show preview, I like that idea a whole bunch.\\nWikipedia MOS where crops wither, midwives weep, and pestilence and plague spreads across the land.\\nAs can be seen at RfCs such as Born2cycle on the other hand, was upholding the principle of consensus it is, after all, enshrined at and was willing to fight on the ice '\\nIt takes 'two' however, to fight on the ice and Dicklyon was clearly the other one his slice things differently poll being a prime example.\\nBorn2cycle also didn t seem to understand that pithy posts on Wikipedia are generally more effective than lengthy and detailed ones.\\nSince we re talking nutshells, here s my 2 on this Here in my city, some off-duty cop was driving drunk, hit someone, and then fled the scene.\\nAnd he sued for back wages and damages, claiming that job stress led to his alcoholism, the city ignored his alcoholism, did nothing to treat him, alcoholism is a disability, the city must accommodate him with his disability, yadda-yadda, political correctness ad nauseam, etcetera.\\nSome sort of Feel good about yourself state committee backed him in his pursuit of scads of money.\\nAfter a public outcry, the city council nixed the proposed settlement.\\nDuring the debate over this a unanimous decision, a city councilman said this  Dicklyon simply wanted his way and I am convinced he knew full well that he was simply playing a game of attrition whereby protracted intransigence was driving others away or making them cave in exasperation\\nBorn2cycle on the other hand, was upholding the principal of consensus it is, after all, enshrined at sock editors who took care to offer insightful reasoning for why they unanimously support '\\nIf the eight editors whom you allege harbor views that are contrary to the consensus view 'truly' had felt they had meritorious arguments, they should have participated in the poll.\\nThat they elected to instead shove their hands into their pockets and slink away rather than 'stand up and articulate persuasive reasoning' shows they felt their arguments were weak at best.\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_author_unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfba5aef",
   "metadata": {},
   "source": [
    "## Common n-grams \n",
    "\n",
    "Return the common n-grams between the two texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d16cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_ngrams(\n",
    "    text1: str,\n",
    "    text2: str,\n",
    "    n: int,\n",
    "    model: Any = None,\n",
    "    tokenizer: Any = None,\n",
    "    include_subgrams: bool = False,\n",
    ") -> Dict[int, Set[Tuple[Any, ...]]]:\n",
    "    \"\"\"\n",
    "    Return shared n-grams of length >= n between two texts.\n",
    "\n",
    "    If include_subgrams is False (default), remove any shared n-gram that is a\n",
    "    contiguous subspan of a longer shared n-gram. (So a 5-gram that’s part of a\n",
    "    shared 6-gram is excluded; unrelated 5-grams remain.)\n",
    "    \"\"\"\n",
    "    if n < 1:\n",
    "        raise ValueError(\"n must be >= 1\")\n",
    "\n",
    "    def _word_tokens(s: str) -> List[str]:\n",
    "        return re.findall(r\"\\w+\", s.casefold())\n",
    "\n",
    "    def _hf_tokens(txt: str) -> List[Any]:\n",
    "        if hasattr(tokenizer, \"tokenize\"):\n",
    "            return list(tokenizer.tokenize(txt))\n",
    "        enc = tokenizer(\n",
    "            txt,\n",
    "            add_special_tokens=False,\n",
    "            return_attention_mask=False,\n",
    "            return_token_type_ids=False,\n",
    "        )\n",
    "        input_ids = enc.get(\"input_ids\", [])\n",
    "        if input_ids and isinstance(input_ids[0], (list, tuple)):\n",
    "            input_ids = input_ids[0]\n",
    "        if hasattr(tokenizer, \"convert_ids_to_tokens\"):\n",
    "            return tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        return input_ids\n",
    "\n",
    "    def _ngrams_by_len(seq: Sequence[Any], min_n: int) -> Dict[int, Set[Tuple[Any, ...]]]:\n",
    "        out: Dict[int, Set[Tuple[Any, ...]]] = {}\n",
    "        L = len(seq)\n",
    "        for k in range(min_n, L + 1):\n",
    "            s: Set[Tuple[Any, ...]] = set()\n",
    "            for i in range(0, L - k + 1):\n",
    "                s.add(tuple(seq[i : i + k]))\n",
    "            if s:\n",
    "                out[k] = s\n",
    "        return out\n",
    "\n",
    "    token_mode = (model is not None) and (tokenizer is not None)\n",
    "    seq1 = _hf_tokens(text1) if token_mode else _word_tokens(text1)\n",
    "    seq2 = _hf_tokens(text2) if token_mode else _word_tokens(text2)\n",
    "\n",
    "    ngrams1 = _ngrams_by_len(seq1, n)\n",
    "    ngrams2 = _ngrams_by_len(seq2, n)\n",
    "\n",
    "    common: Dict[int, Set[Tuple[Any, ...]]] = {}\n",
    "    for k in set(ngrams1.keys()).intersection(ngrams2.keys()):\n",
    "        inter = ngrams1[k] & ngrams2[k]\n",
    "        if inter:\n",
    "            common[k] = inter\n",
    "\n",
    "    if include_subgrams or not common:\n",
    "        return common\n",
    "\n",
    "    # Remove n-grams that are contiguous subspans of any longer shared n-gram\n",
    "    to_remove: Dict[int, Set[Tuple[Any, ...]]] = defaultdict(set)\n",
    "    lengths = sorted(common.keys())\n",
    "    for k in lengths:\n",
    "        # For each longer length, generate all contiguous subspans down to n\n",
    "        for longer_k in [L for L in lengths if L > k]:\n",
    "            for g in common[longer_k]:\n",
    "                # produce all subspans of length k from g\n",
    "                for i in range(0, longer_k - k + 1):\n",
    "                    to_remove[k].add(g[i : i + k])\n",
    "\n",
    "    # Apply removals\n",
    "    for k, rem in to_remove.items():\n",
    "        if k in common:\n",
    "            common[k] = {g for g in common[k] if g not in rem}\n",
    "            if not common[k]:\n",
    "                del common[k]\n",
    "\n",
    "    return common\n",
    "\n",
    "def pretty_print_common_ngrams(\n",
    "    common: Dict[int, Set[Tuple[Any, ...]]],\n",
    "    sep: str = \" \",\n",
    "    order: str = \"count_desc\",  # \"count_desc\" | \"len_asc\" | \"len_desc\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Pretty-print shared n-grams.\n",
    "\n",
    "    - Groups by n (the integer length).\n",
    "    - Converts each n-gram tuple into a string joined by `sep`.\n",
    "    - Prints lists, ordered by the number of n-grams per length (descending) by default.\n",
    "    \"\"\"\n",
    "    if not common:\n",
    "        print(\"{}\")\n",
    "        return\n",
    "\n",
    "    # Convert tuples to strings per length key\n",
    "    grouped: Dict[int, List[str]] = {\n",
    "        n: sorted(sep.join(map(str, g)) for g in grams)\n",
    "        for n, grams in common.items()\n",
    "    }\n",
    "\n",
    "    # Choose group ordering\n",
    "    if order == \"count_desc\":\n",
    "        items = sorted(grouped.items(), key=lambda kv: (-len(kv[1]), kv[0]))\n",
    "    elif order == \"len_asc\":\n",
    "        items = sorted(grouped.items(), key=lambda kv: kv[0])\n",
    "    elif order == \"len_desc\":\n",
    "        items = sorted(grouped.items(), key=lambda kv: -kv[0])\n",
    "    else:\n",
    "        items = sorted(grouped.items(), key=lambda kv: (-len(kv[1]), kv[0]))\n",
    "\n",
    "    # Print: e.g., \"3-grams (5): ['a b c', 'd e f', ...]\"\n",
    "    for n, strings in items:\n",
    "        print(f\"{n}-grams ({len(strings)}): {strings}\")\n",
    "        \n",
    "def keep_before_phrase(text: str, phrase: str, case_insensitive: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Return everything in `text` before the first occurrence of `phrase`.\n",
    "    If `phrase` isn’t found, returns the entire `text`.\n",
    "\n",
    "    :param text:       The full string you want to trim.\n",
    "    :param phrase:     The substring (phrase) you want to stop at.\n",
    "    :param case_insensitive:  If True, match phrase ignoring case.\n",
    "    :return:           The portion of `text` before `phrase`.\n",
    "    \"\"\"\n",
    "    if case_insensitive:\n",
    "        idx = text.lower().find(phrase.lower())\n",
    "    else:\n",
    "        idx = text.find(phrase)\n",
    "\n",
    "    return text[:idx] if idx != -1 else text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2658868c",
   "metadata": {},
   "source": [
    "## Same Author = True Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b0e6aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-grams (5): ['.Ċ You', 'Ġconsensus Ġview', 'Ġof Ġthe', 'Ġsuch Ġas', 'Ġwas Ġa']\n",
      "3-grams (3): ['ĠDick ly on', 'Ġmer itor ious', 'Ġthe Ġeight Ġeditors']\n",
      "9-grams (2): [', Ġthey Ġshould Ġhave Ġparticipated Ġin Ġthe Ġpoll .Ċ', 'Ġallege Ġharbor Ġviews Ġthat Ġare Ġcontrary Ġto Ġthe Ġconsensus']\n",
      "4-grams (1): ['Ġhad Ġfelt Ġthey Ġhad']\n"
     ]
    }
   ],
   "source": [
    "common_same_author = common_ngrams(same_author_known, same_author_unknown, n=2, tokenizer=tokenizer, model=model)\n",
    "pretty_print_common_ngrams(common_same_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ad51c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Set, Tuple\n",
    "\n",
    "def pretty_print_common_ngrams(\n",
    "    common: Dict[int, Set[Tuple[Any, ...]]],\n",
    "    sep: str = \" \",\n",
    "    order: str = \"count_desc\",  # \"count_desc\" | \"len_asc\" | \"len_desc\"\n",
    "    tokenizer=None,             # Optional HuggingFace tokenizer\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Pretty-print shared n-grams.\n",
    "\n",
    "    - Groups by n (the integer length).\n",
    "    - If `tokenizer` is None: converts each n-gram tuple into a string joined by `sep` (original behavior).\n",
    "    - If `tokenizer` is provided: decodes token ids/strings to readable text (special tokens removed).\n",
    "    - Prints lists, ordered by the number of n-grams per length (descending) by default.\n",
    "    \"\"\"\n",
    "    if not common:\n",
    "        print(\"{}\")\n",
    "        return\n",
    "\n",
    "    def stringify_ngram(ngram: Tuple[Any, ...]) -> str:\n",
    "        # Original behavior (no tokenizer): join items with sep\n",
    "        if tokenizer is None:\n",
    "            return sep.join(map(str, ngram))\n",
    "\n",
    "        # With tokenizer: decode to human-readable text\n",
    "        toks = list(ngram)\n",
    "\n",
    "        # If everything is ids, use fast decode\n",
    "        if all(isinstance(t, int) for t in toks):\n",
    "            return tokenizer.decode(\n",
    "                toks,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=False,\n",
    "            )\n",
    "\n",
    "        # Otherwise, we may have token *strings* or a mix of ids & strings\n",
    "        specials = set(getattr(tokenizer, \"all_special_tokens\", []))\n",
    "        norm_tokens: List[str] = []\n",
    "        for t in toks:\n",
    "            if isinstance(t, int):\n",
    "                # convert id -> token string\n",
    "                norm_tokens.append(tokenizer.convert_ids_to_tokens(t))\n",
    "            else:\n",
    "                norm_tokens.append(str(t))\n",
    "\n",
    "        # Drop special tokens (e.g., <s>, </s>)\n",
    "        norm_tokens = [t for t in norm_tokens if t not in specials]\n",
    "\n",
    "        # Let the tokenizer handle spacing/newlines between tokens\n",
    "        return tokenizer.convert_tokens_to_string(norm_tokens)\n",
    "\n",
    "    # Convert tuples to strings per length key\n",
    "    grouped: Dict[int, List[str]] = {\n",
    "        n: sorted(stringify_ngram(g) for g in grams)\n",
    "        for n, grams in common.items()\n",
    "    }\n",
    "\n",
    "    # Choose group ordering\n",
    "    if order == \"count_desc\":\n",
    "        items = sorted(grouped.items(), key=lambda kv: (-len(kv[1]), kv[0]))\n",
    "    elif order == \"len_asc\":\n",
    "        items = sorted(grouped.items(), key=lambda kv: kv[0])\n",
    "    elif order == \"len_desc\":\n",
    "        items = sorted(grouped.items(), key=lambda kv: -kv[0])\n",
    "    else:\n",
    "        items = sorted(grouped.items(), key=lambda kv: (-len(kv[1]), kv[0]))\n",
    "\n",
    "    # Print: e.g., \"3-grams (5): ['a b c', 'd e f', ...]\"\n",
    "    for n, strings in items:\n",
    "        print(f\"{n}-grams ({len(strings)}): {strings}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73a58f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(',',\n",
       "  'Ġthey',\n",
       "  'Ġshould',\n",
       "  'Ġhave',\n",
       "  'Ġparticipated',\n",
       "  'Ġin',\n",
       "  'Ġthe',\n",
       "  'Ġpoll',\n",
       "  '.Ċ'),\n",
       " ('Ġallege',\n",
       "  'Ġharbor',\n",
       "  'Ġviews',\n",
       "  'Ġthat',\n",
       "  'Ġare',\n",
       "  'Ġcontrary',\n",
       "  'Ġto',\n",
       "  'Ġthe',\n",
       "  'Ġconsensus')}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_same_author[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a783a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-grams (5): [' consensus view', ' of the', ' such as', ' was a', '.\\nYou']\n",
      "3-grams (3): [' Dicklyon', ' meritorious', ' the eight editors']\n",
      "4-grams (1): [' had felt they had']\n",
      "9-grams (2): [' allege harbor views that are contrary to the consensus', ', they should have participated in the poll.\\n']\n"
     ]
    }
   ],
   "source": [
    "pretty_print_common_ngrams(common_same_author, tokenizer=tokenizer, order='len_asc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "316de9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_phrase_1 = \"allege harbor views that are contrary to the consensus\"\n",
    "same_para_1 = [\n",
    "    \"allege harbor views that are opposed to the consensus\",\n",
    "    \"allege harbor views that are antithetical to the consensus\",\n",
    "    \"allege harbor opinions that are contrary to the consensus\",\n",
    "    \"allege harbor opinions that are opposed to the consensus\",\n",
    "    \"allege harbor opinions that are antithetical to the consensus\",\n",
    "    \"allege harbor beliefs that are contrary to the consensus\",\n",
    "    \"allege harbor beliefs that are opposed to the consensus\",\n",
    "    \"allege harbor beliefs that are antithetical to the consensus\",\n",
    "    \"allege harbor positions that are contrary to the consensus\",\n",
    "    \"allege harbor positions that are opposed to the consensus\",\n",
    "    \"allege harbor positions that are antithetical to the consensus\",\n",
    "    \"allege hold views that are contrary to the consensus\",\n",
    "    \"allege hold views that are opposed to the consensus\",\n",
    "    \"allege hold views that are antithetical to the consensus\",\n",
    "    \"allege hold opinions that are contrary to the consensus\",\n",
    "    \"allege hold opinions that are opposed to the consensus\",\n",
    "    \"allege hold opinions that are antithetical to the consensus\",\n",
    "    \"allege hold beliefs that are contrary to the consensus\",\n",
    "    \"allege hold beliefs that are opposed to the consensus\",\n",
    "    \"allege hold beliefs that are antithetical to the consensus\",\n",
    "    \"allege hold positions that are contrary to the consensus\",\n",
    "    \"allege hold positions that are opposed to the consensus\",\n",
    "    \"allege hold positions that are antithetical to the consensus\", \n",
    "    \"allege have views that are contrary to the consensus\",\n",
    "    \"allege have views that are opposed to the consensus\", \"allege have views that are antithetical to the consensus\", \"allege have opinions that are contrary to the consensus\", \"allege have opinions that are opposed to the consensus\", \"allege have opinions that are antithetical to the consensus\", \"allege have beliefs that are contrary to the consensus\", \"allege have beliefs that are opposed to the consensus\", \"allege have beliefs that are antithetical to the consensus\", \"allege have positions that are contrary to the consensus\", \"allege have positions that are opposed to the consensus\", \"allege have positions that are antithetical to the consensus\", \"claim harbor views that are contrary to the consensus\", \"claim harbor views that are opposed to the consensus\", \"claim harbor views that are antithetical to the consensus\", \"claim harbor opinions that are contrary to the consensus\", \"claim harbor opinions that are opposed to the consensus\", \"claim harbor opinions that are antithetical to the consensus\", \"claim harbor beliefs that are contrary to the consensus\", \"claim harbor beliefs that are opposed to the consensus\", \"claim harbor beliefs that are antithetical to the consensus\", \"claim harbor positions that are contrary to the consensus\", \"claim harbor positions that are opposed to the consensus\", \"claim harbor positions that are antithetical to the consensus\", \"claim hold views that are contrary to the consensus\", \"claim hold views that are opposed to the consensus\", \"claim hold views that are antithetical to the consensus\", \"claim hold opinions that are contrary to the consensus\", \"claim hold opinions that are opposed to the consensus\", \"claim hold opinions that are antithetical to the consensus\", \"claim hold beliefs that are contrary to the consensus\", \"claim hold beliefs that are opposed to the consensus\", \"claim hold beliefs that are antithetical to the consensus\", \"claim hold positions that are contrary to the consensus\", \"claim hold positions that are opposed to the consensus\", \"claim hold positions that are antithetical to the consensus\", \"claim have views that are contrary to the consensus\", \"claim have views that are opposed to the consensus\", \"claim have views that are antithetical to the consensus\", \"claim have opinions that are contrary to the consensus\", \"claim have opinions that are opposed to the consensus\", \"claim have opinions that are antithetical to the consensus\", \"claim have beliefs that are contrary to the consensus\", \"claim have beliefs that are opposed to the consensus\", \"claim have beliefs that are antithetical to the consensus\", \"claim have positions that are contrary to the consensus\", \"claim have positions that are opposed to the consensus\", \"claim have positions that are antithetical to the consensus\", \"assert harbor views that are contrary to the consensus\", \"assert harbor views that are opposed to the consensus\", \"assert harbor views that are antithetical to the consensus\", \"assert harbor opinions that are contrary to the consensus\", \"assert harbor opinions that are opposed to the consensus\", \"assert harbor opinions that are antithetical to the consensus\", \"assert harbor beliefs that are contrary to the consensus\", \"assert harbor beliefs that are opposed to the consensus\", \"assert harbor beliefs that are antithetical to the consensus\", \"assert harbor positions that are contrary to the consensus\", \"assert harbor positions that are opposed to the consensus\", \"assert harbor positions that are antithetical to the consensus\", \"assert hold views that are contrary to the consensus\", \"assert hold views that are opposed to the consensus\", \"assert hold views that are antithetical to the consensus\", \"assert hold opinions that are contrary to the consensus\", \"assert hold opinions that are opposed to the consensus\", \"assert hold opinions that are antithetical to the consensus\", \"assert hold beliefs that are contrary to the consensus\", \"assert hold beliefs that are opposed to the consensus\", \"assert hold beliefs that are antithetical to the consensus\", \"assert hold positions that are contrary to the consensus\", \"assert hold positions that are opposed to the consensus\", \"assert hold positions that are antithetical to the consensus\", \"assert have views that are contrary to the consensus\", \"assert have views that are opposed to the consensus\", \"assert have views that are antithetical to the consensus\", \"assert have opinions that are contrary to the consensus\", \"assert have opinions that are opposed to the consensus\", \"assert have opinions that are antithetical to the consensus\", \"assert have beliefs that are contrary to the consensus\", \"assert have beliefs that are opposed to the consensus\", \"assert have beliefs that are antithetical to the consensus\", \"assert have positions that are contrary to the consensus\", \"assert have positions that are opposed to the consensus\", \"assert have positions that are antithetical to the consensus\", \"contend harbor views that are contrary to the consensus\", \"contend harbor views that are opposed to the consensus\", \"contend harbor views that are antithetical to the consensus\", \"contend harbor opinions that are contrary to the consensus\", \"contend harbor opinions that are opposed to the consensus\", \"contend harbor opinions that are antithetical to the consensus\", \"contend harbor beliefs that are contrary to the consensus\", \"contend harbor beliefs that are opposed to the consensus\", \"contend harbor beliefs that are antithetical to the consensus\", \"contend harbor positions that are contrary to the consensus\", \"contend harbor positions that are opposed to the consensus\", \"contend harbor positions that are antithetical to the consensus\", \"contend hold views that are contrary to the consensus\", \"contend hold views that are opposed to the consensus\", \"contend hold views that are antithetical to the consensus\", \"contend hold opinions that are contrary to the consensus\", \"contend hold opinions that are opposed to the consensus\", \"contend hold opinions that are antithetical to the consensus\", \"contend hold beliefs that are contrary to the consensus\", \"contend hold beliefs that are opposed to the consensus\", \"contend hold beliefs that are antithetical to the consensus\", \"contend hold positions that are contrary to the consensus\", \"contend hold positions that are opposed to the consensus\", \"contend hold positions that are antithetical to the consensus\", \"contend have views that are contrary to the consensus\", \"contend have views that are opposed to the consensus\", \"contend have views that are antithetical to the consensus\", \"contend have opinions that are contrary to the consensus\", \"contend have opinions that are opposed to the consensus\", \"contend have opinions that are antithetical to the consensus\", \"contend have beliefs that are contrary to the consensus\", \"contend have beliefs that are opposed to the consensus\", \"contend have beliefs that are antithetical to the consensus\", \"contend have positions that are contrary to the consensus\", \"contend have positions that are opposed to the consensus\", \"contend have positions that are antithetical to the consensus\", \"allege harbour views that are contrary to the consensus\", \"allege harbour opinions that are contrary to the consensus\", \"allege harbour beliefs that are contrary to the consensus\", \"allege harbour positions that are contrary to the consensus\", \"claim harbour views that are contrary to the consensus\", \"claim harbour opinions that are contrary to the consensus\", \"claim harbour beliefs that are contrary to the consensus\", \"claim harbour positions that are contrary to the consensus\", \"assert harbour views that are contrary to the consensus\", \"assert harbour opinions that are contrary to the consensus\", \"assert harbour beliefs that are contrary to the consensus\", \"assert harbour positions that are contrary to the consensus\", \"contend harbour views that are contrary to the consensus\", \"contend harbour opinions that are contrary to the consensus\", \"contend harbour beliefs that are contrary to the consensus\", \"contend harbour positions that are contrary to the consensus\", \"allege harbour views that are opposed to the consensus\", \"claim harbour views that are opposed to the consensus\", \"assert harbour views that are opposed to the consensus\", \"contend harbour views that are opposed to the consensus\", \"allege harbour opinions that are opposed to the consensus\", \"claim harbour opinions that are opposed to the consensus\", \"assert harbour opinions that are opposed to the consensus\", \"contend harbour opinions that are opposed to the consensus\", \"allege harbour beliefs that are opposed to the consensus\", \"claim harbour beliefs that are opposed to the consensus\", \"assert harbour beliefs that are opposed to the consensus\", \"contend harbour beliefs that are opposed to the consensus\", \"allege harbour positions that are opposed to the consensus\", \"claim harbour positions that are opposed to the consensus\", \"assert harbour positions that are opposed to the consensus\", \"contend harbour positions that are opposed to the consensus\", \"allege harbour views that are antithetical to the consensus\", \"claim harbour views that are antithetical to the consensus\", \"assert harbour views that are antithetical to the consensus\", \"contend harbour views that are antithetical to the consensus\", \"allege harbour opinions that are antithetical to the consensus\", \"claim harbour opinions that are antithetical to the consensus\", \"assert harbour opinions that are antithetical to the consensus\", \"contend harbour opinions that are antithetical to the consensus\", \"allege harbour beliefs that are antithetical to the consensus\", \"claim harbour beliefs that are antithetical to the consensus\", \"assert harbour beliefs that are antithetical to the consensus\", \"contend harbour beliefs that are antithetical to the consensus\", \"allege harbour positions that are antithetical to the consensus\", \"claim harbour positions that are antithetical to the consensus\", \"assert harbour positions that are antithetical to the consensus\", \"contend harbour positions that are antithetical to the consensus\", \"alledge harbor views that are contrary to the consensus\", \"aver harbor views that are contrary to the consensus\", \"allege keep views that are contrary to the consensus\"]\n",
    "same_known_text_1 = keep_before_phrase(same_author_known, same_phrase_1)\n",
    "same_unknown_text_1 = keep_before_phrase(same_author_unknown, same_phrase_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ed87969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_phrase_2 = \"they should have participated in the poll\"\n",
    "same_para_2 = [\"they should've participated in the poll\", \"they should have taken part in the poll\", \"they should've taken part in the poll\", \"they should have partaken in the poll\", \"they should've partaken in the poll\", \"in the poll, they should have participated\", \"in the poll, they should've participated\", \"in the poll, they should have taken part\", \"in the poll, they should've taken part\", \"in the poll, they should have partaken\", \"in the poll, they should've partaken\", \"they should of participated in the poll\", \"they should of taken part in the poll\", \"they should of partaken in the poll\", \"they should have particpated in the poll\", \"they should've particpated in the poll\", \"they should have participated in the vote\", \"they should've participated in the vote\", \"they should have taken part in the vote\", \"they should've taken part in the vote\", \"they should have partaken in the vote\", \"they should've partaken in the vote\", \"in the vote, they should have participated\", \"in the vote, they should've participated\", \"in the vote, they should have taken part\", \"in the vote, they should've taken part\", \"in the vote, they should have partaken\", \"in the vote, they should've partaken\", \"they shoulda participated in the poll\", \"they shoulda taken part in the poll\", \"they shoulda partaken in the poll\"]\n",
    "same_known_text_2 = keep_before_phrase(same_author_known, same_phrase_2)\n",
    "same_unknown_text_2 = keep_before_phrase(same_author_unknown, same_phrase_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2e6a1cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_phrase_3 = \"truly had felt they had\"\n",
    "same_para_3 = [\"truely had felt they had\", \"had truly felt they had\", \"truly had felt they'd\", \"had truly felt they'd\", \"truly had felt they possessed\", \"had truly felt they possessed\", \"truly had believed they had\", \"had truly believed they had\", \"truly had believed they'd\", \"had truly believed they'd\", \"truly had believed they possessed\", \"had truly believed they possessed\", \"truly had thought they had\", \"had truly thought they had\", \"truly had thought they'd\", \"had truly thought they'd\", \"truly had thought they possessed\", \"had truly thought they possessed\", \"truly had reckoned they had\", \"had truly reckoned they had\", \"truly had reckoned they'd\", \"had truly reckoned they'd\", \"truly had reckoned they possessed\", \"had truly reckoned they possessed\", \"truly had supposed they had\", \"had truly supposed they had\", \"truly had supposed they'd\", \"had truly supposed they'd\", \"truly had supposed they possessed\", \"had truly supposed they possessed\", \"truly had figured they had\", \"had truly figured they had\", \"truly had figured they'd\", \"had truly figured they'd\", \"truly had figured they possessed\", \"had truly figured they possessed\", \"really had felt they had\", \"had really felt they had\", \"really had felt they'd\", \"had really felt they'd\", \"really had felt they possessed\", \"had really felt they possessed\", \"really had believed they had\", \"had really believed they had\", \"really had believed they'd\", \"had really believed they'd\", \"really had believed they possessed\", \"had really believed they possessed\", \"really had thought they had\", \"had really thought they had\", \"really had thought they'd\", \"had really thought they'd\", \"really had thought they possessed\", \"had really thought they possessed\", \"really had reckoned they had\", \"had really reckoned they had\", \"really had reckoned they'd\", \"had really reckoned they'd\", \"really had reckoned they possessed\", \"had really reckoned they possessed\", \"really had supposed they had\", \"had really supposed they had\", \"really had supposed they'd\", \"had really supposed they'd\", \"really had supposed they possessed\", \"had really supposed they possessed\", \"really had figured they had\", \"had really figured they had\", \"really had figured they'd\", \"had really figured they'd\", \"really had figured they possessed\", \"had really figured they possessed\", \"genuinely had felt they had\", \"had genuinely felt they had\", \"genuinely had felt they'd\", \"had genuinely felt they'd\", \"genuinely had felt they possessed\", \"had genuinely felt they possessed\", \"genuinely had believed they had\", \"had genuinely believed they had\", \"genuinely had believed they'd\", \"had genuinely believed they'd\", \"genuinely had believed they possessed\", \"had genuinely believed they possessed\", \"genuinely had thought they had\", \"had genuinely thought they had\", \"genuinely had thought they'd\", \"had genuinely thought they'd\", \"genuinely had thought they possessed\", \"had genuinely thought they possessed\", \"genuinely had reckoned they had\", \"had genuinely reckoned they had\", \"genuinely had reckoned they'd\", \"had genuinely reckoned they'd\", \"genuinely had reckoned they possessed\", \"had genuinely reckoned they possessed\", \"genuinely had supposed they had\", \"had genuinely supposed they had\", \"genuinely had supposed they'd\", \"had genuinely supposed they'd\", \"genuinely had supposed they possessed\", \"had genuinely supposed they possessed\", \"genuinely had figured they had\", \"had genuinely figured they had\", \"genuinely had figured they'd\", \"had genuinely figured they'd\", \"genuinely had figured they possessed\", \"had genuinely figured they possessed\", \"indeed had felt they had\", \"had indeed felt they had\", \"indeed had felt they'd\", \"had indeed felt they'd\", \"indeed had felt they possessed\", \"had indeed felt they possessed\", \"indeed had believed they had\", \"had indeed believed they had\", \"indeed had believed they'd\", \"had indeed believed they'd\", \"indeed had believed they possessed\", \"had indeed believed they possessed\", \"indeed had thought they had\", \"had indeed thought they had\", \"indeed had thought they'd\", \"had indeed thought they'd\", \"indeed had thought they possessed\", \"had indeed thought they possessed\", \"indeed had reckoned they had\", \"had indeed reckoned they had\", \"indeed had reckoned they'd\", \"had indeed reckoned they'd\", \"indeed had reckoned they possessed\", \"had indeed reckoned they possessed\", \"indeed had supposed they had\", \"had indeed supposed they had\", \"indeed had supposed they'd\", \"had indeed supposed they'd\", \"indeed had supposed they possessed\", \"had indeed supposed they possessed\", \"indeed had figured they had\", \"had indeed figured they had\", \"indeed had figured they'd\", \"had indeed figured they'd\", \"indeed had figured they possessed\", \"had indeed figured they possessed\", \"actually had felt they had\", \"had actually felt they had\", \"actually had felt they'd\", \"had actually felt they'd\", \"actually had felt they possessed\", \"had actually felt they possessed\", \"actually had believed they had\", \"had actually believed they had\", \"actually had believed they'd\", \"had actually believed they'd\", \"actually had believed they possessed\", \"had actually believed they possessed\", \"actually had thought they had\", \"had actually thought they had\", \"actually had thought they'd\", \"had actually thought they'd\", \"actually had thought they possessed\", \"had actually thought they possessed\", \"actually had reckoned they had\", \"had actually reckoned they had\", \"actually had reckoned they'd\", \"had actually reckoned they'd\", \"actually had reckoned they possessed\", \"had actually reckoned they possessed\", \"actually had supposed they had\", \"had actually supposed they had\", \"actually had supposed they'd\", \"had actually supposed they'd\", \"actually had supposed they possessed\", \"had actually supposed they possessed\", \"actually had figured they had\", \"had actually figured they had\", \"actually had figured they'd\", \"had actually figured they'd\", \"actually had figured they possessed\", \"had actually figured they possessed\", \"truly had had a feeling they had\", \"had truly had a feeling they had\", \"truly had had a feeling they'd\", \"had truly had a feeling they'd\", \"truly had had a feeling they possessed\", \"had truly had a feeling they possessed\", \"really had had a feeling they had\", \"had really had a feeling they had\", \"really had had a feeling they'd\", \"had really had a feeling they'd\", \"really had had a feeling they possessed\", \"had really had a feeling they possessed\"]\n",
    "same_known_text_3 = keep_before_phrase(same_author_known, same_phrase_3)\n",
    "same_unknown_text_3 = keep_before_phrase(same_author_unknown, same_phrase_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3ef80ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_phrase_4 = \"the consensus view\"\n",
    "same_para_4 = [\"the consensus opinion\", \"the consensus position\", \"the consensus stance\", \"the consensus perspective\", \"the consensus viewpoint\", \"the consensus standpoint\", \"the consensus take\", \"the consensus outlook\", \"the consensus judgment\", \"the consensus judgement\", \"the consensus POV\", \"the consensus's view\", \"the consensus' view\", \"the consensus's opinion\", \"the consensus' opinion\", \"the consensus's position\", \"the consensus' position\", \"the consensus's stance\", \"the consensus' stance\", \"the consensus's perspective\", \"the consensus' perspective\", \"the consensus's viewpoint\", \"the consensus' viewpoint\", \"the consensus's standpoint\", \"the consensus' standpoint\", \"the consensus's take\", \"the consensus' take\", \"the consensus's outlook\", \"the consensus' outlook\", \"the consensus's judgment\", \"the consensus' judgment\", \"the consensus's judgement\", \"the consensus' judgement\", \"the consensus's POV\", \"the consensus' POV\", \"the view of the consensus\", \"the opinion of the consensus\", \"the position of the consensus\", \"the stance of the consensus\", \"the perspective of the consensus\", \"the viewpoint of the consensus\", \"the standpoint of the consensus\", \"the take of the consensus\", \"the outlook of the consensus\", \"the judgment of the consensus\", \"the judgement of the consensus\", \"the POV of the consensus\", \"the concensus view\", \"the consencus view\", \"the consensus veiw\", \"the concensus opinion\", \"the consencus opinion\", \"the concensus viewpoint\", \"the consencus viewpoint\", \"the concensus POV\", \"the consencus POV\"]\n",
    "same_known_text_4 = keep_before_phrase(same_author_known, same_phrase_4)\n",
    "same_unknown_text_4 = keep_before_phrase(same_author_unknown, same_phrase_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eea3558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_phrase_5 = \"they elected to\"\n",
    "same_para_5 = [\"they 'elected' to\", \"they chose to\", \"they decided to\", \"they opted to\", \"they resolved to\", \"they determined to\", \"they made the decision to\", \"they made a decision to\", \"they took the decision to\", \"they took a decision to\", \"they reached a decision to\", \"they reached the decision to\", \"they came to a decision to\", \"they came to the decision to\", \"they arrived at a decision to\", \"they made the choice to\", \"they made a choice to\", \"they made the call to\", \"they made up their minds to\", \"they made up their mind to\", \"they made a determination to\", \"they reached a determination to\", \"they elcted to\", \"they decieded to\", \"they optd to\", \"they resovled to\", \"they determind to\", \"they made a desision to\",]\n",
    "same_known_text_5 = keep_before_phrase(same_author_known, same_phrase_5)\n",
    "same_unknown_text_5 = keep_before_phrase(same_author_unknown, same_phrase_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c161c132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "def compute_log_probs_with_median(text: str, tokenizer, model):\n",
    "    \"\"\"\n",
    "    For each token (including the first), returns:\n",
    "      - tokens: list of tokenizer.convert_ids_to_tokens\n",
    "      - log_probs: list of log-probs for each token\n",
    "      - median_logprobs: median log-prob of the distribution at each step\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"]         # shape [1, seq_len]\n",
    "    # --- ALIGN TOKENS CORRECTLY HERE ---\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    logits = outputs.logits                 # [1, seq_len, vocab_size]\n",
    "\n",
    "    log_probs = []\n",
    "    median_logprobs = []\n",
    "    # for each position i, look at logits from i-1 (or the BOS for i=0)\n",
    "    for i in range(input_ids.size(1)):\n",
    "        prev_idx = 0 if i == 0 else i - 1\n",
    "        dist = torch.log_softmax(logits[0, prev_idx], dim=-1)\n",
    "        log_prob = dist[input_ids[0, i]].item()\n",
    "        median_lp = float(dist.median().item())\n",
    "        log_probs.append(log_prob)\n",
    "        median_logprobs.append(median_lp)\n",
    "\n",
    "    return tokens, log_probs, median_logprobs\n",
    "\n",
    "def score_phrases(\n",
    "    base_text: str,\n",
    "    ref_phrase: str,\n",
    "    paraphrases: List[str],\n",
    "    tokenizer,\n",
    "    model\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    1) Score base_text alone → base_total\n",
    "    2) For each phrase (reference + paraphrases):\n",
    "         a) Get its token count by scoring phrase alone\n",
    "         b) Score base_text + phrase → full tokens & log_probs\n",
    "         c) sum_before = sum(full log_probs)\n",
    "         d) phrase_tokens    = last n_phrase tokens of full tokens\n",
    "         e) phrase_log_probs = last n_phrase values of full log_probs\n",
    "         f) phrase_total     = sum(phrase_log_probs)\n",
    "         g) difference       = base_total - sum_before\n",
    "         h) APPEND row\n",
    "    3) Return DataFrame with columns:\n",
    "       phrase_type, phrase, tokens, base_total, sum_before,\n",
    "       log_probs, phrase_total, difference\n",
    "    \"\"\"\n",
    "    # 1) score base_text\n",
    "    print(\"→ Scoring base_text alone…\")\n",
    "    _, log_probs_base, _ = compute_log_probs_with_median(base_text.strip(), tokenizer, model)\n",
    "    base_total = sum(log_probs_base)\n",
    "    print(f\"   base_total = {base_total:.4f}\\n\")\n",
    "\n",
    "    items = [(\"reference\", ref_phrase)] + [(\"paraphrase\", p) for p in paraphrases]\n",
    "    rows = []\n",
    "\n",
    "    for idx, (ptype, phrase) in enumerate(items, start=1):\n",
    "        print(f\"→ [{idx}/{len(items)}] Processing {ptype}…\")\n",
    "\n",
    "        # a) phrase alone → get token count\n",
    "        tokens_phrase, log_probs_phrase, _ = compute_log_probs_with_median(phrase, tokenizer, model)\n",
    "        n_phrase_tokens = len(tokens_phrase)\n",
    "        # b) full sequence\n",
    "        full_text = base_text + phrase\n",
    "        tokens_full, log_probs_full, _ = compute_log_probs_with_median(full_text, tokenizer, model)\n",
    "        # c) full sum\n",
    "        sum_before = sum(log_probs_full)\n",
    "        # d/e) slice last n_phrase_tokens\n",
    "        phrase_tokens    = tokens_full[-n_phrase_tokens:]\n",
    "        phrase_log_probs = log_probs_full[-n_phrase_tokens:]\n",
    "        # f/g) compute sums\n",
    "        phrase_total = sum(phrase_log_probs)\n",
    "        difference   = base_total - sum_before\n",
    "        # h) collect\n",
    "        rows.append({\n",
    "            \"phrase_type\":  ptype,\n",
    "            \"phrase\":       phrase,\n",
    "            \"tokens\":       phrase_tokens,\n",
    "            \"sum_log_probs_base\":   base_total,\n",
    "            \"sum_log_probs_inc_phrase\":   sum_before,\n",
    "            \"difference\":   difference,\n",
    "            \"phrase_log_probs\":    phrase_log_probs,\n",
    "            \"sum_log_probs_phrase\": phrase_total,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\n",
    "        \"phrase_type\", \"phrase\", \"tokens\",\n",
    "        \"sum_log_probs_base\", \"sum_log_probs_inc_phrase\",\n",
    "        \"difference\", \"phrase_log_probs\", \"sum_log_probs_phrase\",\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b0577c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "from typing import Sequence, Union, Optional\n",
    "\n",
    "def _logsumexp(xs: Sequence[float]) -> float:\n",
    "    m = max(xs)\n",
    "    return m + math.log(sum(math.exp(x - m) for x in xs))\n",
    "\n",
    "def add_pmf_column(\n",
    "    df: pd.DataFrame,\n",
    "    logprob_col: str,\n",
    "    priors: Optional[Union[str, Sequence[float]]] = None,\n",
    "    out_col: str = \"pmf\",\n",
    "    keep_logZ: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Treat each row as a candidate. `logprob_col` contains a list of token log-probs per row.\n",
    "    Computes P(row i) ∝ exp(sum(logprobs_i)) * prior_i and writes it to `out_col`.\n",
    "\n",
    "    priors:\n",
    "      - None: uniform\n",
    "      - str: name of a column holding prior probabilities per row\n",
    "      - Sequence[float]: prior probs aligned with df.index\n",
    "    \"\"\"\n",
    "    # sequence log-likelihood per row\n",
    "    L = df[logprob_col].apply(\n",
    "        lambda xs: sum(xs) if isinstance(xs, (list, tuple)) and len(xs) > 0 else float(\"-inf\")\n",
    "    ).tolist()\n",
    "\n",
    "    # apply priors (in probability space)\n",
    "    if priors is None:\n",
    "        L_adj = L\n",
    "    else:\n",
    "        if isinstance(priors, str):\n",
    "            prior_vals = df[priors].tolist()\n",
    "        else:\n",
    "            prior_vals = list(priors)\n",
    "            if len(prior_vals) != len(L):\n",
    "                raise ValueError(\"Length of `priors` must match number of rows.\")\n",
    "        L_adj = [Li + (math.log(p) if (p is not None and p > 0) else float(\"-inf\"))\n",
    "                 for Li, p in zip(L, prior_vals)]\n",
    "\n",
    "    # normalize across rows (stable)\n",
    "    logZ = _logsumexp(L_adj)\n",
    "    pmf = [math.exp(Li - logZ) if Li != float(\"-inf\") else 0.0 for Li in L_adj]\n",
    "\n",
    "    df = df.copy()\n",
    "    df[out_col] = pmf\n",
    "    if keep_logZ:\n",
    "        df[\"_logZ\"] = logZ  # same for all rows\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f5b86153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Scoring base_text alone…\n",
      "   base_total = -167.7171\n",
      "\n",
      "→ [1/195] Processing reference…\n",
      "→ [2/195] Processing paraphrase…\n",
      "→ [3/195] Processing paraphrase…\n",
      "→ [4/195] Processing paraphrase…\n",
      "→ [5/195] Processing paraphrase…\n",
      "→ [6/195] Processing paraphrase…\n",
      "→ [7/195] Processing paraphrase…\n",
      "→ [8/195] Processing paraphrase…\n",
      "→ [9/195] Processing paraphrase…\n",
      "→ [10/195] Processing paraphrase…\n",
      "→ [11/195] Processing paraphrase…\n",
      "→ [12/195] Processing paraphrase…\n",
      "→ [13/195] Processing paraphrase…\n",
      "→ [14/195] Processing paraphrase…\n",
      "→ [15/195] Processing paraphrase…\n",
      "→ [16/195] Processing paraphrase…\n",
      "→ [17/195] Processing paraphrase…\n",
      "→ [18/195] Processing paraphrase…\n",
      "→ [19/195] Processing paraphrase…\n",
      "→ [20/195] Processing paraphrase…\n",
      "→ [21/195] Processing paraphrase…\n",
      "→ [22/195] Processing paraphrase…\n",
      "→ [23/195] Processing paraphrase…\n",
      "→ [24/195] Processing paraphrase…\n",
      "→ [25/195] Processing paraphrase…\n",
      "→ [26/195] Processing paraphrase…\n",
      "→ [27/195] Processing paraphrase…\n",
      "→ [28/195] Processing paraphrase…\n",
      "→ [29/195] Processing paraphrase…\n",
      "→ [30/195] Processing paraphrase…\n",
      "→ [31/195] Processing paraphrase…\n",
      "→ [32/195] Processing paraphrase…\n",
      "→ [33/195] Processing paraphrase…\n",
      "→ [34/195] Processing paraphrase…\n",
      "→ [35/195] Processing paraphrase…\n",
      "→ [36/195] Processing paraphrase…\n",
      "→ [37/195] Processing paraphrase…\n",
      "→ [38/195] Processing paraphrase…\n",
      "→ [39/195] Processing paraphrase…\n",
      "→ [40/195] Processing paraphrase…\n",
      "→ [41/195] Processing paraphrase…\n",
      "→ [42/195] Processing paraphrase…\n",
      "→ [43/195] Processing paraphrase…\n",
      "→ [44/195] Processing paraphrase…\n",
      "→ [45/195] Processing paraphrase…\n",
      "→ [46/195] Processing paraphrase…\n",
      "→ [47/195] Processing paraphrase…\n",
      "→ [48/195] Processing paraphrase…\n",
      "→ [49/195] Processing paraphrase…\n",
      "→ [50/195] Processing paraphrase…\n",
      "→ [51/195] Processing paraphrase…\n",
      "→ [52/195] Processing paraphrase…\n",
      "→ [53/195] Processing paraphrase…\n",
      "→ [54/195] Processing paraphrase…\n",
      "→ [55/195] Processing paraphrase…\n",
      "→ [56/195] Processing paraphrase…\n",
      "→ [57/195] Processing paraphrase…\n",
      "→ [58/195] Processing paraphrase…\n",
      "→ [59/195] Processing paraphrase…\n",
      "→ [60/195] Processing paraphrase…\n",
      "→ [61/195] Processing paraphrase…\n",
      "→ [62/195] Processing paraphrase…\n",
      "→ [63/195] Processing paraphrase…\n",
      "→ [64/195] Processing paraphrase…\n",
      "→ [65/195] Processing paraphrase…\n",
      "→ [66/195] Processing paraphrase…\n",
      "→ [67/195] Processing paraphrase…\n",
      "→ [68/195] Processing paraphrase…\n",
      "→ [69/195] Processing paraphrase…\n",
      "→ [70/195] Processing paraphrase…\n",
      "→ [71/195] Processing paraphrase…\n",
      "→ [72/195] Processing paraphrase…\n",
      "→ [73/195] Processing paraphrase…\n",
      "→ [74/195] Processing paraphrase…\n",
      "→ [75/195] Processing paraphrase…\n",
      "→ [76/195] Processing paraphrase…\n",
      "→ [77/195] Processing paraphrase…\n",
      "→ [78/195] Processing paraphrase…\n",
      "→ [79/195] Processing paraphrase…\n",
      "→ [80/195] Processing paraphrase…\n",
      "→ [81/195] Processing paraphrase…\n",
      "→ [82/195] Processing paraphrase…\n",
      "→ [83/195] Processing paraphrase…\n",
      "→ [84/195] Processing paraphrase…\n",
      "→ [85/195] Processing paraphrase…\n",
      "→ [86/195] Processing paraphrase…\n",
      "→ [87/195] Processing paraphrase…\n",
      "→ [88/195] Processing paraphrase…\n",
      "→ [89/195] Processing paraphrase…\n",
      "→ [90/195] Processing paraphrase…\n",
      "→ [91/195] Processing paraphrase…\n",
      "→ [92/195] Processing paraphrase…\n",
      "→ [93/195] Processing paraphrase…\n",
      "→ [94/195] Processing paraphrase…\n",
      "→ [95/195] Processing paraphrase…\n",
      "→ [96/195] Processing paraphrase…\n",
      "→ [97/195] Processing paraphrase…\n",
      "→ [98/195] Processing paraphrase…\n",
      "→ [99/195] Processing paraphrase…\n",
      "→ [100/195] Processing paraphrase…\n",
      "→ [101/195] Processing paraphrase…\n",
      "→ [102/195] Processing paraphrase…\n",
      "→ [103/195] Processing paraphrase…\n",
      "→ [104/195] Processing paraphrase…\n",
      "→ [105/195] Processing paraphrase…\n",
      "→ [106/195] Processing paraphrase…\n",
      "→ [107/195] Processing paraphrase…\n",
      "→ [108/195] Processing paraphrase…\n",
      "→ [109/195] Processing paraphrase…\n",
      "→ [110/195] Processing paraphrase…\n",
      "→ [111/195] Processing paraphrase…\n",
      "→ [112/195] Processing paraphrase…\n",
      "→ [113/195] Processing paraphrase…\n",
      "→ [114/195] Processing paraphrase…\n",
      "→ [115/195] Processing paraphrase…\n",
      "→ [116/195] Processing paraphrase…\n",
      "→ [117/195] Processing paraphrase…\n",
      "→ [118/195] Processing paraphrase…\n",
      "→ [119/195] Processing paraphrase…\n",
      "→ [120/195] Processing paraphrase…\n",
      "→ [121/195] Processing paraphrase…\n",
      "→ [122/195] Processing paraphrase…\n",
      "→ [123/195] Processing paraphrase…\n",
      "→ [124/195] Processing paraphrase…\n",
      "→ [125/195] Processing paraphrase…\n",
      "→ [126/195] Processing paraphrase…\n",
      "→ [127/195] Processing paraphrase…\n",
      "→ [128/195] Processing paraphrase…\n",
      "→ [129/195] Processing paraphrase…\n",
      "→ [130/195] Processing paraphrase…\n",
      "→ [131/195] Processing paraphrase…\n",
      "→ [132/195] Processing paraphrase…\n",
      "→ [133/195] Processing paraphrase…\n",
      "→ [134/195] Processing paraphrase…\n",
      "→ [135/195] Processing paraphrase…\n",
      "→ [136/195] Processing paraphrase…\n",
      "→ [137/195] Processing paraphrase…\n",
      "→ [138/195] Processing paraphrase…\n",
      "→ [139/195] Processing paraphrase…\n",
      "→ [140/195] Processing paraphrase…\n",
      "→ [141/195] Processing paraphrase…\n",
      "→ [142/195] Processing paraphrase…\n",
      "→ [143/195] Processing paraphrase…\n",
      "→ [144/195] Processing paraphrase…\n",
      "→ [145/195] Processing paraphrase…\n",
      "→ [146/195] Processing paraphrase…\n",
      "→ [147/195] Processing paraphrase…\n",
      "→ [148/195] Processing paraphrase…\n",
      "→ [149/195] Processing paraphrase…\n",
      "→ [150/195] Processing paraphrase…\n",
      "→ [151/195] Processing paraphrase…\n",
      "→ [152/195] Processing paraphrase…\n",
      "→ [153/195] Processing paraphrase…\n",
      "→ [154/195] Processing paraphrase…\n",
      "→ [155/195] Processing paraphrase…\n",
      "→ [156/195] Processing paraphrase…\n",
      "→ [157/195] Processing paraphrase…\n",
      "→ [158/195] Processing paraphrase…\n",
      "→ [159/195] Processing paraphrase…\n",
      "→ [160/195] Processing paraphrase…\n",
      "→ [161/195] Processing paraphrase…\n",
      "→ [162/195] Processing paraphrase…\n",
      "→ [163/195] Processing paraphrase…\n",
      "→ [164/195] Processing paraphrase…\n",
      "→ [165/195] Processing paraphrase…\n",
      "→ [166/195] Processing paraphrase…\n",
      "→ [167/195] Processing paraphrase…\n",
      "→ [168/195] Processing paraphrase…\n",
      "→ [169/195] Processing paraphrase…\n",
      "→ [170/195] Processing paraphrase…\n",
      "→ [171/195] Processing paraphrase…\n",
      "→ [172/195] Processing paraphrase…\n",
      "→ [173/195] Processing paraphrase…\n",
      "→ [174/195] Processing paraphrase…\n",
      "→ [175/195] Processing paraphrase…\n",
      "→ [176/195] Processing paraphrase…\n",
      "→ [177/195] Processing paraphrase…\n",
      "→ [178/195] Processing paraphrase…\n",
      "→ [179/195] Processing paraphrase…\n",
      "→ [180/195] Processing paraphrase…\n",
      "→ [181/195] Processing paraphrase…\n",
      "→ [182/195] Processing paraphrase…\n",
      "→ [183/195] Processing paraphrase…\n",
      "→ [184/195] Processing paraphrase…\n",
      "→ [185/195] Processing paraphrase…\n",
      "→ [186/195] Processing paraphrase…\n",
      "→ [187/195] Processing paraphrase…\n",
      "→ [188/195] Processing paraphrase…\n",
      "→ [189/195] Processing paraphrase…\n",
      "→ [190/195] Processing paraphrase…\n",
      "→ [191/195] Processing paraphrase…\n",
      "→ [192/195] Processing paraphrase…\n",
      "→ [193/195] Processing paraphrase…\n",
      "→ [194/195] Processing paraphrase…\n",
      "→ [195/195] Processing paraphrase…\n",
      "→ Scoring base_text alone…\n",
      "   base_total = -262.6725\n",
      "\n",
      "→ [1/32] Processing reference…\n",
      "→ [2/32] Processing paraphrase…\n",
      "→ [3/32] Processing paraphrase…\n",
      "→ [4/32] Processing paraphrase…\n",
      "→ [5/32] Processing paraphrase…\n",
      "→ [6/32] Processing paraphrase…\n",
      "→ [7/32] Processing paraphrase…\n",
      "→ [8/32] Processing paraphrase…\n",
      "→ [9/32] Processing paraphrase…\n",
      "→ [10/32] Processing paraphrase…\n",
      "→ [11/32] Processing paraphrase…\n",
      "→ [12/32] Processing paraphrase…\n",
      "→ [13/32] Processing paraphrase…\n",
      "→ [14/32] Processing paraphrase…\n",
      "→ [15/32] Processing paraphrase…\n",
      "→ [16/32] Processing paraphrase…\n",
      "→ [17/32] Processing paraphrase…\n",
      "→ [18/32] Processing paraphrase…\n",
      "→ [19/32] Processing paraphrase…\n",
      "→ [20/32] Processing paraphrase…\n",
      "→ [21/32] Processing paraphrase…\n",
      "→ [22/32] Processing paraphrase…\n",
      "→ [23/32] Processing paraphrase…\n",
      "→ [24/32] Processing paraphrase…\n",
      "→ [25/32] Processing paraphrase…\n",
      "→ [26/32] Processing paraphrase…\n",
      "→ [27/32] Processing paraphrase…\n",
      "→ [28/32] Processing paraphrase…\n",
      "→ [29/32] Processing paraphrase…\n",
      "→ [30/32] Processing paraphrase…\n",
      "→ [31/32] Processing paraphrase…\n",
      "→ [32/32] Processing paraphrase…\n",
      "→ Scoring base_text alone…\n",
      "   base_total = -214.5701\n",
      "\n",
      "→ [1/193] Processing reference…\n",
      "→ [2/193] Processing paraphrase…\n",
      "→ [3/193] Processing paraphrase…\n",
      "→ [4/193] Processing paraphrase…\n",
      "→ [5/193] Processing paraphrase…\n",
      "→ [6/193] Processing paraphrase…\n",
      "→ [7/193] Processing paraphrase…\n",
      "→ [8/193] Processing paraphrase…\n",
      "→ [9/193] Processing paraphrase…\n",
      "→ [10/193] Processing paraphrase…\n",
      "→ [11/193] Processing paraphrase…\n",
      "→ [12/193] Processing paraphrase…\n",
      "→ [13/193] Processing paraphrase…\n",
      "→ [14/193] Processing paraphrase…\n",
      "→ [15/193] Processing paraphrase…\n",
      "→ [16/193] Processing paraphrase…\n",
      "→ [17/193] Processing paraphrase…\n",
      "→ [18/193] Processing paraphrase…\n",
      "→ [19/193] Processing paraphrase…\n",
      "→ [20/193] Processing paraphrase…\n",
      "→ [21/193] Processing paraphrase…\n",
      "→ [22/193] Processing paraphrase…\n",
      "→ [23/193] Processing paraphrase…\n",
      "→ [24/193] Processing paraphrase…\n",
      "→ [25/193] Processing paraphrase…\n",
      "→ [26/193] Processing paraphrase…\n",
      "→ [27/193] Processing paraphrase…\n",
      "→ [28/193] Processing paraphrase…\n",
      "→ [29/193] Processing paraphrase…\n",
      "→ [30/193] Processing paraphrase…\n",
      "→ [31/193] Processing paraphrase…\n",
      "→ [32/193] Processing paraphrase…\n",
      "→ [33/193] Processing paraphrase…\n",
      "→ [34/193] Processing paraphrase…\n",
      "→ [35/193] Processing paraphrase…\n",
      "→ [36/193] Processing paraphrase…\n",
      "→ [37/193] Processing paraphrase…\n",
      "→ [38/193] Processing paraphrase…\n",
      "→ [39/193] Processing paraphrase…\n",
      "→ [40/193] Processing paraphrase…\n",
      "→ [41/193] Processing paraphrase…\n",
      "→ [42/193] Processing paraphrase…\n",
      "→ [43/193] Processing paraphrase…\n",
      "→ [44/193] Processing paraphrase…\n",
      "→ [45/193] Processing paraphrase…\n",
      "→ [46/193] Processing paraphrase…\n",
      "→ [47/193] Processing paraphrase…\n",
      "→ [48/193] Processing paraphrase…\n",
      "→ [49/193] Processing paraphrase…\n",
      "→ [50/193] Processing paraphrase…\n",
      "→ [51/193] Processing paraphrase…\n",
      "→ [52/193] Processing paraphrase…\n",
      "→ [53/193] Processing paraphrase…\n",
      "→ [54/193] Processing paraphrase…\n",
      "→ [55/193] Processing paraphrase…\n",
      "→ [56/193] Processing paraphrase…\n",
      "→ [57/193] Processing paraphrase…\n",
      "→ [58/193] Processing paraphrase…\n",
      "→ [59/193] Processing paraphrase…\n",
      "→ [60/193] Processing paraphrase…\n",
      "→ [61/193] Processing paraphrase…\n",
      "→ [62/193] Processing paraphrase…\n",
      "→ [63/193] Processing paraphrase…\n",
      "→ [64/193] Processing paraphrase…\n",
      "→ [65/193] Processing paraphrase…\n",
      "→ [66/193] Processing paraphrase…\n",
      "→ [67/193] Processing paraphrase…\n",
      "→ [68/193] Processing paraphrase…\n",
      "→ [69/193] Processing paraphrase…\n",
      "→ [70/193] Processing paraphrase…\n",
      "→ [71/193] Processing paraphrase…\n",
      "→ [72/193] Processing paraphrase…\n",
      "→ [73/193] Processing paraphrase…\n",
      "→ [74/193] Processing paraphrase…\n",
      "→ [75/193] Processing paraphrase…\n",
      "→ [76/193] Processing paraphrase…\n",
      "→ [77/193] Processing paraphrase…\n",
      "→ [78/193] Processing paraphrase…\n",
      "→ [79/193] Processing paraphrase…\n",
      "→ [80/193] Processing paraphrase…\n",
      "→ [81/193] Processing paraphrase…\n",
      "→ [82/193] Processing paraphrase…\n",
      "→ [83/193] Processing paraphrase…\n",
      "→ [84/193] Processing paraphrase…\n",
      "→ [85/193] Processing paraphrase…\n",
      "→ [86/193] Processing paraphrase…\n",
      "→ [87/193] Processing paraphrase…\n",
      "→ [88/193] Processing paraphrase…\n",
      "→ [89/193] Processing paraphrase…\n",
      "→ [90/193] Processing paraphrase…\n",
      "→ [91/193] Processing paraphrase…\n",
      "→ [92/193] Processing paraphrase…\n",
      "→ [93/193] Processing paraphrase…\n",
      "→ [94/193] Processing paraphrase…\n",
      "→ [95/193] Processing paraphrase…\n",
      "→ [96/193] Processing paraphrase…\n",
      "→ [97/193] Processing paraphrase…\n",
      "→ [98/193] Processing paraphrase…\n",
      "→ [99/193] Processing paraphrase…\n",
      "→ [100/193] Processing paraphrase…\n",
      "→ [101/193] Processing paraphrase…\n",
      "→ [102/193] Processing paraphrase…\n",
      "→ [103/193] Processing paraphrase…\n",
      "→ [104/193] Processing paraphrase…\n",
      "→ [105/193] Processing paraphrase…\n",
      "→ [106/193] Processing paraphrase…\n",
      "→ [107/193] Processing paraphrase…\n",
      "→ [108/193] Processing paraphrase…\n",
      "→ [109/193] Processing paraphrase…\n",
      "→ [110/193] Processing paraphrase…\n",
      "→ [111/193] Processing paraphrase…\n",
      "→ [112/193] Processing paraphrase…\n",
      "→ [113/193] Processing paraphrase…\n",
      "→ [114/193] Processing paraphrase…\n",
      "→ [115/193] Processing paraphrase…\n",
      "→ [116/193] Processing paraphrase…\n",
      "→ [117/193] Processing paraphrase…\n",
      "→ [118/193] Processing paraphrase…\n",
      "→ [119/193] Processing paraphrase…\n",
      "→ [120/193] Processing paraphrase…\n",
      "→ [121/193] Processing paraphrase…\n",
      "→ [122/193] Processing paraphrase…\n",
      "→ [123/193] Processing paraphrase…\n",
      "→ [124/193] Processing paraphrase…\n",
      "→ [125/193] Processing paraphrase…\n",
      "→ [126/193] Processing paraphrase…\n",
      "→ [127/193] Processing paraphrase…\n",
      "→ [128/193] Processing paraphrase…\n",
      "→ [129/193] Processing paraphrase…\n",
      "→ [130/193] Processing paraphrase…\n",
      "→ [131/193] Processing paraphrase…\n",
      "→ [132/193] Processing paraphrase…\n",
      "→ [133/193] Processing paraphrase…\n",
      "→ [134/193] Processing paraphrase…\n",
      "→ [135/193] Processing paraphrase…\n",
      "→ [136/193] Processing paraphrase…\n",
      "→ [137/193] Processing paraphrase…\n",
      "→ [138/193] Processing paraphrase…\n",
      "→ [139/193] Processing paraphrase…\n",
      "→ [140/193] Processing paraphrase…\n",
      "→ [141/193] Processing paraphrase…\n",
      "→ [142/193] Processing paraphrase…\n",
      "→ [143/193] Processing paraphrase…\n",
      "→ [144/193] Processing paraphrase…\n",
      "→ [145/193] Processing paraphrase…\n",
      "→ [146/193] Processing paraphrase…\n",
      "→ [147/193] Processing paraphrase…\n",
      "→ [148/193] Processing paraphrase…\n",
      "→ [149/193] Processing paraphrase…\n",
      "→ [150/193] Processing paraphrase…\n",
      "→ [151/193] Processing paraphrase…\n",
      "→ [152/193] Processing paraphrase…\n",
      "→ [153/193] Processing paraphrase…\n",
      "→ [154/193] Processing paraphrase…\n",
      "→ [155/193] Processing paraphrase…\n",
      "→ [156/193] Processing paraphrase…\n",
      "→ [157/193] Processing paraphrase…\n",
      "→ [158/193] Processing paraphrase…\n",
      "→ [159/193] Processing paraphrase…\n",
      "→ [160/193] Processing paraphrase…\n",
      "→ [161/193] Processing paraphrase…\n",
      "→ [162/193] Processing paraphrase…\n",
      "→ [163/193] Processing paraphrase…\n",
      "→ [164/193] Processing paraphrase…\n",
      "→ [165/193] Processing paraphrase…\n",
      "→ [166/193] Processing paraphrase…\n",
      "→ [167/193] Processing paraphrase…\n",
      "→ [168/193] Processing paraphrase…\n",
      "→ [169/193] Processing paraphrase…\n",
      "→ [170/193] Processing paraphrase…\n",
      "→ [171/193] Processing paraphrase…\n",
      "→ [172/193] Processing paraphrase…\n",
      "→ [173/193] Processing paraphrase…\n",
      "→ [174/193] Processing paraphrase…\n",
      "→ [175/193] Processing paraphrase…\n",
      "→ [176/193] Processing paraphrase…\n",
      "→ [177/193] Processing paraphrase…\n",
      "→ [178/193] Processing paraphrase…\n",
      "→ [179/193] Processing paraphrase…\n",
      "→ [180/193] Processing paraphrase…\n",
      "→ [181/193] Processing paraphrase…\n",
      "→ [182/193] Processing paraphrase…\n",
      "→ [183/193] Processing paraphrase…\n",
      "→ [184/193] Processing paraphrase…\n",
      "→ [185/193] Processing paraphrase…\n",
      "→ [186/193] Processing paraphrase…\n",
      "→ [187/193] Processing paraphrase…\n",
      "→ [188/193] Processing paraphrase…\n",
      "→ [189/193] Processing paraphrase…\n",
      "→ [190/193] Processing paraphrase…\n",
      "→ [191/193] Processing paraphrase…\n",
      "→ [192/193] Processing paraphrase…\n",
      "→ [193/193] Processing paraphrase…\n",
      "→ Scoring base_text alone…\n",
      "   base_total = -892.9659\n",
      "\n",
      "→ [1/57] Processing reference…\n",
      "→ [2/57] Processing paraphrase…\n",
      "→ [3/57] Processing paraphrase…\n",
      "→ [4/57] Processing paraphrase…\n",
      "→ [5/57] Processing paraphrase…\n",
      "→ [6/57] Processing paraphrase…\n",
      "→ [7/57] Processing paraphrase…\n",
      "→ [8/57] Processing paraphrase…\n",
      "→ [9/57] Processing paraphrase…\n",
      "→ [10/57] Processing paraphrase…\n",
      "→ [11/57] Processing paraphrase…\n",
      "→ [12/57] Processing paraphrase…\n",
      "→ [13/57] Processing paraphrase…\n",
      "→ [14/57] Processing paraphrase…\n",
      "→ [15/57] Processing paraphrase…\n",
      "→ [16/57] Processing paraphrase…\n",
      "→ [17/57] Processing paraphrase…\n",
      "→ [18/57] Processing paraphrase…\n",
      "→ [19/57] Processing paraphrase…\n",
      "→ [20/57] Processing paraphrase…\n",
      "→ [21/57] Processing paraphrase…\n",
      "→ [22/57] Processing paraphrase…\n",
      "→ [23/57] Processing paraphrase…\n",
      "→ [24/57] Processing paraphrase…\n",
      "→ [25/57] Processing paraphrase…\n",
      "→ [26/57] Processing paraphrase…\n",
      "→ [27/57] Processing paraphrase…\n",
      "→ [28/57] Processing paraphrase…\n",
      "→ [29/57] Processing paraphrase…\n",
      "→ [30/57] Processing paraphrase…\n",
      "→ [31/57] Processing paraphrase…\n",
      "→ [32/57] Processing paraphrase…\n",
      "→ [33/57] Processing paraphrase…\n",
      "→ [34/57] Processing paraphrase…\n",
      "→ [35/57] Processing paraphrase…\n",
      "→ [36/57] Processing paraphrase…\n",
      "→ [37/57] Processing paraphrase…\n",
      "→ [38/57] Processing paraphrase…\n",
      "→ [39/57] Processing paraphrase…\n",
      "→ [40/57] Processing paraphrase…\n",
      "→ [41/57] Processing paraphrase…\n",
      "→ [42/57] Processing paraphrase…\n",
      "→ [43/57] Processing paraphrase…\n",
      "→ [44/57] Processing paraphrase…\n",
      "→ [45/57] Processing paraphrase…\n",
      "→ [46/57] Processing paraphrase…\n",
      "→ [47/57] Processing paraphrase…\n",
      "→ [48/57] Processing paraphrase…\n",
      "→ [49/57] Processing paraphrase…\n",
      "→ [50/57] Processing paraphrase…\n",
      "→ [51/57] Processing paraphrase…\n",
      "→ [52/57] Processing paraphrase…\n",
      "→ [53/57] Processing paraphrase…\n",
      "→ [54/57] Processing paraphrase…\n",
      "→ [55/57] Processing paraphrase…\n",
      "→ [56/57] Processing paraphrase…\n",
      "→ [57/57] Processing paraphrase…\n",
      "→ Scoring base_text alone…\n",
      "   base_total = -892.9659\n",
      "\n",
      "→ [1/29] Processing reference…\n",
      "→ [2/29] Processing paraphrase…\n",
      "→ [3/29] Processing paraphrase…\n",
      "→ [4/29] Processing paraphrase…\n",
      "→ [5/29] Processing paraphrase…\n",
      "→ [6/29] Processing paraphrase…\n",
      "→ [7/29] Processing paraphrase…\n",
      "→ [8/29] Processing paraphrase…\n",
      "→ [9/29] Processing paraphrase…\n",
      "→ [10/29] Processing paraphrase…\n",
      "→ [11/29] Processing paraphrase…\n",
      "→ [12/29] Processing paraphrase…\n",
      "→ [13/29] Processing paraphrase…\n",
      "→ [14/29] Processing paraphrase…\n",
      "→ [15/29] Processing paraphrase…\n",
      "→ [16/29] Processing paraphrase…\n",
      "→ [17/29] Processing paraphrase…\n",
      "→ [18/29] Processing paraphrase…\n",
      "→ [19/29] Processing paraphrase…\n",
      "→ [20/29] Processing paraphrase…\n",
      "→ [21/29] Processing paraphrase…\n",
      "→ [22/29] Processing paraphrase…\n",
      "→ [23/29] Processing paraphrase…\n",
      "→ [24/29] Processing paraphrase…\n",
      "→ [25/29] Processing paraphrase…\n",
      "→ [26/29] Processing paraphrase…\n",
      "→ [27/29] Processing paraphrase…\n",
      "→ [28/29] Processing paraphrase…\n",
      "→ [29/29] Processing paraphrase…\n"
     ]
    }
   ],
   "source": [
    "same_known_result_1 = score_phrases(same_known_text_1, same_phrase_1, same_para_1, tokenizer, model)\n",
    "same_known_result_2 = score_phrases(same_known_text_2, same_phrase_2, same_para_2, tokenizer, model)\n",
    "same_known_result_3 = score_phrases(same_known_text_3, same_phrase_3, same_para_3, tokenizer, model)\n",
    "same_known_result_4 = score_phrases(same_known_text_4, same_phrase_4, same_para_4, tokenizer, model)\n",
    "same_known_result_5 = score_phrases(same_known_text_5, same_phrase_5, same_para_5, tokenizer, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b6bd0134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Scoring base_text alone…\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[92], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m same_unknown_result_1 \u001b[38;5;241m=\u001b[39m \u001b[43mscore_phrases\u001b[49m\u001b[43m(\u001b[49m\u001b[43msame_unknown_text_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msame_phrase_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msame_para_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m      2\u001b[0m same_unknown_result_2 \u001b[38;5;241m=\u001b[39m score_phrases(same_unknown_text_2, same_phrase_2, same_para_2, tokenizer, model)\n",
      "\u001b[1;32m      3\u001b[0m same_unknown_result_3 \u001b[38;5;241m=\u001b[39m score_phrases(same_unknown_text_3, same_phrase_3, same_para_3, tokenizer, model)\n",
      "\n",
      "Cell \u001b[0;32mIn[80], line 58\u001b[0m, in \u001b[0;36mscore_phrases\u001b[0;34m(base_text, ref_phrase, paraphrases, tokenizer, model)\u001b[0m\n",
      "\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# 1) score base_text\u001b[39;00m\n",
      "\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m→ Scoring base_text alone…\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m---> 58\u001b[0m _, log_probs_base, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_log_probs_with_median\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     59\u001b[0m base_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(log_probs_base)\n",
      "\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   base_total = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_total\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "Cell \u001b[0;32mIn[80], line 18\u001b[0m, in \u001b[0;36mcompute_log_probs_with_median\u001b[0;34m(text, tokenizer, model)\u001b[0m\n",
      "\u001b[1;32m     15\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(input_ids[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;32m---> 18\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     19\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits                 \u001b[38;5;66;03m# [1, seq_len, vocab_size]\u001b[39;00m\n",
      "\u001b[1;32m     21\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m []\n",
      "\n",
      "File \u001b[0;32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "File \u001b[0;32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/transformers/utils/generic.py:943\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    940\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 943\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n",
      "\u001b[1;32m    945\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "\n",
      "File \u001b[0;32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:543\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    538\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[1;32m    539\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n",
      "\u001b[1;32m    540\u001b[0m )\n",
      "\u001b[1;32m    542\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n",
      "\u001b[0;32m--> 543\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    554\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    556\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n",
      "\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "File \u001b[0;32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/transformers/utils/generic.py:943\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    940\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 943\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n",
      "\u001b[1;32m    945\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "\n",
      "File \u001b[0;32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:431\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n",
      "\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n",
      "\u001b[1;32m    429\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "\u001b[0;32m--> 431\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    441\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    443\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\n",
      "File \u001b[0;32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/transformers/modeling_layers.py:83\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     80\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n",
      "\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "File \u001b[0;32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:252\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    250\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "\u001b[1;32m    251\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n",
      "\u001b[0;32m--> 252\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    253\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "\u001b[1;32m    255\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "\n",
      "File \u001b[0;32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "File \u001b[0;32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:48\u001b[0m, in \u001b[0;36mQwen2MLP.forward\u001b[0;34m(self, x)\u001b[0m\n",
      "\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n",
      "\u001b[0;32m---> 48\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\n",
      "File \u001b[0;32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "File \u001b[0;32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n",
      "\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n",
      "\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "same_unknown_result_1 = score_phrases(same_unknown_text_1, same_phrase_1, same_para_1, tokenizer, model)\n",
    "same_unknown_result_2 = score_phrases(same_unknown_text_2, same_phrase_2, same_para_2, tokenizer, model)\n",
    "same_unknown_result_3 = score_phrases(same_unknown_text_3, same_phrase_3, same_para_3, tokenizer, model)\n",
    "same_unknown_result_4 = score_phrases(same_unknown_text_4, same_phrase_4, same_para_4, tokenizer, model)\n",
    "same_unknown_result_5 = score_phrases(same_unknown_text_5, same_phrase_5, same_para_5, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0dc6f2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_pmf_known_1 = add_pmf_column(same_known_result_1, logprob_col='phrase_log_probs')\n",
    "same_pmf_known_2 = add_pmf_column(same_known_result_2, logprob_col='phrase_log_probs')\n",
    "same_pmf_known_3 = add_pmf_column(same_known_result_3, logprob_col='phrase_log_probs')\n",
    "same_pmf_known_4 = add_pmf_column(same_known_result_4, logprob_col='phrase_log_probs')\n",
    "same_pmf_known_5 = add_pmf_column(same_known_result_5, logprob_col='phrase_log_probs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "28861d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_pmf_unknown_1 = add_pmf_column(same_unknown_result_1, logprob_col='phrase_log_probs')\n",
    "same_pmf_unknown_2 = add_pmf_column(same_unknown_result_2, logprob_col='phrase_log_probs')\n",
    "same_pmf_unknown_3 = add_pmf_column(same_unknown_result_3, logprob_col='phrase_log_probs')\n",
    "same_pmf_unknown_4 = add_pmf_column(same_unknown_result_4, logprob_col='phrase_log_probs')\n",
    "same_pmf_unknown_5 = add_pmf_column(same_unknown_result_5, logprob_col='phrase_log_probs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "78cdb979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "same_pmf_unknown_1.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "39260d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_phrases_no_context(\n",
    "    ref_phrase: str,\n",
    "    paraphrases: List[str],\n",
    "    tokenizer,\n",
    "    model\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    1) Score base_text alone → base_total\n",
    "    2) For each phrase (reference + paraphrases):\n",
    "         a) Get its token count by scoring phrase alone\n",
    "         b) Score base_text + phrase → full tokens & log_probs\n",
    "         c) sum_before = sum(full log_probs)\n",
    "         d) phrase_tokens    = last n_phrase tokens of full tokens\n",
    "         e) phrase_log_probs = last n_phrase values of full log_probs\n",
    "         f) phrase_total     = sum(phrase_log_probs)\n",
    "         g) difference       = base_total - sum_before\n",
    "         h) APPEND row\n",
    "    3) Return DataFrame with columns:\n",
    "       phrase_type, phrase, tokens, base_total, sum_before,\n",
    "       log_probs, phrase_total, difference\n",
    "    \"\"\"\n",
    "    # 1) score base_text\n",
    "    items = [(\"reference\", ref_phrase)] + [(\"paraphrase\", p) for p in paraphrases]\n",
    "    rows = []\n",
    "\n",
    "    for idx, (ptype, phrase) in enumerate(items, start=1):\n",
    "        print(f\"→ [{idx}/{len(items)}] Processing {ptype}…\")\n",
    "\n",
    "        # a) phrase alone → get token count\n",
    "        tokens_phrase, log_probs_phrase, _ = compute_log_probs_with_median(phrase, tokenizer, model)\n",
    "        # b) compute sum\n",
    "        phrase_total = sum(log_probs_phrase)\n",
    "        # h) collect\n",
    "        rows.append({\n",
    "            \"phrase_type\":  ptype,\n",
    "            \"phrase\":       phrase,\n",
    "            \"tokens\":       tokens_phrase,\n",
    "            \"log_probs\":    log_probs_phrase,\n",
    "            \"sum_log_probs\": phrase_total,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\n",
    "        \"phrase_type\", \"phrase\", \"tokens\",\n",
    "        \"log_probs\", \"sum_log_probs\",\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "71e2307f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ [1/195] Processing reference…\n",
      "→ [2/195] Processing paraphrase…\n",
      "→ [3/195] Processing paraphrase…\n",
      "→ [4/195] Processing paraphrase…\n",
      "→ [5/195] Processing paraphrase…\n",
      "→ [6/195] Processing paraphrase…\n",
      "→ [7/195] Processing paraphrase…\n",
      "→ [8/195] Processing paraphrase…\n",
      "→ [9/195] Processing paraphrase…\n",
      "→ [10/195] Processing paraphrase…\n",
      "→ [11/195] Processing paraphrase…\n",
      "→ [12/195] Processing paraphrase…\n",
      "→ [13/195] Processing paraphrase…\n",
      "→ [14/195] Processing paraphrase…\n",
      "→ [15/195] Processing paraphrase…\n",
      "→ [16/195] Processing paraphrase…\n",
      "→ [17/195] Processing paraphrase…\n",
      "→ [18/195] Processing paraphrase…\n",
      "→ [19/195] Processing paraphrase…\n",
      "→ [20/195] Processing paraphrase…\n",
      "→ [21/195] Processing paraphrase…\n",
      "→ [22/195] Processing paraphrase…\n",
      "→ [23/195] Processing paraphrase…\n",
      "→ [24/195] Processing paraphrase…\n",
      "→ [25/195] Processing paraphrase…\n",
      "→ [26/195] Processing paraphrase…\n",
      "→ [27/195] Processing paraphrase…\n",
      "→ [28/195] Processing paraphrase…\n",
      "→ [29/195] Processing paraphrase…\n",
      "→ [30/195] Processing paraphrase…\n",
      "→ [31/195] Processing paraphrase…\n",
      "→ [32/195] Processing paraphrase…\n",
      "→ [33/195] Processing paraphrase…\n",
      "→ [34/195] Processing paraphrase…\n",
      "→ [35/195] Processing paraphrase…\n",
      "→ [36/195] Processing paraphrase…\n",
      "→ [37/195] Processing paraphrase…\n",
      "→ [38/195] Processing paraphrase…\n",
      "→ [39/195] Processing paraphrase…\n",
      "→ [40/195] Processing paraphrase…\n",
      "→ [41/195] Processing paraphrase…\n",
      "→ [42/195] Processing paraphrase…\n",
      "→ [43/195] Processing paraphrase…\n",
      "→ [44/195] Processing paraphrase…\n",
      "→ [45/195] Processing paraphrase…\n",
      "→ [46/195] Processing paraphrase…\n",
      "→ [47/195] Processing paraphrase…\n",
      "→ [48/195] Processing paraphrase…\n",
      "→ [49/195] Processing paraphrase…\n",
      "→ [50/195] Processing paraphrase…\n",
      "→ [51/195] Processing paraphrase…\n",
      "→ [52/195] Processing paraphrase…\n",
      "→ [53/195] Processing paraphrase…\n",
      "→ [54/195] Processing paraphrase…\n",
      "→ [55/195] Processing paraphrase…\n",
      "→ [56/195] Processing paraphrase…\n",
      "→ [57/195] Processing paraphrase…\n",
      "→ [58/195] Processing paraphrase…\n",
      "→ [59/195] Processing paraphrase…\n",
      "→ [60/195] Processing paraphrase…\n",
      "→ [61/195] Processing paraphrase…\n",
      "→ [62/195] Processing paraphrase…\n",
      "→ [63/195] Processing paraphrase…\n",
      "→ [64/195] Processing paraphrase…\n",
      "→ [65/195] Processing paraphrase…\n",
      "→ [66/195] Processing paraphrase…\n",
      "→ [67/195] Processing paraphrase…\n",
      "→ [68/195] Processing paraphrase…\n",
      "→ [69/195] Processing paraphrase…\n",
      "→ [70/195] Processing paraphrase…\n",
      "→ [71/195] Processing paraphrase…\n",
      "→ [72/195] Processing paraphrase…\n",
      "→ [73/195] Processing paraphrase…\n",
      "→ [74/195] Processing paraphrase…\n",
      "→ [75/195] Processing paraphrase…\n",
      "→ [76/195] Processing paraphrase…\n",
      "→ [77/195] Processing paraphrase…\n",
      "→ [78/195] Processing paraphrase…\n",
      "→ [79/195] Processing paraphrase…\n",
      "→ [80/195] Processing paraphrase…\n",
      "→ [81/195] Processing paraphrase…\n",
      "→ [82/195] Processing paraphrase…\n",
      "→ [83/195] Processing paraphrase…\n",
      "→ [84/195] Processing paraphrase…\n",
      "→ [85/195] Processing paraphrase…\n",
      "→ [86/195] Processing paraphrase…\n",
      "→ [87/195] Processing paraphrase…\n",
      "→ [88/195] Processing paraphrase…\n",
      "→ [89/195] Processing paraphrase…\n",
      "→ [90/195] Processing paraphrase…\n",
      "→ [91/195] Processing paraphrase…\n",
      "→ [92/195] Processing paraphrase…\n",
      "→ [93/195] Processing paraphrase…\n",
      "→ [94/195] Processing paraphrase…\n",
      "→ [95/195] Processing paraphrase…\n",
      "→ [96/195] Processing paraphrase…\n",
      "→ [97/195] Processing paraphrase…\n",
      "→ [98/195] Processing paraphrase…\n",
      "→ [99/195] Processing paraphrase…\n",
      "→ [100/195] Processing paraphrase…\n",
      "→ [101/195] Processing paraphrase…\n",
      "→ [102/195] Processing paraphrase…\n",
      "→ [103/195] Processing paraphrase…\n",
      "→ [104/195] Processing paraphrase…\n",
      "→ [105/195] Processing paraphrase…\n",
      "→ [106/195] Processing paraphrase…\n",
      "→ [107/195] Processing paraphrase…\n",
      "→ [108/195] Processing paraphrase…\n",
      "→ [109/195] Processing paraphrase…\n",
      "→ [110/195] Processing paraphrase…\n",
      "→ [111/195] Processing paraphrase…\n",
      "→ [112/195] Processing paraphrase…\n",
      "→ [113/195] Processing paraphrase…\n",
      "→ [114/195] Processing paraphrase…\n",
      "→ [115/195] Processing paraphrase…\n",
      "→ [116/195] Processing paraphrase…\n",
      "→ [117/195] Processing paraphrase…\n",
      "→ [118/195] Processing paraphrase…\n",
      "→ [119/195] Processing paraphrase…\n",
      "→ [120/195] Processing paraphrase…\n",
      "→ [121/195] Processing paraphrase…\n",
      "→ [122/195] Processing paraphrase…\n",
      "→ [123/195] Processing paraphrase…\n",
      "→ [124/195] Processing paraphrase…\n",
      "→ [125/195] Processing paraphrase…\n",
      "→ [126/195] Processing paraphrase…\n",
      "→ [127/195] Processing paraphrase…\n",
      "→ [128/195] Processing paraphrase…\n",
      "→ [129/195] Processing paraphrase…\n",
      "→ [130/195] Processing paraphrase…\n",
      "→ [131/195] Processing paraphrase…\n",
      "→ [132/195] Processing paraphrase…\n",
      "→ [133/195] Processing paraphrase…\n",
      "→ [134/195] Processing paraphrase…\n",
      "→ [135/195] Processing paraphrase…\n",
      "→ [136/195] Processing paraphrase…\n",
      "→ [137/195] Processing paraphrase…\n",
      "→ [138/195] Processing paraphrase…\n",
      "→ [139/195] Processing paraphrase…\n",
      "→ [140/195] Processing paraphrase…\n",
      "→ [141/195] Processing paraphrase…\n",
      "→ [142/195] Processing paraphrase…\n",
      "→ [143/195] Processing paraphrase…\n",
      "→ [144/195] Processing paraphrase…\n",
      "→ [145/195] Processing paraphrase…\n",
      "→ [146/195] Processing paraphrase…\n",
      "→ [147/195] Processing paraphrase…\n",
      "→ [148/195] Processing paraphrase…\n",
      "→ [149/195] Processing paraphrase…\n",
      "→ [150/195] Processing paraphrase…\n",
      "→ [151/195] Processing paraphrase…\n",
      "→ [152/195] Processing paraphrase…\n",
      "→ [153/195] Processing paraphrase…\n",
      "→ [154/195] Processing paraphrase…\n",
      "→ [155/195] Processing paraphrase…\n",
      "→ [156/195] Processing paraphrase…\n",
      "→ [157/195] Processing paraphrase…\n",
      "→ [158/195] Processing paraphrase…\n",
      "→ [159/195] Processing paraphrase…\n",
      "→ [160/195] Processing paraphrase…\n",
      "→ [161/195] Processing paraphrase…\n",
      "→ [162/195] Processing paraphrase…\n",
      "→ [163/195] Processing paraphrase…\n",
      "→ [164/195] Processing paraphrase…\n",
      "→ [165/195] Processing paraphrase…\n",
      "→ [166/195] Processing paraphrase…\n",
      "→ [167/195] Processing paraphrase…\n",
      "→ [168/195] Processing paraphrase…\n",
      "→ [169/195] Processing paraphrase…\n",
      "→ [170/195] Processing paraphrase…\n",
      "→ [171/195] Processing paraphrase…\n",
      "→ [172/195] Processing paraphrase…\n",
      "→ [173/195] Processing paraphrase…\n",
      "→ [174/195] Processing paraphrase…\n",
      "→ [175/195] Processing paraphrase…\n",
      "→ [176/195] Processing paraphrase…\n",
      "→ [177/195] Processing paraphrase…\n",
      "→ [178/195] Processing paraphrase…\n",
      "→ [179/195] Processing paraphrase…\n",
      "→ [180/195] Processing paraphrase…\n",
      "→ [181/195] Processing paraphrase…\n",
      "→ [182/195] Processing paraphrase…\n",
      "→ [183/195] Processing paraphrase…\n",
      "→ [184/195] Processing paraphrase…\n",
      "→ [185/195] Processing paraphrase…\n",
      "→ [186/195] Processing paraphrase…\n",
      "→ [187/195] Processing paraphrase…\n",
      "→ [188/195] Processing paraphrase…\n",
      "→ [189/195] Processing paraphrase…\n",
      "→ [190/195] Processing paraphrase…\n",
      "→ [191/195] Processing paraphrase…\n",
      "→ [192/195] Processing paraphrase…\n",
      "→ [193/195] Processing paraphrase…\n",
      "→ [194/195] Processing paraphrase…\n",
      "→ [195/195] Processing paraphrase…\n",
      "→ [1/32] Processing reference…\n",
      "→ [2/32] Processing paraphrase…\n",
      "→ [3/32] Processing paraphrase…\n",
      "→ [4/32] Processing paraphrase…\n",
      "→ [5/32] Processing paraphrase…\n",
      "→ [6/32] Processing paraphrase…\n",
      "→ [7/32] Processing paraphrase…\n",
      "→ [8/32] Processing paraphrase…\n",
      "→ [9/32] Processing paraphrase…\n",
      "→ [10/32] Processing paraphrase…\n",
      "→ [11/32] Processing paraphrase…\n",
      "→ [12/32] Processing paraphrase…\n",
      "→ [13/32] Processing paraphrase…\n",
      "→ [14/32] Processing paraphrase…\n",
      "→ [15/32] Processing paraphrase…\n",
      "→ [16/32] Processing paraphrase…\n",
      "→ [17/32] Processing paraphrase…\n",
      "→ [18/32] Processing paraphrase…\n",
      "→ [19/32] Processing paraphrase…\n",
      "→ [20/32] Processing paraphrase…\n",
      "→ [21/32] Processing paraphrase…\n",
      "→ [22/32] Processing paraphrase…\n",
      "→ [23/32] Processing paraphrase…\n",
      "→ [24/32] Processing paraphrase…\n",
      "→ [25/32] Processing paraphrase…\n",
      "→ [26/32] Processing paraphrase…\n",
      "→ [27/32] Processing paraphrase…\n",
      "→ [28/32] Processing paraphrase…\n",
      "→ [29/32] Processing paraphrase…\n",
      "→ [30/32] Processing paraphrase…\n",
      "→ [31/32] Processing paraphrase…\n",
      "→ [32/32] Processing paraphrase…\n",
      "→ [1/193] Processing reference…\n",
      "→ [2/193] Processing paraphrase…\n",
      "→ [3/193] Processing paraphrase…\n",
      "→ [4/193] Processing paraphrase…\n",
      "→ [5/193] Processing paraphrase…\n",
      "→ [6/193] Processing paraphrase…\n",
      "→ [7/193] Processing paraphrase…\n",
      "→ [8/193] Processing paraphrase…\n",
      "→ [9/193] Processing paraphrase…\n",
      "→ [10/193] Processing paraphrase…\n",
      "→ [11/193] Processing paraphrase…\n",
      "→ [12/193] Processing paraphrase…\n",
      "→ [13/193] Processing paraphrase…\n",
      "→ [14/193] Processing paraphrase…\n",
      "→ [15/193] Processing paraphrase…\n",
      "→ [16/193] Processing paraphrase…\n",
      "→ [17/193] Processing paraphrase…\n",
      "→ [18/193] Processing paraphrase…\n",
      "→ [19/193] Processing paraphrase…\n",
      "→ [20/193] Processing paraphrase…\n",
      "→ [21/193] Processing paraphrase…\n",
      "→ [22/193] Processing paraphrase…\n",
      "→ [23/193] Processing paraphrase…\n",
      "→ [24/193] Processing paraphrase…\n",
      "→ [25/193] Processing paraphrase…\n",
      "→ [26/193] Processing paraphrase…\n",
      "→ [27/193] Processing paraphrase…\n",
      "→ [28/193] Processing paraphrase…\n",
      "→ [29/193] Processing paraphrase…\n",
      "→ [30/193] Processing paraphrase…\n",
      "→ [31/193] Processing paraphrase…\n",
      "→ [32/193] Processing paraphrase…\n",
      "→ [33/193] Processing paraphrase…\n",
      "→ [34/193] Processing paraphrase…\n",
      "→ [35/193] Processing paraphrase…\n",
      "→ [36/193] Processing paraphrase…\n",
      "→ [37/193] Processing paraphrase…\n",
      "→ [38/193] Processing paraphrase…\n",
      "→ [39/193] Processing paraphrase…\n",
      "→ [40/193] Processing paraphrase…\n",
      "→ [41/193] Processing paraphrase…\n",
      "→ [42/193] Processing paraphrase…\n",
      "→ [43/193] Processing paraphrase…\n",
      "→ [44/193] Processing paraphrase…\n",
      "→ [45/193] Processing paraphrase…\n",
      "→ [46/193] Processing paraphrase…\n",
      "→ [47/193] Processing paraphrase…\n",
      "→ [48/193] Processing paraphrase…\n",
      "→ [49/193] Processing paraphrase…\n",
      "→ [50/193] Processing paraphrase…\n",
      "→ [51/193] Processing paraphrase…\n",
      "→ [52/193] Processing paraphrase…\n",
      "→ [53/193] Processing paraphrase…\n",
      "→ [54/193] Processing paraphrase…\n",
      "→ [55/193] Processing paraphrase…\n",
      "→ [56/193] Processing paraphrase…\n",
      "→ [57/193] Processing paraphrase…\n",
      "→ [58/193] Processing paraphrase…\n",
      "→ [59/193] Processing paraphrase…\n",
      "→ [60/193] Processing paraphrase…\n",
      "→ [61/193] Processing paraphrase…\n",
      "→ [62/193] Processing paraphrase…\n",
      "→ [63/193] Processing paraphrase…\n",
      "→ [64/193] Processing paraphrase…\n",
      "→ [65/193] Processing paraphrase…\n",
      "→ [66/193] Processing paraphrase…\n",
      "→ [67/193] Processing paraphrase…\n",
      "→ [68/193] Processing paraphrase…\n",
      "→ [69/193] Processing paraphrase…\n",
      "→ [70/193] Processing paraphrase…\n",
      "→ [71/193] Processing paraphrase…\n",
      "→ [72/193] Processing paraphrase…\n",
      "→ [73/193] Processing paraphrase…\n",
      "→ [74/193] Processing paraphrase…\n",
      "→ [75/193] Processing paraphrase…\n",
      "→ [76/193] Processing paraphrase…\n",
      "→ [77/193] Processing paraphrase…\n",
      "→ [78/193] Processing paraphrase…\n",
      "→ [79/193] Processing paraphrase…\n",
      "→ [80/193] Processing paraphrase…\n",
      "→ [81/193] Processing paraphrase…\n",
      "→ [82/193] Processing paraphrase…\n",
      "→ [83/193] Processing paraphrase…\n",
      "→ [84/193] Processing paraphrase…\n",
      "→ [85/193] Processing paraphrase…\n",
      "→ [86/193] Processing paraphrase…\n",
      "→ [87/193] Processing paraphrase…\n",
      "→ [88/193] Processing paraphrase…\n",
      "→ [89/193] Processing paraphrase…\n",
      "→ [90/193] Processing paraphrase…\n",
      "→ [91/193] Processing paraphrase…\n",
      "→ [92/193] Processing paraphrase…\n",
      "→ [93/193] Processing paraphrase…\n",
      "→ [94/193] Processing paraphrase…\n",
      "→ [95/193] Processing paraphrase…\n",
      "→ [96/193] Processing paraphrase…\n",
      "→ [97/193] Processing paraphrase…\n",
      "→ [98/193] Processing paraphrase…\n",
      "→ [99/193] Processing paraphrase…\n",
      "→ [100/193] Processing paraphrase…\n",
      "→ [101/193] Processing paraphrase…\n",
      "→ [102/193] Processing paraphrase…\n",
      "→ [103/193] Processing paraphrase…\n",
      "→ [104/193] Processing paraphrase…\n",
      "→ [105/193] Processing paraphrase…\n",
      "→ [106/193] Processing paraphrase…\n",
      "→ [107/193] Processing paraphrase…\n",
      "→ [108/193] Processing paraphrase…\n",
      "→ [109/193] Processing paraphrase…\n",
      "→ [110/193] Processing paraphrase…\n",
      "→ [111/193] Processing paraphrase…\n",
      "→ [112/193] Processing paraphrase…\n",
      "→ [113/193] Processing paraphrase…\n",
      "→ [114/193] Processing paraphrase…\n",
      "→ [115/193] Processing paraphrase…\n",
      "→ [116/193] Processing paraphrase…\n",
      "→ [117/193] Processing paraphrase…\n",
      "→ [118/193] Processing paraphrase…\n",
      "→ [119/193] Processing paraphrase…\n",
      "→ [120/193] Processing paraphrase…\n",
      "→ [121/193] Processing paraphrase…\n",
      "→ [122/193] Processing paraphrase…\n",
      "→ [123/193] Processing paraphrase…\n",
      "→ [124/193] Processing paraphrase…\n",
      "→ [125/193] Processing paraphrase…\n",
      "→ [126/193] Processing paraphrase…\n",
      "→ [127/193] Processing paraphrase…\n",
      "→ [128/193] Processing paraphrase…\n",
      "→ [129/193] Processing paraphrase…\n",
      "→ [130/193] Processing paraphrase…\n",
      "→ [131/193] Processing paraphrase…\n",
      "→ [132/193] Processing paraphrase…\n",
      "→ [133/193] Processing paraphrase…\n",
      "→ [134/193] Processing paraphrase…\n",
      "→ [135/193] Processing paraphrase…\n",
      "→ [136/193] Processing paraphrase…\n",
      "→ [137/193] Processing paraphrase…\n",
      "→ [138/193] Processing paraphrase…\n",
      "→ [139/193] Processing paraphrase…\n",
      "→ [140/193] Processing paraphrase…\n",
      "→ [141/193] Processing paraphrase…\n",
      "→ [142/193] Processing paraphrase…\n",
      "→ [143/193] Processing paraphrase…\n",
      "→ [144/193] Processing paraphrase…\n",
      "→ [145/193] Processing paraphrase…\n",
      "→ [146/193] Processing paraphrase…\n",
      "→ [147/193] Processing paraphrase…\n",
      "→ [148/193] Processing paraphrase…\n",
      "→ [149/193] Processing paraphrase…\n",
      "→ [150/193] Processing paraphrase…\n",
      "→ [151/193] Processing paraphrase…\n",
      "→ [152/193] Processing paraphrase…\n",
      "→ [153/193] Processing paraphrase…\n",
      "→ [154/193] Processing paraphrase…\n",
      "→ [155/193] Processing paraphrase…\n",
      "→ [156/193] Processing paraphrase…\n",
      "→ [157/193] Processing paraphrase…\n",
      "→ [158/193] Processing paraphrase…\n",
      "→ [159/193] Processing paraphrase…\n",
      "→ [160/193] Processing paraphrase…\n",
      "→ [161/193] Processing paraphrase…\n",
      "→ [162/193] Processing paraphrase…\n",
      "→ [163/193] Processing paraphrase…\n",
      "→ [164/193] Processing paraphrase…\n",
      "→ [165/193] Processing paraphrase…\n",
      "→ [166/193] Processing paraphrase…\n",
      "→ [167/193] Processing paraphrase…\n",
      "→ [168/193] Processing paraphrase…\n",
      "→ [169/193] Processing paraphrase…\n",
      "→ [170/193] Processing paraphrase…\n",
      "→ [171/193] Processing paraphrase…\n",
      "→ [172/193] Processing paraphrase…\n",
      "→ [173/193] Processing paraphrase…\n",
      "→ [174/193] Processing paraphrase…\n",
      "→ [175/193] Processing paraphrase…\n",
      "→ [176/193] Processing paraphrase…\n",
      "→ [177/193] Processing paraphrase…\n",
      "→ [178/193] Processing paraphrase…\n",
      "→ [179/193] Processing paraphrase…\n",
      "→ [180/193] Processing paraphrase…\n",
      "→ [181/193] Processing paraphrase…\n",
      "→ [182/193] Processing paraphrase…\n",
      "→ [183/193] Processing paraphrase…\n",
      "→ [184/193] Processing paraphrase…\n",
      "→ [185/193] Processing paraphrase…\n",
      "→ [186/193] Processing paraphrase…\n",
      "→ [187/193] Processing paraphrase…\n",
      "→ [188/193] Processing paraphrase…\n",
      "→ [189/193] Processing paraphrase…\n",
      "→ [190/193] Processing paraphrase…\n",
      "→ [191/193] Processing paraphrase…\n",
      "→ [192/193] Processing paraphrase…\n",
      "→ [193/193] Processing paraphrase…\n",
      "→ [1/57] Processing reference…\n",
      "→ [2/57] Processing paraphrase…\n",
      "→ [3/57] Processing paraphrase…\n",
      "→ [4/57] Processing paraphrase…\n",
      "→ [5/57] Processing paraphrase…\n",
      "→ [6/57] Processing paraphrase…\n",
      "→ [7/57] Processing paraphrase…\n",
      "→ [8/57] Processing paraphrase…\n",
      "→ [9/57] Processing paraphrase…\n",
      "→ [10/57] Processing paraphrase…\n",
      "→ [11/57] Processing paraphrase…\n",
      "→ [12/57] Processing paraphrase…\n",
      "→ [13/57] Processing paraphrase…\n",
      "→ [14/57] Processing paraphrase…\n",
      "→ [15/57] Processing paraphrase…\n",
      "→ [16/57] Processing paraphrase…\n",
      "→ [17/57] Processing paraphrase…\n",
      "→ [18/57] Processing paraphrase…\n",
      "→ [19/57] Processing paraphrase…\n",
      "→ [20/57] Processing paraphrase…\n",
      "→ [21/57] Processing paraphrase…\n",
      "→ [22/57] Processing paraphrase…\n",
      "→ [23/57] Processing paraphrase…\n",
      "→ [24/57] Processing paraphrase…\n",
      "→ [25/57] Processing paraphrase…\n",
      "→ [26/57] Processing paraphrase…\n",
      "→ [27/57] Processing paraphrase…\n",
      "→ [28/57] Processing paraphrase…\n",
      "→ [29/57] Processing paraphrase…\n",
      "→ [30/57] Processing paraphrase…\n",
      "→ [31/57] Processing paraphrase…\n",
      "→ [32/57] Processing paraphrase…\n",
      "→ [33/57] Processing paraphrase…\n",
      "→ [34/57] Processing paraphrase…\n",
      "→ [35/57] Processing paraphrase…\n",
      "→ [36/57] Processing paraphrase…\n",
      "→ [37/57] Processing paraphrase…\n",
      "→ [38/57] Processing paraphrase…\n",
      "→ [39/57] Processing paraphrase…\n",
      "→ [40/57] Processing paraphrase…\n",
      "→ [41/57] Processing paraphrase…\n",
      "→ [42/57] Processing paraphrase…\n",
      "→ [43/57] Processing paraphrase…\n",
      "→ [44/57] Processing paraphrase…\n",
      "→ [45/57] Processing paraphrase…\n",
      "→ [46/57] Processing paraphrase…\n",
      "→ [47/57] Processing paraphrase…\n",
      "→ [48/57] Processing paraphrase…\n",
      "→ [49/57] Processing paraphrase…\n",
      "→ [50/57] Processing paraphrase…\n",
      "→ [51/57] Processing paraphrase…\n",
      "→ [52/57] Processing paraphrase…\n",
      "→ [53/57] Processing paraphrase…\n",
      "→ [54/57] Processing paraphrase…\n",
      "→ [55/57] Processing paraphrase…\n",
      "→ [56/57] Processing paraphrase…\n",
      "→ [57/57] Processing paraphrase…\n",
      "→ [1/29] Processing reference…\n",
      "→ [2/29] Processing paraphrase…\n",
      "→ [3/29] Processing paraphrase…\n",
      "→ [4/29] Processing paraphrase…\n",
      "→ [5/29] Processing paraphrase…\n",
      "→ [6/29] Processing paraphrase…\n",
      "→ [7/29] Processing paraphrase…\n",
      "→ [8/29] Processing paraphrase…\n",
      "→ [9/29] Processing paraphrase…\n",
      "→ [10/29] Processing paraphrase…\n",
      "→ [11/29] Processing paraphrase…\n",
      "→ [12/29] Processing paraphrase…\n",
      "→ [13/29] Processing paraphrase…\n",
      "→ [14/29] Processing paraphrase…\n",
      "→ [15/29] Processing paraphrase…\n",
      "→ [16/29] Processing paraphrase…\n",
      "→ [17/29] Processing paraphrase…\n",
      "→ [18/29] Processing paraphrase…\n",
      "→ [19/29] Processing paraphrase…\n",
      "→ [20/29] Processing paraphrase…\n",
      "→ [21/29] Processing paraphrase…\n",
      "→ [22/29] Processing paraphrase…\n",
      "→ [23/29] Processing paraphrase…\n",
      "→ [24/29] Processing paraphrase…\n",
      "→ [25/29] Processing paraphrase…\n",
      "→ [26/29] Processing paraphrase…\n",
      "→ [27/29] Processing paraphrase…\n",
      "→ [28/29] Processing paraphrase…\n",
      "→ [29/29] Processing paraphrase…\n"
     ]
    }
   ],
   "source": [
    "phrase_1_no_context = score_phrases_no_context(same_phrase_1, same_para_1, tokenizer, model)\n",
    "phrase_2_no_context = score_phrases_no_context(same_phrase_2, same_para_2, tokenizer, model)\n",
    "phrase_3_no_context = score_phrases_no_context(same_phrase_3, same_para_3, tokenizer, model)\n",
    "phrase_4_no_context = score_phrases_no_context(same_phrase_4, same_para_4, tokenizer, model)\n",
    "phrase_5_no_context = score_phrases_no_context(same_phrase_5, same_para_5, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "de720cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmf_phrase_1 = add_pmf_column(phrase_1_no_context, logprob_col='log_probs')\n",
    "pmf_phrase_2 = add_pmf_column(phrase_2_no_context, logprob_col='log_probs')\n",
    "pmf_phrase_3 = add_pmf_column(phrase_3_no_context, logprob_col='log_probs')\n",
    "pmf_phrase_4 = add_pmf_column(phrase_4_no_context, logprob_col='log_probs')\n",
    "pmf_phrase_5 = add_pmf_column(phrase_5_no_context, logprob_col='log_probs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "80ad1a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "pmf_phrase_5.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e7a5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "phrase_no_context_results.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebe9711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'p_ref': 0.02942703608435714,\n",
       " 'pmf': [0.02942703608435714,\n",
       "  0.08518705975877248,\n",
       "  0.00153309522815474,\n",
       "  0.02223181571976897,\n",
       "  0.49502898376941395,\n",
       "  0.006620540822840208,\n",
       "  0.05334360039450061,\n",
       "  0.005839424315377672,\n",
       "  0.19029799722300392,\n",
       "  0.0279927686630204,\n",
       "  0.00036593318764960883,\n",
       "  4.4279190953408884e-07,\n",
       "  0.016561591261916014,\n",
       "  0.012231002196408955,\n",
       "  0.015443991910654193,\n",
       "  0.015744169946309096,\n",
       "  0.01598870792759976,\n",
       "  0.0019328462398086792,\n",
       "  0.00202757444268726,\n",
       "  0.00030843112278552016,\n",
       "  0.0005150109174092671,\n",
       "  0.0007774717419987056,\n",
       "  0.00018063978406505414,\n",
       "  0.00011064914097456609,\n",
       "  0.00016519444097009707,\n",
       "  2.539807869046445e-05,\n",
       "  1.2079753541652437e-05,\n",
       "  3.586496373663749e-05,\n",
       "  4.6547386795370134e-05,\n",
       "  9.675178073893586e-06,\n",
       "  1.0786488684721038e-05,\n",
       "  1.4202459703150708e-07,\n",
       "  1.32861536726391e-06,\n",
       "  1.0233554992817302e-10,\n",
       "  5.5139786685459756e-14,\n",
       "  2.1847368120972857e-06,\n",
       "  1.3638956227979283e-08],\n",
       " 'log_den': -33.29816781462455,\n",
       " 'den': 3.4577171230513927e-15,\n",
       " 'llr_ref_vs_rest': -3.4959727350786665,\n",
       " 'odds_ref': 0.03031924149796808}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_metrics(phrase_no_context_results['log_probs'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4895eff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
