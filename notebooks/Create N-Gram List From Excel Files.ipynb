{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af7bd658",
   "metadata": {},
   "source": [
    "# Create N-Gram List from Excel Files\n",
    "\n",
    "This notebook allows the user to enter a directory containing pre-processed Excel files after passing through an LLM in order to grab the distinct list of n-grams created when doing n-gram tracing. This will be used to filter out past results but also for future iterations this idea will be used prior to sending requests to an LLM.\n",
    "\n",
    "In the future i will process all of the documents in n-gram tracing and output a file similar to this containing all possible n-grams in common. Then i will create a filtered list to remove some before processing them next time around and keep iterating on this list with each corpus.\n",
    "\n",
    "Done to save time and money if using OpenAI as i feel the main 2-grams will be featured reguoarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4434322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import threading\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10def25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dir = '/Volumes/BCross/paraphrase examples slurm/Wiki-test-auto/'\n",
    "\n",
    "output_path = \"/Volumes/BCross/paraphrase examples slurm/wiki-phrase-list-raw.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe35576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_distinct_references(doc_loc, sheet_name='no context'):\n",
    "    \"\"\"\n",
    "    Reads a single Excel sheet, extracts distinct reference phrases, \n",
    "    parses the tokens list, and sorts the output by token count and phrase length.\n",
    "    \"\"\"\n",
    "\n",
    "    # Only read the required columns for speed and memory efficiency\n",
    "    use_cols = ['phrase_type', 'phrase', 'tokens']\n",
    "\n",
    "    try:\n",
    "        no_context = pd.read_excel(doc_loc, sheet_name=sheet_name, usecols=use_cols)\n",
    "    except Exception as e:\n",
    "        # Return empty DataFrame if the file or sheet can't be read\n",
    "        print(f\"âš ï¸ Failed to read {doc_loc}: {e}\")\n",
    "        return pd.DataFrame(columns=['phrase', 'tokens', 'num_tokens'])\n",
    "\n",
    "    # Filter only the rows where phrase_type == 'reference'\n",
    "    references = no_context[no_context['phrase_type'] == 'reference']\n",
    "\n",
    "    # Remove duplicates and reset index\n",
    "    references = references[['phrase', 'tokens']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Safely parse stringified lists (e.g., \"['a','b']\") into Python lists\n",
    "    def parse_tokens(x):\n",
    "        try:\n",
    "            return ast.literal_eval(x) if isinstance(x, str) else x\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "    references['tokens'] = references['tokens'].map(parse_tokens)\n",
    "\n",
    "    # Count number of tokens in each list\n",
    "    references['num_tokens'] = references['tokens'].map(len)\n",
    "\n",
    "    # Also compute phrase length in characters (for secondary sorting)\n",
    "    references['phrase_len'] = references['phrase'].str.len()\n",
    "\n",
    "    # Sort by number of tokens first, then phrase length\n",
    "    sorted_refs = references.sort_values(by=['num_tokens', 'phrase_len']).reset_index(drop=True)\n",
    "\n",
    "    # Remove temporary sorting column before returning\n",
    "    return sorted_refs.drop(columns='phrase_len')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0662a3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_ngrams_dir(doc_dir, sheet_name='no context'):\n",
    "    \"\"\"\n",
    "    Loops through all Excel files in a directory, extracts distinct reference phrases \n",
    "    from each file in parallel, merges them, and returns a sorted, deduplicated DataFrame.\n",
    "    Progress is shown as percentages (10%, 20%, ..., 100%).\n",
    "    \"\"\"\n",
    "\n",
    "    excel_dir = Path(doc_dir)\n",
    "\n",
    "    # Collect all Excel files (skip temporary lock files)\n",
    "    excel_files = sorted([f for f in excel_dir.glob(\"*.xlsx\") if not f.name.startswith(\"~$\")])\n",
    "    total_files = len(excel_files)\n",
    "    print(f\"ğŸŸ¢ Processing {total_files} Excel files in parallel...\")\n",
    "\n",
    "    # Shared state for progress tracking\n",
    "    progress_state = {'count': 0, 'next_threshold': 10}\n",
    "    lock = threading.Lock()\n",
    "\n",
    "    def load_and_track_progress(excel_path):\n",
    "        \"\"\"Loads and processes a single Excel file, updating progress.\"\"\"\n",
    "        try:\n",
    "            return get_sorted_distinct_references(excel_path, sheet_name=sheet_name)\n",
    "        finally:\n",
    "            with lock:\n",
    "                progress_state['count'] += 1\n",
    "                pct = (progress_state['count'] / total_files) * 100\n",
    "\n",
    "                # Print at clean 10% intervals\n",
    "                if pct >= progress_state['next_threshold']:\n",
    "                    print(f\"Progress: {int(progress_state['next_threshold'])}%\")\n",
    "                    progress_state['next_threshold'] += 10\n",
    "\n",
    "    # Run file processing in parallel\n",
    "    phrases_dataframe_list = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(load_and_track_progress, f) for f in excel_files]\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if not result.empty:\n",
    "                phrases_dataframe_list.append(result)\n",
    "\n",
    "    print(\"âœ… All files processed.\")\n",
    "\n",
    "    # Combine all results into one DataFrame\n",
    "    combined = pd.concat(phrases_dataframe_list, ignore_index=True)\n",
    "\n",
    "    # Convert lists to tuples so they can be hashed (for drop_duplicates)\n",
    "    combined['tokens'] = combined['tokens'].map(lambda x: tuple(x) if isinstance(x, list) else x)\n",
    "\n",
    "    # Drop duplicates across all files\n",
    "    combined = combined[['phrase', 'tokens', 'num_tokens']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Compute phrase length for sorting\n",
    "    combined['phrase_len'] = combined['phrase'].str.len()\n",
    "\n",
    "    # Sort by number of tokens first, then phrase length\n",
    "    sorted_combined = combined.sort_values(by=['num_tokens', 'phrase_len']).reset_index(drop=True)\n",
    "\n",
    "    # Drop helper column before returning\n",
    "    return sorted_combined.drop(columns='phrase_len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6015f031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŸ¢ Processing 661 Excel files in parallel...\n",
      "Progress: 10%\n",
      "Progress: 20%\n",
      "Progress: 30%\n",
      "Progress: 40%\n",
      "Progress: 50%\n",
      "Progress: 60%\n",
      "Progress: 70%\n",
      "Progress: 80%\n",
      "Progress: 90%\n",
      "Progress: 100%\n",
      "âœ… All files processed.\n"
     ]
    }
   ],
   "source": [
    "reference_phrases = get_sorted_ngrams_dir(doc_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0c44ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>tokens</th>\n",
       "      <th>num_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7 is</td>\n",
       "      <td>(7, Ä is)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s of</td>\n",
       "      <td>(s, Ä of)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'s a</td>\n",
       "      <td>('s, Ä a)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am</td>\n",
       "      <td>(i, Ä am)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'t a</td>\n",
       "      <td>('t, Ä a)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3021</th>\n",
       "      <td>in the interest of not starting a flame war or...</td>\n",
       "      <td>(in, Ä the, Ä interest, Ä of, Ä not, Ä starting, Ä a...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3022</th>\n",
       "      <td>my advice to mr asquith is to stay away from w...</td>\n",
       "      <td>(my, Ä advice, Ä to, Ä mr, Ä as, qu, ith, Ä is, Ä to...</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3023</th>\n",
       "      <td>.\\ni was unaware of wikipedia, as i do not ref...</td>\n",
       "      <td>(.ÄŠ, i, Ä was, Ä unaware, Ä of, Ä wikipedia, ,, Ä a...</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3024</th>\n",
       "      <td>indeed, this is pure logic just think about th...</td>\n",
       "      <td>(inde, ed, ,, Ä this, Ä is, Ä pure, Ä logic, Ä just...</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3025</th>\n",
       "      <td>while this does not involve sequencing the ent...</td>\n",
       "      <td>(while, Ä this, Ä does, Ä not, Ä involve, Ä sequenc...</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3026 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 phrase  \\\n",
       "0                                                  7 is   \n",
       "1                                                  s of   \n",
       "2                                                  's a   \n",
       "3                                                  i am   \n",
       "4                                                  't a   \n",
       "...                                                 ...   \n",
       "3021  in the interest of not starting a flame war or...   \n",
       "3022  my advice to mr asquith is to stay away from w...   \n",
       "3023  .\\ni was unaware of wikipedia, as i do not ref...   \n",
       "3024  indeed, this is pure logic just think about th...   \n",
       "3025  while this does not involve sequencing the ent...   \n",
       "\n",
       "                                                 tokens  num_tokens  \n",
       "0                                              (7, Ä is)           2  \n",
       "1                                              (s, Ä of)           2  \n",
       "2                                              ('s, Ä a)           2  \n",
       "3                                              (i, Ä am)           2  \n",
       "4                                              ('t, Ä a)           2  \n",
       "...                                                 ...         ...  \n",
       "3021  (in, Ä the, Ä interest, Ä of, Ä not, Ä starting, Ä a...          72  \n",
       "3022  (my, Ä advice, Ä to, Ä mr, Ä as, qu, ith, Ä is, Ä to...          88  \n",
       "3023  (.ÄŠ, i, Ä was, Ä unaware, Ä of, Ä wikipedia, ,, Ä a...         154  \n",
       "3024  (inde, ed, ,, Ä this, Ä is, Ä pure, Ä logic, Ä just...         300  \n",
       "3025  (while, Ä this, Ä does, Ä not, Ä involve, Ä sequenc...         320  \n",
       "\n",
       "[3026 rows x 3 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ae289f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference_phrases.to_excel(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "652d9ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_phrase_or_tokens_in_dir(doc_dir, sheet_name='no context', target=None):\n",
    "    \"\"\"\n",
    "    Searches through all Excel files in a directory (in parallel) \n",
    "    to find which ones contain a specific phrase or a specific list/tuple of tokens.\n",
    "    Progress is shown as percentages (10%, 20%, ..., 100%).\n",
    "    \"\"\"\n",
    "\n",
    "    if target is None:\n",
    "        raise ValueError(\"You must provide a target phrase or tuple of tokens.\")\n",
    "\n",
    "    # Determine if this is a token or phrase search\n",
    "    is_token_search = isinstance(target, (list, tuple))\n",
    "    target_list = list(target) if is_token_search else str(target)\n",
    "\n",
    "    excel_dir = Path(doc_dir)\n",
    "    excel_files = sorted([f for f in excel_dir.glob(\"*.xlsx\") if not f.name.startswith(\"~$\")])\n",
    "    total_files = len(excel_files)\n",
    "\n",
    "    print(f\"ğŸ” Searching {total_files} files for {'tokens' if is_token_search else 'phrase'}: {target_list}\")\n",
    "\n",
    "    # Shared progress tracker\n",
    "    progress_state = {'count': 0, 'next_threshold': 10}\n",
    "    lock = threading.Lock()\n",
    "\n",
    "    def check_file(excel_path):\n",
    "        \"\"\"Worker function for one file.\"\"\"\n",
    "        try:\n",
    "            df = get_sorted_distinct_references(excel_path, sheet_name=sheet_name)\n",
    "            if df.empty:\n",
    "                return None\n",
    "\n",
    "            # Token-based or phrase-based matching\n",
    "            if is_token_search:\n",
    "                token_tuples = set(df['tokens'].map(tuple))\n",
    "                if tuple(target_list) in token_tuples:\n",
    "                    return excel_path.name\n",
    "            else:\n",
    "                phrases_set = set(df['phrase'])\n",
    "                if target_list in phrases_set:\n",
    "                    return excel_path.name\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Failed to process {excel_path.name}: {e}\")\n",
    "        finally:\n",
    "            with lock:\n",
    "                progress_state['count'] += 1\n",
    "                pct = (progress_state['count'] / total_files) * 100\n",
    "\n",
    "                # Print at clean 10% increments\n",
    "                if pct >= progress_state['next_threshold']:\n",
    "                    print(f\"Progress: {int(progress_state['next_threshold'])}%\")\n",
    "                    progress_state['next_threshold'] += 10\n",
    "\n",
    "        return None\n",
    "\n",
    "    matched_files = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(check_file, f) for f in excel_files]\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                matched_files.append(result)\n",
    "\n",
    "    print(\"âœ… Search complete.\")\n",
    "    return matched_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0b44f9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Searching 661 files for tokens: [',', 'Ä i', \"'d\"]\n",
      "Progress: 10%\n",
      "Progress: 20%\n",
      "Progress: 30%\n",
      "Progress: 40%\n",
      "Progress: 50%\n",
      "Progress: 60%\n",
      "Progress: 70%\n",
      "Progress: 80%\n",
      "Progress: 90%\n",
      "Progress: 100%\n",
      "âœ… Search complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['jasper_deng_text_2 vs jasper_deng_text_4.xlsx',\n",
       " 'pinkampersand_text_2 vs pinkampersand_text_4.xlsx',\n",
       " 'pinkampersand_text_3 vs pinkampersand_text_4.xlsx',\n",
       " 'pinkampersand_text_3 vs pro_lick_text_1.xlsx',\n",
       " 'pro_lick_text_3 vs pro_lick_text_1.xlsx',\n",
       " 'stephenbuxton_text_1 vs stillstanding_247_text_5.xlsx']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_phrase_doc_list = find_phrase_or_tokens_in_dir(\n",
    "    doc_dir,\n",
    "    sheet_name='no context',\n",
    "    target=(',', 'Ä i', \"'d\")\n",
    "\n",
    ")\n",
    "common_phrase_doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab1ed68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
