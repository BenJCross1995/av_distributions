{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0db8ae60-5a34-4c1d-b29b-83df93e6bf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "406ed005-dc23-4fca-b965-dd0da11ddf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "from read_and_write_docs import read_jsonl, write_jsonl, read_rds\n",
    "from utils import apply_temp_doc_id, build_metadata_df\n",
    "from lambdaG import extract_ngrams, lambdaG_paraphrase, lambdaG, lambdaG_v2\n",
    "from performance import performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82a94cfc-c850-4067-aee8-603f674b5325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(logprobs):\n",
    "    \"\"\"\n",
    "    Compute sentence-level perplexity from token log-probabilities.\n",
    "    Assumes log-probs are natural logs (base e), as provided by Qwenâ€‘2.5.\n",
    "    \"\"\"\n",
    "    return np.exp(-np.mean(logprobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b709205f-ba50-487b-af8d-4721a6400524",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_loc = \"/Volumes/BCross/datasets/author_verification\"\n",
    "\n",
    "data_type = \"training\"\n",
    "corpus = \"Wiki\"\n",
    "\n",
    "model = \"Qwen2.5-1.5B-Instruct\"\n",
    "model_name = model.lower().replace(\"-\", \"_\")\n",
    "token_type = model\n",
    "\n",
    "known_loc = f\"{base_loc}/sentence_log_probs_datasets/{data_type}/{corpus}/known_sentence_logprobs_{model_name}.jsonl\"\n",
    "known_loc = \"/Users/user/Documents/test_data/known_sentence_logprobs_qwen2.5_1.5b_instruct.jsonl\"\n",
    "known = read_jsonl(known_loc)\n",
    "known.rename(columns={'sentence': 'text'}, inplace=True)\n",
    "known = apply_temp_doc_id(known)\n",
    "known = known[known['num_tokens'] > 0]\n",
    "known['perplexity'] = known['log_probs'].apply(compute_perplexity)\n",
    "\n",
    "unknown_loc = f\"{base_loc}/sentence_log_probs_datasets/{data_type}/{corpus}/unknown_sentence_logprobs_{model_name}.jsonl\"\n",
    "unknown_loc = \"/Users/user/Documents/test_data/unknown_sentence_logprobs_qwen2.5_1.5b_instruct.jsonl\"\n",
    "unknown = read_jsonl(unknown_loc)\n",
    "unknown.rename(columns={'sentence': 'text'}, inplace=True)\n",
    "unknown = apply_temp_doc_id(unknown)\n",
    "unknown = unknown[unknown['num_tokens'] > 0]\n",
    "unknown['perplexity'] = unknown['log_probs'].apply(compute_perplexity)\n",
    "\n",
    "metadata_loc = f\"{base_loc}/{data_type}/metadata.rds\"\n",
    "metadata_loc = \"/Users/user/Documents/test_data/metadata.rds\"\n",
    "metadata = read_rds(metadata_loc)\n",
    "filtered_metadata = metadata[metadata['corpus'] == corpus]\n",
    "agg_metadata = build_metadata_df(filtered_metadata, known, unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5f99db-95e0-42ca-9227-bbd0689bab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "impostor_loc = \"/Users/user/Documents/test_data/top_impostors_tokenized\"\n",
    "test_impostors = read_jsonl(f\"{impostor_loc}/alanyst_text_13.jsonl\")\n",
    "\n",
    "test_impostors.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a00374-8794-4004-bc73-c08a015aeea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_filtered = known[known['doc_id'].isin(['athenean_text_1', 'alanyst_text_13'])]\n",
    "known_filtered = known[known['author'].isin(['Alanyst', 'Athenean'])]\n",
    "known_filtered.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf9fdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_metadata[agg_metadata['known_author'] == 'Alanyst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d83db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_filtered = known[known['author'].isin(['Alanyst', 'Athenean'])]\n",
    "known_tokens = known_filtered['tokens']\n",
    "\n",
    "unknown_filtered = unknown[unknown['author'].isin(['Alanyst'])]\n",
    "unknown_tokens = unknown_filtered['tokens']\n",
    "\n",
    "unknown_filtered_2 = unknown[unknown['author'].isin(['AlasdairGreen27'])]\n",
    "unknown_tokens_2 = unknown_filtered_2['tokens']\n",
    "\n",
    "refs_filtered = known[~known['author'].isin(['Alanyst', 'AlasdairGreen27'])]\n",
    "refs_tokens = refs_filtered['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc72a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "refs_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248f6200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.lm import KneserNeyInterpolated\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from collections import defaultdict\n",
    "\n",
    "def build_kn_model(sentences, N):\n",
    "    \"\"\"\n",
    "    Build an N-gram language model with Kneser-Ney smoothing.\n",
    "    sentences: list of token lists, e.g. [['This','is','.'], ['Another','one','.']]\n",
    "    \"\"\"\n",
    "    train_data, padded_vocab = padded_everygram_pipeline(N, sentences)\n",
    "    model = KneserNeyInterpolated(order=N)\n",
    "    model.fit(train_data, padded_vocab)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35ccef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_model = build_kn_model(known_tokens, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce406c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_log10_prob(model, sentence, N):\n",
    "    \"\"\"\n",
    "    Compute log10 probability of a single tokenized sentence under the model.\n",
    "    \"\"\"\n",
    "    # generate N-grams\n",
    "    grams = list(nltk.lm.preprocessing.padded_everygrams(N, sentence))\n",
    "    logp = 0.0\n",
    "    for gram in grams:\n",
    "        context, word = tuple(gram[:-1]), gram[-1]\n",
    "        # model.score returns P(word | context)\n",
    "        p = model.score(word, context)\n",
    "        # avoid log(0)\n",
    "        if p <= 0:\n",
    "            p = 1e-300\n",
    "        logp += math.log10(p)\n",
    "    return logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1506a7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_logp_1 = sum(sentence_log10_prob(k_model, s, 10) for s in unknown_tokens)\n",
    "k_logp_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76150030",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_logp_2 = sum(sentence_log10_prob(k_model, s, 10) for s in unknown_tokens_2)\n",
    "k_logp_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8af56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_sentence = known_filtered.iloc[1, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a54b653",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_log10_prob(test_model, second_sentence, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31e7c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(known_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd35dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sum_1 = 0.0\n",
    "for _ in range(30):\n",
    "    sampled = random.sample(sorted(refs_tokens), len(known_tokens))\n",
    "    ref_model = build_kn_model(sampled, 10)\n",
    "    ref_logp = sum(sentence_log10_prob(ref_model, s, 10) for s in unknown_tokens)\n",
    "    lr_sum_1 += (k_logp_1 - ref_logp)\n",
    "avg_lr_1 = lr_sum_1 / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2b33f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_lr_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4e62ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sum_2 = 0.0\n",
    "for _ in range(30):\n",
    "    sampled = random.sample(sorted(refs_tokens), len(known_tokens))\n",
    "    ref_model = build_kn_model(sampled, 10)\n",
    "    ref_logp = sum(sentence_log10_prob(ref_model, s, 10) for s in unknown_tokens_2)\n",
    "    lr_sum += (k_logp_2 - ref_logp)\n",
    "avg_lr_2 = lr_sum_2 / 30\n",
    "avg_lr_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c4d456",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = lambdaG(unknown, known_filtered, known, metadata=agg_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f07259",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_v2 = lambdaG_v2(unknown, known_filtered, known, metadata=agg_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09579b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cbe8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b09a95-7191-44c3-bfed-c9ee31a06ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = lambdaG_paraphrase(unknown, known_filtered,\n",
    "                             metadata=agg_metadata, impostor_loc=impostor_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9253f86c-2242-4d4f-a30f-e6f5c8d66035",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84528095-4e29-46b7-b9e7-20647c3c9929",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_agg = (\n",
    "    results\n",
    "    .groupby(['problem', 'target'], as_index=False)\n",
    "    ['score']\n",
    "    .mean()\n",
    ")\n",
    "score_col = 'score'\n",
    "target_col = 'target'\n",
    "performance(results_agg,\n",
    "            score_col,\n",
    "            target_col,\n",
    "            additional_metadata={\n",
    "                'corpus': corpus\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd7e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_agg = (\n",
    "    results_v2\n",
    "    .groupby(['problem', 'target'], as_index=False)\n",
    "    ['score']\n",
    "    .mean()\n",
    ")\n",
    "score_col = 'score'\n",
    "target_col = 'target'\n",
    "performance(results_agg,\n",
    "            score_col,\n",
    "            target_col,\n",
    "            additional_metadata={\n",
    "                'corpus': corpus\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb973e13-da1a-4093-9a48-4b65e6bcfa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "\n",
    "for rep in range(1, 6): \n",
    "    print(f\"Repetition {rep}\")\n",
    "    df = lambdaG_v2(unknown, known, known,\n",
    "                    metadata=agg_metadata)\n",
    "    # Add the repetition column at the start:\n",
    "    df.insert(0, 'repetition', rep)\n",
    "    df.insert(1, 'corpus', corpus)      # move corpus next\n",
    "    df.insert(2, 'data_type', data_type)\n",
    "    df.insert(2, 'token_type', token_type) \n",
    "    all_results.append(df)\n",
    "\n",
    "# Combine all repetitions into one DataFrame\n",
    "results = pd.concat(all_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23361d2-7213-4a30-b811-be82fe2e232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_agg = (\n",
    "#     results\n",
    "#     .groupby(['problem', 'target'], as_index=False)\n",
    "#     ['score']\n",
    "#     .mean()\n",
    "# )\n",
    "# score_col = 'score'\n",
    "# target_col = 'target'\n",
    "# performance(results_agg,\n",
    "#             score_col,\n",
    "#             target_col,\n",
    "#             additional_metadata={\n",
    "#                 'corpus': corpus\n",
    "#             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a7916d-70c0-4096-85b5-b5b3e9fc8c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_loc = f\"{base_loc}/lambda_g_results/{corpus}_{data_type}_{model_name}_raw.jsonl\"\n",
    "# write_jsonl(results, save_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a1add2-35b9-4745-8f78-8d5d924eb71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_loc = f\"{base_loc}/lambda_g_results/{corpus}_training_{model_name}_raw.jsonl\"\n",
    "# training = read_jsonl(training_loc)\n",
    "\n",
    "# test_loc = f\"{base_loc}/lambda_g_results/{corpus}_test_{model_name}_raw.jsonl\"\n",
    "# test = read_jsonl(test_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4087c02-06ca-44bd-aa94-593dfaa6d182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_results_agg = (\n",
    "#     training\n",
    "#     .groupby(['problem', 'target'], as_index=False)\n",
    "#     ['score']\n",
    "#     .mean()\n",
    "# )\n",
    "\n",
    "# test_results_agg = (\n",
    "#     test\n",
    "#     .groupby(['problem', 'target'], as_index=False)\n",
    "#     ['score']\n",
    "#     .mean()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90000ede-3536-4c63-a0d9-34d35d288d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_col = 'score'\n",
    "# target_col = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f52fa9-8f55-4947-aba8-5a81711603d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_metrics = performance(training_results_agg,\n",
    "#                              score_col,\n",
    "#                              target_col,\n",
    "#                              df_test=test_results_agg,\n",
    "#                              additional_metadata={\n",
    "#                                  'corpus': corpus\n",
    "#                              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b02b7f-524e-4d47-9a81-83861d8595a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "av_dists",
   "language": "python",
   "name": "my_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
