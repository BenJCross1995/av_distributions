{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0db8ae60-5a34-4c1d-b29b-83df93e6bf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "406ed005-dc23-4fca-b965-dd0da11ddf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "from read_and_write_docs import read_jsonl, write_jsonl, read_rds\n",
    "from utils import apply_temp_doc_id, build_metadata_df\n",
    "from lambdaG import extract_ngrams, lambdaG_paraphrase, lambdaG, lambdaG_v2\n",
    "from performance import performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82a94cfc-c850-4067-aee8-603f674b5325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(logprobs):\n",
    "    \"\"\"\n",
    "    Compute sentence-level perplexity from token log-probabilities.\n",
    "    Assumes log-probs are natural logs (base e), as provided by Qwenâ€‘2.5.\n",
    "    \"\"\"\n",
    "    return np.exp(-np.mean(logprobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b709205f-ba50-487b-af8d-4721a6400524",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_loc = \"/Volumes/BCross/datasets/author_verification\"\n",
    "\n",
    "data_type = \"training\"\n",
    "corpus = \"Wiki\"\n",
    "\n",
    "model = \"Qwen2.5-1.5B-Instruct\"\n",
    "model_name = model.lower().replace(\"-\", \"_\")\n",
    "token_type = model\n",
    "\n",
    "known_loc = f\"{base_loc}/sentence_log_probs_datasets/{data_type}/{corpus}/known_sentence_logprobs_{model_name}.jsonl\"\n",
    "known_loc = \"/Users/user/Documents/test_data/known_sentence_logprobs_qwen2.5_1.5b_instruct.jsonl\"\n",
    "known = read_jsonl(known_loc)\n",
    "known.rename(columns={'sentence': 'text'}, inplace=True)\n",
    "known = apply_temp_doc_id(known)\n",
    "known = known[known['num_tokens'] > 0]\n",
    "known['perplexity'] = known['log_probs'].apply(compute_perplexity)\n",
    "\n",
    "unknown_loc = f\"{base_loc}/sentence_log_probs_datasets/{data_type}/{corpus}/unknown_sentence_logprobs_{model_name}.jsonl\"\n",
    "unknown_loc = \"/Users/user/Documents/test_data/unknown_sentence_logprobs_qwen2.5_1.5b_instruct.jsonl\"\n",
    "unknown = read_jsonl(unknown_loc)\n",
    "unknown.rename(columns={'sentence': 'text'}, inplace=True)\n",
    "unknown = apply_temp_doc_id(unknown)\n",
    "unknown = unknown[unknown['num_tokens'] > 0]\n",
    "unknown['perplexity'] = unknown['log_probs'].apply(compute_perplexity)\n",
    "\n",
    "metadata_loc = f\"{base_loc}/{data_type}/metadata.rds\"\n",
    "metadata_loc = \"/Users/user/Documents/test_data/metadata.rds\"\n",
    "metadata = read_rds(metadata_loc)\n",
    "filtered_metadata = metadata[metadata['corpus'] == corpus]\n",
    "agg_metadata = build_metadata_df(filtered_metadata, known, unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5f99db-95e0-42ca-9227-bbd0689bab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "impostor_loc = \"/Users/user/Documents/test_data/top_impostors_tokenized\"\n",
    "test_impostors = read_jsonl(f\"{impostor_loc}/alanyst_text_13.jsonl\")\n",
    "\n",
    "test_impostors.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a00374-8794-4004-bc73-c08a015aeea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_filtered = known[known['doc_id'].isin(['athenean_text_1', 'alanyst_text_13'])]\n",
    "known_filtered = known[known['author'].isin(['Alanyst', 'Athenean'])]\n",
    "known_filtered.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf9fdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_metadata[agg_metadata['known_author'] == 'Alanyst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3d83db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_filtered = known[known['author'].isin(['Alanyst', 'Athenean'])]\n",
    "known_tokens = known_filtered['tokens']\n",
    "\n",
    "unknown_filtered = unknown[unknown['author'].isin(['Alanyst'])]\n",
    "unknown_tokens = unknown_filtered['tokens']\n",
    "\n",
    "unknown_filtered_2 = unknown[unknown['author'].isin(['AlasdairGreen27'])]\n",
    "unknown_tokens_2 = unknown_filtered_2['tokens']\n",
    "\n",
    "refs_filtered = known[~known['author'].isin(['Alanyst', 'AlasdairGreen27'])]\n",
    "refs_tokens = refs_filtered['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc72a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "refs_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248f6200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.lm import KneserNeyInterpolated\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from collections import defaultdict\n",
    "\n",
    "def build_kn_model(sentences, N):\n",
    "    \"\"\"\n",
    "    Build an N-gram language model with Kneser-Ney smoothing.\n",
    "    sentences: list of token lists, e.g. [['This','is','.'], ['Another','one','.']]\n",
    "    \"\"\"\n",
    "    train_data, padded_vocab = padded_everygram_pipeline(N, sentences)\n",
    "    model = KneserNeyInterpolated(order=N)\n",
    "    model.fit(train_data, padded_vocab)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35ccef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_model = build_kn_model(known_tokens, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce406c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_log10_prob(model, sentence, N):\n",
    "    \"\"\"\n",
    "    Compute log10 probability of a single tokenized sentence under the model.\n",
    "    \"\"\"\n",
    "    # generate N-grams\n",
    "    grams = list(nltk.lm.preprocessing.padded_everygrams(N, sentence))\n",
    "    logp = 0.0\n",
    "    for gram in grams:\n",
    "        context, word = tuple(gram[:-1]), gram[-1]\n",
    "        # model.score returns P(word | context)\n",
    "        p = model.score(word, context)\n",
    "        # avoid log(0)\n",
    "        if p <= 0:\n",
    "            p = 1e-300\n",
    "        logp += math.log10(p)\n",
    "    return logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1506a7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_logp_1 = sum(sentence_log10_prob(k_model, s, 10) for s in unknown_tokens)\n",
    "k_logp_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76150030",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_logp_2 = sum(sentence_log10_prob(k_model, s, 10) for s in unknown_tokens_2)\n",
    "k_logp_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8af56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_sentence = known_filtered.iloc[1, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a54b653",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_log10_prob(test_model, second_sentence, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31e7c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(known_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd35dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sum_1 = 0.0\n",
    "for _ in range(30):\n",
    "    sampled = random.sample(sorted(refs_tokens), len(known_tokens))\n",
    "    ref_model = build_kn_model(sampled, 10)\n",
    "    ref_logp = sum(sentence_log10_prob(ref_model, s, 10) for s in unknown_tokens)\n",
    "    lr_sum_1 += (k_logp_1 - ref_logp)\n",
    "avg_lr_1 = lr_sum_1 / 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2b33f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_lr_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4e62ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sum_2 = 0.0\n",
    "for _ in range(30):\n",
    "    sampled = random.sample(sorted(refs_tokens), len(known_tokens))\n",
    "    ref_model = build_kn_model(sampled, 10)\n",
    "    ref_logp = sum(sentence_log10_prob(ref_model, s, 10) for s in unknown_tokens_2)\n",
    "    lr_sum += (k_logp_2 - ref_logp)\n",
    "avg_lr_2 = lr_sum_2 / 30\n",
    "avg_lr_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02c4d456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    There are 2 known author(s) and 4 problem(s) in the dataset.\n",
      "        Working on problem 1 of 4: Alanyst vs Alanyst\n",
      "        Working on problem 2 of 4: Alanyst vs AlasdairGreen27\n",
      "        Working on problem 3 of 4: Athenean vs Athenean\n",
      "        Working on problem 4 of 4: Athenean vs Avraham\n"
     ]
    }
   ],
   "source": [
    "results = lambdaG(unknown, known_filtered, known, metadata=agg_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7f07259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    There are 2 known author(s) and 4 problem(s) in the dataset.\n",
      "        Working on problem 1 of 4: Alanyst vs Alanyst\n",
      "        Working on problem 2 of 4: Alanyst vs AlasdairGreen27\n",
      "        Working on problem 3 of 4: Athenean vs Athenean\n",
      "        Working on problem 4 of 4: Athenean vs Avraham\n"
     ]
    }
   ],
   "source": [
    "results_v2 = lambdaG_v2(unknown, known_filtered, known, metadata=agg_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09579b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>known_author</th>\n",
       "      <th>unknown_author</th>\n",
       "      <th>target</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alanyst vs Alanyst</td>\n",
       "      <td>Alanyst</td>\n",
       "      <td>Alanyst</td>\n",
       "      <td>True</td>\n",
       "      <td>8365.254806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alanyst vs AlasdairGreen27</td>\n",
       "      <td>Alanyst</td>\n",
       "      <td>AlasdairGreen27</td>\n",
       "      <td>False</td>\n",
       "      <td>3450.470084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Athenean vs Athenean</td>\n",
       "      <td>Athenean</td>\n",
       "      <td>Athenean</td>\n",
       "      <td>True</td>\n",
       "      <td>-3658.483830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Athenean vs Avraham</td>\n",
       "      <td>Athenean</td>\n",
       "      <td>Avraham</td>\n",
       "      <td>False</td>\n",
       "      <td>-8486.122589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      problem known_author   unknown_author  target  \\\n",
       "0          Alanyst vs Alanyst      Alanyst          Alanyst    True   \n",
       "1  Alanyst vs AlasdairGreen27      Alanyst  AlasdairGreen27   False   \n",
       "2        Athenean vs Athenean     Athenean         Athenean    True   \n",
       "3         Athenean vs Avraham     Athenean          Avraham   False   \n",
       "\n",
       "         score  \n",
       "0  8365.254806  \n",
       "1  3450.470084  \n",
       "2 -3658.483830  \n",
       "3 -8486.122589  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7cbe8d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>known_author</th>\n",
       "      <th>unknown_author</th>\n",
       "      <th>target</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alanyst vs Alanyst</td>\n",
       "      <td>Alanyst</td>\n",
       "      <td>Alanyst</td>\n",
       "      <td>True</td>\n",
       "      <td>81683.804182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alanyst vs AlasdairGreen27</td>\n",
       "      <td>Alanyst</td>\n",
       "      <td>AlasdairGreen27</td>\n",
       "      <td>False</td>\n",
       "      <td>23076.959751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Athenean vs Athenean</td>\n",
       "      <td>Athenean</td>\n",
       "      <td>Athenean</td>\n",
       "      <td>True</td>\n",
       "      <td>-45137.383344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Athenean vs Avraham</td>\n",
       "      <td>Athenean</td>\n",
       "      <td>Avraham</td>\n",
       "      <td>False</td>\n",
       "      <td>-79036.280003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      problem known_author   unknown_author  target  \\\n",
       "0          Alanyst vs Alanyst      Alanyst          Alanyst    True   \n",
       "1  Alanyst vs AlasdairGreen27      Alanyst  AlasdairGreen27   False   \n",
       "2        Athenean vs Athenean     Athenean         Athenean    True   \n",
       "3         Athenean vs Avraham     Athenean          Avraham   False   \n",
       "\n",
       "          score  \n",
       "0  81683.804182  \n",
       "1  23076.959751  \n",
       "2 -45137.383344  \n",
       "3 -79036.280003  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b09a95-7191-44c3-bfed-c9ee31a06ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = lambdaG_paraphrase(unknown, known_filtered,\n",
    "                             metadata=agg_metadata, impostor_loc=impostor_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9253f86c-2242-4d4f-a30f-e6f5c8d66035",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84528095-4e29-46b7-b9e7-20647c3c9929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Documents/GitHub/av_distributions/src/performance.py:146: RuntimeWarning: divide by zero encountered in divide\n",
      "  pred_llrs = np.log10(pred_probs / (1 - pred_probs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>Cllr</th>\n",
       "      <th>Cllr_min</th>\n",
       "      <th>EER</th>\n",
       "      <th>Mean_TRUE_LLR</th>\n",
       "      <th>Mean_FALSE_LLR</th>\n",
       "      <th>TRUE_trials</th>\n",
       "      <th>FALSE_trials</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Balanced_Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wiki</td>\n",
       "      <td>32.731863</td>\n",
       "      <td>32.731863</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-11.831542</td>\n",
       "      <td>inf</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  corpus       Cllr   Cllr_min  EER  Mean_TRUE_LLR  Mean_FALSE_LLR  \\\n",
       "0   Wiki  32.731863  32.731863  1.0     -11.831542             inf   \n",
       "\n",
       "   TRUE_trials  FALSE_trials  AUC  Balanced_Accuracy  Precision  Recall   F1  \\\n",
       "0            2             2  0.0                0.0        0.0     0.0  0.0   \n",
       "\n",
       "   TP  FP  FN  TN  \n",
       "0   0   2   2   0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_agg = (\n",
    "    results\n",
    "    .groupby(['problem', 'target'], as_index=False)\n",
    "    ['score']\n",
    "    .mean()\n",
    ")\n",
    "score_col = 'score'\n",
    "target_col = 'target'\n",
    "performance(results_agg,\n",
    "            score_col,\n",
    "            target_col,\n",
    "            additional_metadata={\n",
    "                'corpus': corpus\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89bd7e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/Documents/GitHub/av_distributions/src/performance.py:146: RuntimeWarning: divide by zero encountered in divide\n",
      "  pred_llrs = np.log10(pred_probs / (1 - pred_probs))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>Cllr</th>\n",
       "      <th>Cllr_min</th>\n",
       "      <th>EER</th>\n",
       "      <th>Mean_TRUE_LLR</th>\n",
       "      <th>Mean_FALSE_LLR</th>\n",
       "      <th>TRUE_trials</th>\n",
       "      <th>FALSE_trials</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Balanced_Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wiki</td>\n",
       "      <td>34.38611</td>\n",
       "      <td>34.371296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-12.906524</td>\n",
       "      <td>inf</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  corpus      Cllr   Cllr_min  EER  Mean_TRUE_LLR  Mean_FALSE_LLR  \\\n",
       "0   Wiki  34.38611  34.371296  1.0     -12.906524             inf   \n",
       "\n",
       "   TRUE_trials  FALSE_trials  AUC  Balanced_Accuracy  Precision  Recall   F1  \\\n",
       "0            2             2  0.0                0.0        0.0     0.0  0.0   \n",
       "\n",
       "   TP  FP  FN  TN  \n",
       "0   0   2   2   0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_agg = (\n",
    "    results_v2\n",
    "    .groupby(['problem', 'target'], as_index=False)\n",
    "    ['score']\n",
    "    .mean()\n",
    ")\n",
    "score_col = 'score'\n",
    "target_col = 'target'\n",
    "performance(results_agg,\n",
    "            score_col,\n",
    "            target_col,\n",
    "            additional_metadata={\n",
    "                'corpus': corpus\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb973e13-da1a-4093-9a48-4b65e6bcfa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition 1\n",
      "    There are 75 known author(s) and 150 problem(s) in the dataset.\n",
      "        Working on problem 1 of 150: 142.196.88.228 vs 142.196.88.228\n",
      "        Working on problem 2 of 150: 142.196.88.228 vs Aban1313\n",
      "        Working on problem 3 of 150: A_Man_In_Black vs A_Man_In_Black\n",
      "        Working on problem 4 of 150: A_Man_In_Black vs Bankhallbretherton\n",
      "        Working on problem 5 of 150: Aban1313 vs Aban1313\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m rep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m6\u001b[39m): \n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRepetition \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrep\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     df = \u001b[43mlambdaG_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43munknown\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mknown\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43magg_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Add the repetition column at the start:\u001b[39;00m\n\u001b[32m      8\u001b[39m     df.insert(\u001b[32m0\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrepetition\u001b[39m\u001b[33m'\u001b[39m, rep)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/av_distributions/src/lambdaG.py:348\u001b[39m, in \u001b[36mlambdaG_v2\u001b[39m\u001b[34m(unknown, known, refs, metadata, N, r, cores, vectorise)\u001b[39m\n\u001b[32m    345\u001b[39m ref_sentences = random.sample(all_refs, num_known_sentences)\n\u001b[32m    347\u001b[39m ref_model = build_kn_model(ref_sentences, N)\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m ref_logps = \u001b[43m[\u001b[49m\u001b[43msentence_log10_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43munknown_sentences\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# Sum per-sentence log-LR\u001b[39;00m\n\u001b[32m    351\u001b[39m lr = \u001b[38;5;28msum\u001b[39m(k - r_ \u001b[38;5;28;01mfor\u001b[39;00m k, r_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(known_logps, ref_logps))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/av_distributions/src/lambdaG.py:348\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    345\u001b[39m ref_sentences = random.sample(all_refs, num_known_sentences)\n\u001b[32m    347\u001b[39m ref_model = build_kn_model(ref_sentences, N)\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m ref_logps = [\u001b[43msentence_log10_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m unknown_sentences]\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# Sum per-sentence log-LR\u001b[39;00m\n\u001b[32m    351\u001b[39m lr = \u001b[38;5;28msum\u001b[39m(k - r_ \u001b[38;5;28;01mfor\u001b[39;00m k, r_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(known_logps, ref_logps))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/av_distributions/src/lambdaG.py:106\u001b[39m, in \u001b[36msentence_log10_prob\u001b[39m\u001b[34m(model, tokens, N)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ngram \u001b[38;5;129;01min\u001b[39;00m padded_everygrams(N, tokens):\n\u001b[32m    105\u001b[39m     context, word = \u001b[38;5;28mtuple\u001b[39m(ngram[:-\u001b[32m1\u001b[39m]), ngram[-\u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     p = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     \u001b[38;5;66;03m# Replace zeros with the smallest positive float\u001b[39;00m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m p <= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/nltk/lm/api.py:124\u001b[39m, in \u001b[36mLanguageModel.score\u001b[39m\u001b[34m(self, word, context)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mscore\u001b[39m(\u001b[38;5;28mself\u001b[39m, word, context=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    119\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Masks out of vocab (OOV) words and computes their model score.\u001b[39;00m\n\u001b[32m    120\u001b[39m \n\u001b[32m    121\u001b[39m \u001b[33;03m    For model-specific logic of calculating scores, see the `unmasked_score`\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[33;03m    method.\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43munmasked_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/nltk/lm/models.py:111\u001b[39m, in \u001b[36mInterpolatedLanguageModel.unmasked_score\u001b[39m\u001b[34m(self, word, context)\u001b[39m\n\u001b[32m    109\u001b[39m     alpha, gamma = \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     alpha, gamma = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43malpha_gamma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m alpha + gamma * \u001b[38;5;28mself\u001b[39m.unmasked_score(word, context[\u001b[32m1\u001b[39m:])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/nltk/lm/smoothing.py:105\u001b[39m, in \u001b[36mKneserNey.alpha_gamma\u001b[39m\u001b[34m(self, word, context)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34malpha_gamma\u001b[39m(\u001b[38;5;28mself\u001b[39m, word, context):\n\u001b[32m    101\u001b[39m     prefix_counts = \u001b[38;5;28mself\u001b[39m.counts[context]\n\u001b[32m    102\u001b[39m     word_continuation_count, total_count = (\n\u001b[32m    103\u001b[39m         (prefix_counts[word], prefix_counts.N())\n\u001b[32m    104\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(context) + \u001b[32m1\u001b[39m == \u001b[38;5;28mself\u001b[39m._order\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_continuation_counts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m     )\n\u001b[32m    107\u001b[39m     alpha = \u001b[38;5;28mmax\u001b[39m(word_continuation_count - \u001b[38;5;28mself\u001b[39m.discount, \u001b[32m0.0\u001b[39m) / total_count\n\u001b[32m    108\u001b[39m     gamma = \u001b[38;5;28mself\u001b[39m.discount * _count_values_gt_zero(prefix_counts) / total_count\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/nltk/lm/smoothing.py:124\u001b[39m, in \u001b[36mKneserNey._continuation_counts\u001b[39m\u001b[34m(self, word, context)\u001b[39m\n\u001b[32m    118\u001b[39m higher_order_ngrams_with_context = (\n\u001b[32m    119\u001b[39m     counts\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m prefix_ngram, counts \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.counts[\u001b[38;5;28mlen\u001b[39m(context) + \u001b[32m2\u001b[39m].items()\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m prefix_ngram[\u001b[32m1\u001b[39m:] == context\n\u001b[32m    122\u001b[39m )\n\u001b[32m    123\u001b[39m higher_order_ngrams_with_word_count, total = \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhigher_order_ngrams_with_context\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhigher_order_ngrams_with_word_count\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_count_values_gt_zero\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/av_distributions/my_venv/lib/python3.11/site-packages/nltk/lm/smoothing.py:121\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_continuation_counts\u001b[39m(\u001b[38;5;28mself\u001b[39m, word, context=\u001b[38;5;28mtuple\u001b[39m()):\n\u001b[32m    112\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Count continuations that end with context and word.\u001b[39;00m\n\u001b[32m    113\u001b[39m \n\u001b[32m    114\u001b[39m \u001b[33;03m    Continuations track unique ngram \"types\", regardless of how many\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[33;03m    instances were observed for each \"type\".\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[33;03m    This is different than raw ngram counts which track number of instances.\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    118\u001b[39m     higher_order_ngrams_with_context = (\n\u001b[32m    119\u001b[39m         counts\n\u001b[32m    120\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m prefix_ngram, counts \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.counts[\u001b[38;5;28mlen\u001b[39m(context) + \u001b[32m2\u001b[39m].items()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m prefix_ngram[\u001b[32m1\u001b[39m:] == context\n\u001b[32m    122\u001b[39m     )\n\u001b[32m    123\u001b[39m     higher_order_ngrams_with_word_count, total = \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m counts \u001b[38;5;129;01min\u001b[39;00m higher_order_ngrams_with_context:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "\n",
    "for rep in range(1, 6): \n",
    "    print(f\"Repetition {rep}\")\n",
    "    df = lambdaG_v2(unknown, known, known,\n",
    "                    metadata=agg_metadata)\n",
    "    # Add the repetition column at the start:\n",
    "    df.insert(0, 'repetition', rep)\n",
    "    df.insert(1, 'corpus', corpus)      # move corpus next\n",
    "    df.insert(2, 'data_type', data_type)\n",
    "    df.insert(2, 'token_type', token_type) \n",
    "    all_results.append(df)\n",
    "\n",
    "# Combine all repetitions into one DataFrame\n",
    "results = pd.concat(all_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23361d2-7213-4a30-b811-be82fe2e232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_agg = (\n",
    "#     results\n",
    "#     .groupby(['problem', 'target'], as_index=False)\n",
    "#     ['score']\n",
    "#     .mean()\n",
    "# )\n",
    "# score_col = 'score'\n",
    "# target_col = 'target'\n",
    "# performance(results_agg,\n",
    "#             score_col,\n",
    "#             target_col,\n",
    "#             additional_metadata={\n",
    "#                 'corpus': corpus\n",
    "#             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a7916d-70c0-4096-85b5-b5b3e9fc8c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_loc = f\"{base_loc}/lambda_g_results/{corpus}_{data_type}_{model_name}_raw.jsonl\"\n",
    "# write_jsonl(results, save_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a1add2-35b9-4745-8f78-8d5d924eb71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_loc = f\"{base_loc}/lambda_g_results/{corpus}_training_{model_name}_raw.jsonl\"\n",
    "# training = read_jsonl(training_loc)\n",
    "\n",
    "# test_loc = f\"{base_loc}/lambda_g_results/{corpus}_test_{model_name}_raw.jsonl\"\n",
    "# test = read_jsonl(test_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4087c02-06ca-44bd-aa94-593dfaa6d182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_results_agg = (\n",
    "#     training\n",
    "#     .groupby(['problem', 'target'], as_index=False)\n",
    "#     ['score']\n",
    "#     .mean()\n",
    "# )\n",
    "\n",
    "# test_results_agg = (\n",
    "#     test\n",
    "#     .groupby(['problem', 'target'], as_index=False)\n",
    "#     ['score']\n",
    "#     .mean()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90000ede-3536-4c63-a0d9-34d35d288d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_col = 'score'\n",
    "# target_col = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f52fa9-8f55-4947-aba8-5a81711603d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_metrics = performance(training_results_agg,\n",
    "#                              score_col,\n",
    "#                              target_col,\n",
    "#                              df_test=test_results_agg,\n",
    "#                              additional_metadata={\n",
    "#                                  'corpus': corpus\n",
    "#                              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b02b7f-524e-4d47-9a81-83861d8595a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "av_dists",
   "language": "python",
   "name": "my_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
