{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de546f7b-d43d-4cbe-b916-6c134f0389c2",
   "metadata": {},
   "source": [
    "# Vectorize Full Document Paraphrases\n",
    "\n",
    "This notebook will be used to test vectorizing the full document paraphrases into sentences. It uses the same framework done on the original data which is to perform named entitiy recognition, then vectorize and then add the named entities back in to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da85b073-2c7c-4df4-aab9-5c09ce77f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48966aa3-b98d-4527-a801-f388c62cb6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "from read_and_write_docs import read_jsonl, write_jsonl\n",
    "from preprocessing import vectorize_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddea35d7-2c74-4281-85c1-2f31bdc883f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get spacy model, run in terminal\n",
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e06610e-02ae-4fe6-ade7-b42ebc286107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get nltk to work\n",
    "# import nltk\n",
    "# nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f30339-b5b7-49d2-b321-debb50c991f9",
   "metadata": {},
   "source": [
    "## Set Locations\n",
    "\n",
    "Set the locations of the top impostors after parascoring and finding the top n impostors according to the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e9d2513-2156-46b0-9bb3-af8974a9e330",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_loc = \"//bc_nas_storage/BCross/datasets/author_verification\"\n",
    "\n",
    "data_type = \"training\"\n",
    "\n",
    "corpus = 'Wiki'\n",
    "\n",
    "data_loc = f\"{base_loc}/{data_type}/{corpus}/Qwen_2.5_1.5B/top_impostors\"\n",
    "data_loc_path = Path(data_loc)\n",
    "\n",
    "save_loc = f\"{base_loc}/{data_type}/{corpus}/Qwen_2.5_1.5B/top_impostors_tokenized\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3f7103-8e7e-4ef8-a8c1-a6ed52c2d765",
   "metadata": {},
   "source": [
    "## Load the Data and Preprocess\n",
    "\n",
    "There are several steps to load the data and preprocess it so it fits the vectorizing code as it currently stands. Basic changes to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a782e1d-d8af-478b-897b-58f5a4609dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_jsonl(f\"{data_loc}/142_196_88_228_text_1.jsonl\")\n",
    "df['impostor_id'] = df.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1477ed8-4f80-4843-8989-d5eb5a83d889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>orig_doc_id</th>\n",
       "      <th>corpus</th>\n",
       "      <th>author</th>\n",
       "      <th>texttype</th>\n",
       "      <th>original</th>\n",
       "      <th>generated_text</th>\n",
       "      <th>time_sec</th>\n",
       "      <th>tokens_per_sec</th>\n",
       "      <th>rephrased</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>clean_stage</th>\n",
       "      <th>parsing_errors</th>\n",
       "      <th>parascore_free</th>\n",
       "      <th>impostor_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>142_196_88_228_text_1</td>\n",
       "      <td>known [142.196.88.228 - Text-1].txt</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>142.196.88.228</td>\n",
       "      <td>known</td>\n",
       "      <td>The article that is being referred to via the ...</td>\n",
       "      <td>The article mentioned through the broken link ...</td>\n",
       "      <td>11.709744</td>\n",
       "      <td>107.944289</td>\n",
       "      <td>The article mentioned through the broken link ...</td>\n",
       "      <td>1</td>\n",
       "      <td>wrap_plain_text</td>\n",
       "      <td>[original: Expecting value: line 1 column 1 (c...</td>\n",
       "      <td>0.870214</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>142_196_88_228_text_1</td>\n",
       "      <td>known [142.196.88.228 - Text-1].txt</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>142.196.88.228</td>\n",
       "      <td>known</td>\n",
       "      <td>The article that is being referred to via the ...</td>\n",
       "      <td>The article referenced through the broken link...</td>\n",
       "      <td>10.373190</td>\n",
       "      <td>116.646860</td>\n",
       "      <td>The article referenced through the broken link...</td>\n",
       "      <td>1</td>\n",
       "      <td>wrap_plain_text</td>\n",
       "      <td>[original: Expecting value: line 1 column 1 (c...</td>\n",
       "      <td>0.862308</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  doc_id                          orig_doc_id corpus  \\\n",
       "0  142_196_88_228_text_1  known [142.196.88.228 - Text-1].txt   Wiki   \n",
       "1  142_196_88_228_text_1  known [142.196.88.228 - Text-1].txt   Wiki   \n",
       "\n",
       "           author texttype                                           original  \\\n",
       "0  142.196.88.228    known  The article that is being referred to via the ...   \n",
       "1  142.196.88.228    known  The article that is being referred to via the ...   \n",
       "\n",
       "                                      generated_text   time_sec  \\\n",
       "0  The article mentioned through the broken link ...  11.709744   \n",
       "1  The article referenced through the broken link...  10.373190   \n",
       "\n",
       "   tokens_per_sec                                          rephrased  \\\n",
       "0      107.944289  The article mentioned through the broken link ...   \n",
       "1      116.646860  The article referenced through the broken link...   \n",
       "\n",
       "   text_cleaned      clean_stage  \\\n",
       "0             1  wrap_plain_text   \n",
       "1             1  wrap_plain_text   \n",
       "\n",
       "                                      parsing_errors  parascore_free  \\\n",
       "0  [original: Expecting value: line 1 column 1 (c...        0.870214   \n",
       "1  [original: Expecting value: line 1 column 1 (c...        0.862308   \n",
       "\n",
       "   impostor_id  \n",
       "0            1  \n",
       "1            2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "642acfca-6b6e-47ff-bf33-d0e2da545957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>corpus</th>\n",
       "      <th>impostor_id</th>\n",
       "      <th>author</th>\n",
       "      <th>texttype</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>142_196_88_228_text_1</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>1</td>\n",
       "      <td>142.196.88.228</td>\n",
       "      <td>known</td>\n",
       "      <td>The article mentioned through the broken link ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>142_196_88_228_text_1</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>2</td>\n",
       "      <td>142.196.88.228</td>\n",
       "      <td>known</td>\n",
       "      <td>The article referenced through the broken link...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>142_196_88_228_text_1</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>3</td>\n",
       "      <td>142.196.88.228</td>\n",
       "      <td>known</td>\n",
       "      <td>The referenced article concludes that winemaki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>142_196_88_228_text_1</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>4</td>\n",
       "      <td>142.196.88.228</td>\n",
       "      <td>known</td>\n",
       "      <td>An article linked through a broken hyperlink s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142_196_88_228_text_1</td>\n",
       "      <td>Wiki</td>\n",
       "      <td>5</td>\n",
       "      <td>142.196.88.228</td>\n",
       "      <td>known</td>\n",
       "      <td>An article linked through a broken connection ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  doc_id corpus  impostor_id          author texttype  \\\n",
       "0  142_196_88_228_text_1   Wiki            1  142.196.88.228    known   \n",
       "1  142_196_88_228_text_1   Wiki            2  142.196.88.228    known   \n",
       "2  142_196_88_228_text_1   Wiki            3  142.196.88.228    known   \n",
       "3  142_196_88_228_text_1   Wiki            4  142.196.88.228    known   \n",
       "4  142_196_88_228_text_1   Wiki            5  142.196.88.228    known   \n",
       "\n",
       "                                                text  \n",
       "0  The article mentioned through the broken link ...  \n",
       "1  The article referenced through the broken link...  \n",
       "2  The referenced article concludes that winemaki...  \n",
       "3  An article linked through a broken hyperlink s...  \n",
       "4  An article linked through a broken connection ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['doc_id','corpus', 'impostor_id', 'author', 'texttype', 'rephrased']]\n",
    "df.rename(columns={'rephrased': 'text'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e93c2a8-8c64-4063-b58d-419da11ec3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_df = vectorize_df(df, impostors=True)\n",
    "vectorized_df.rename(columns={'sentence': 'text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab849532-415d-488e-b8cb-ecbc8878d8da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>impostor_id</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>author</th>\n",
       "      <th>texttype</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wiki</td>\n",
       "      <td>142_196_88_228_text_1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>142.196.88.228</td>\n",
       "      <td>known</td>\n",
       "      <td>The article mentioned through the broken link ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wiki</td>\n",
       "      <td>142_196_88_228_text_1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>142.196.88.228</td>\n",
       "      <td>known</td>\n",
       "      <td>However, this study offers little evidence sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wiki</td>\n",
       "      <td>142_196_88_228_text_1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>142.196.88.228</td>\n",
       "      <td>known</td>\n",
       "      <td>The Georgian theory lacks substantial proof si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wiki</td>\n",
       "      <td>142_196_88_228_text_1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>142.196.88.228</td>\n",
       "      <td>known</td>\n",
       "      <td>Scholars generally believe that due to the inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wiki</td>\n",
       "      <td>142_196_88_228_text_1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>142.196.88.228</td>\n",
       "      <td>known</td>\n",
       "      <td>Therefore, it should be categorized within Mes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5964</th>\n",
       "      <td>Wiki</td>\n",
       "      <td>142_196_88_228_text_1</td>\n",
       "      <td>500</td>\n",
       "      <td>9</td>\n",
       "      <td>142.196.88.228</td>\n",
       "      <td>known</td>\n",
       "      <td>As far as concrete proof, there is essentially...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5965</th>\n",
       "      <td>Wiki</td>\n",
       "      <td>142_196_88_228_text_1</td>\n",
       "      <td>500</td>\n",
       "      <td>10</td>\n",
       "      <td>142.196.88.228</td>\n",
       "      <td>known</td>\n",
       "      <td>Some argue that the diverse cultures prevent W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5966</th>\n",
       "      <td>Wiki</td>\n",
       "      <td>142_196_88_228_text_1</td>\n",
       "      <td>500</td>\n",
       "      <td>11</td>\n",
       "      <td>142.196.88.228</td>\n",
       "      <td>known</td>\n",
       "      <td>The prominence of Susa's culture is clearly es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5967</th>\n",
       "      <td>Wiki</td>\n",
       "      <td>142_196_88_228_text_1</td>\n",
       "      <td>500</td>\n",
       "      <td>12</td>\n",
       "      <td>142.196.88.228</td>\n",
       "      <td>known</td>\n",
       "      <td>Furthermore, your British Museum source contra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5968</th>\n",
       "      <td>Wiki</td>\n",
       "      <td>142_196_88_228_text_1</td>\n",
       "      <td>500</td>\n",
       "      <td>13</td>\n",
       "      <td>142.196.88.228</td>\n",
       "      <td>known</td>\n",
       "      <td>You can explore the 'Geography' section on the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5969 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     corpus                 doc_id  impostor_id  chunk_id          author  \\\n",
       "0      Wiki  142_196_88_228_text_1            1         1  142.196.88.228   \n",
       "1      Wiki  142_196_88_228_text_1            1         2  142.196.88.228   \n",
       "2      Wiki  142_196_88_228_text_1            1         3  142.196.88.228   \n",
       "3      Wiki  142_196_88_228_text_1            1         4  142.196.88.228   \n",
       "4      Wiki  142_196_88_228_text_1            1         5  142.196.88.228   \n",
       "...     ...                    ...          ...       ...             ...   \n",
       "5964   Wiki  142_196_88_228_text_1          500         9  142.196.88.228   \n",
       "5965   Wiki  142_196_88_228_text_1          500        10  142.196.88.228   \n",
       "5966   Wiki  142_196_88_228_text_1          500        11  142.196.88.228   \n",
       "5967   Wiki  142_196_88_228_text_1          500        12  142.196.88.228   \n",
       "5968   Wiki  142_196_88_228_text_1          500        13  142.196.88.228   \n",
       "\n",
       "     texttype                                               text  \n",
       "0       known  The article mentioned through the broken link ...  \n",
       "1       known  However, this study offers little evidence sup...  \n",
       "2       known  The Georgian theory lacks substantial proof si...  \n",
       "3       known  Scholars generally believe that due to the inf...  \n",
       "4       known  Therefore, it should be categorized within Mes...  \n",
       "...       ...                                                ...  \n",
       "5964    known  As far as concrete proof, there is essentially...  \n",
       "5965    known  Some argue that the diverse cultures prevent W...  \n",
       "5966    known  The prominence of Susa's culture is clearly es...  \n",
       "5967    known  Furthermore, your British Museum source contra...  \n",
       "5968    known  You can explore the 'Geography' section on the...  \n",
       "\n",
       "[5969 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0225b4-9159-41c8-af3b-1d266a209c8f",
   "metadata": {},
   "source": [
    "## Get LogProbs Code\n",
    "\n",
    "This needs moving to a .py file for ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "419a1a43-7771-46d1-86b4-ab9de127e330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benjc\\Documents\\GitHub\\av_distributions\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "567a5ad3-d7fc-4050-a8c0-3b52091e8799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model and tokenizer once\n",
    "model_name = \"Qwen2.5-1.5B-Instruct\"\n",
    "model_loc = f\"C:/Users/benjc/Documents/local models/{model_name}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_loc)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_loc)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ceb4644-c9fe-4aaa-b0ea-b97b47da14a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_probs_with_median(text: str):\n",
    "    \"\"\"\n",
    "    For each token (excluding first), return:\n",
    "    - tokens: list of tokens in the text\n",
    "    - log_probs: list of chosen-token log-probs\n",
    "    - median_logprobs: list of median log-probs for each token\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    tokens = tokenizer.decode(input_ids[0]).split()  # Convert input_ids to tokens\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "    logits = outputs.logits  # [batch_size=1, seq_len, vocab_size]\n",
    "    \n",
    "    log_probs = []\n",
    "    median_logprobs = []\n",
    "    # We start from the second token, as the first one has no previous token to condition on\n",
    "    for i in range(0, input_ids.size(1)):\n",
    "        if i == 0:\n",
    "            logits_prev = logits[0, 0]\n",
    "        else:\n",
    "            logits_prev = logits[0, i - 1]\n",
    "        dist = torch.log_softmax(logits_prev, dim=-1)\n",
    "        \n",
    "        # Extract the log probabilities\n",
    "        log_prob = dist[input_ids[0, i].item()].item()\n",
    "        median_logprob = float(dist.median().item())\n",
    "        \n",
    "        # Append to lists\n",
    "        log_probs.append(log_prob)\n",
    "        median_logprobs.append(median_logprob)\n",
    "    \n",
    "    # The tokens list starts from the first token, but the log_probs and median_logprobs start from the second\n",
    "    # To align them, we need to slice the tokens list to match the lengths\n",
    "    tokens = tokens[0:]  # Match the length of log_probs and median_logprobs\n",
    "    \n",
    "    return tokens, log_probs, median_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b23cd8bb-694d-48da-a486-2edbf783f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataframe(df: pd.DataFrame, text_column: str = \"text\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes a dataframe with a column of texts and computes:\n",
    "    - list of log-probs per token\n",
    "    - median log-probs per token\n",
    "    - number of tokens\n",
    "    - sum of log probs\n",
    "    - average log-prob\n",
    "    - differences between log_probs and median log-probs\n",
    "    - absolute differences between log_probs and median log-probs\n",
    "    - mean of differences\n",
    "    - mean of absolute differences\n",
    "    \"\"\"\n",
    "    tqdm.pandas(desc=\"Scoring texts\")\n",
    "    df = df.copy()\n",
    "\n",
    "    # Step 1: Extract tokens, log_probs, and median log_probs\n",
    "    df[['tokens', 'log_probs', 'med_log_prob']] = df[text_column].progress_apply(\n",
    "        lambda t: pd.Series(compute_log_probs_with_median(t))\n",
    "    )\n",
    "\n",
    "    # Step 2: Compute differences\n",
    "    df['differences'] = df.apply(\n",
    "        lambda row: [lp - mlp for lp, mlp in zip(row['log_probs'], row['med_log_prob'])],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Compute absolute differences\n",
    "    df['abs_differences'] = df.apply(\n",
    "        lambda row: [abs(lp - mlp) for lp, mlp in zip(row['log_probs'], row['med_log_prob'])],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Compute summary stats\n",
    "    df[\"num_tokens\"] = df[\"log_probs\"].apply(len)\n",
    "    df[\"sum_log_prob\"] = df[\"log_probs\"].apply(sum)\n",
    "    df[\"avg_log_prob\"] = df[\"sum_log_prob\"] / df[\"num_tokens\"]\n",
    "\n",
    "    # Compute mean of differences and absolute differences\n",
    "    df[\"mean_diff\"] = df[\"differences\"].apply(np.mean)\n",
    "    df[\"mean_abs_diff\"] = df[\"abs_differences\"].apply(np.mean)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a279b5-9351-4f7a-8048-99c173e1301f",
   "metadata": {},
   "source": [
    "## Score all files in data loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a43c406-6157-476e-bf88-bac20dbd8cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum threads available: 12 - Using 10\n"
     ]
    }
   ],
   "source": [
    "max_threads = os.cpu_count()\n",
    "print(f\"Maximum threads available: {max_threads} - Using {max_threads - 2}\")\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(max_threads - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13abcb9f-3414-47ce-b026-ce0100deffe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1 of 78: alienus_text_10.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring texts:   2%|█▍                                                              | 168/7757 [01:26<59:06,  2.14it/s]"
     ]
    }
   ],
   "source": [
    "# If all your .jsonl files are directly in that folder:\n",
    "jsonl_paths = list(data_loc_path.glob(\"*.jsonl\"))\n",
    "\n",
    "# Extract only the filename for each\n",
    "jsonl_names = [p.name for p in jsonl_paths]\n",
    "\n",
    "total = len(jsonl_names)\n",
    "for idx, j in enumerate(jsonl_names, start=1):\n",
    "    print(f\"Processing file {idx} of {total}: {j}\")\n",
    "    save_path = os.path.join(save_loc, j)\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"  → Skipping {j} (already exists)\")\n",
    "        continue\n",
    "\n",
    "    df = read_jsonl(os.path.join(data_loc, j))\n",
    "    df['impostor_id'] = df.index + 1\n",
    "\n",
    "    df = df[['doc_id', 'corpus', 'impostor_id', 'author', 'texttype', 'rephrased']]\n",
    "    df.rename(columns={'rephrased': 'text'}, inplace=True)\n",
    "\n",
    "    vectorized_df = vectorize_df(df, impostors=True)\n",
    "    vectorized_df.rename(columns={'sentence': 'text'}, inplace=True)\n",
    "\n",
    "    scored_df = score_dataframe(vectorized_df)\n",
    "\n",
    "    write_jsonl(scored_df, save_path)\n",
    "    print(f\"  ✓ Finished {j}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "av_dists",
   "language": "python",
   "name": "av_dists"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
