{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74023ce4-fe16-42ef-be4a-f8fe48e65f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcde231-d80d-4197-80d7-f42296f6a177",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "from read_and_write_docs import read_jsonl, write_jsonl\n",
    "from utils import apply_temp_doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8aaf6f-cf3b-45c1-9443-6e0d9ac9255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_threads = os.cpu_count()\n",
    "print(f\"Maximum threads available: {max_threads} - Using {max_threads - 2}\")\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(max_threads - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1030b2d3-61de-4045-8517-cb000988b3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_subdir_names(dir_path):\n",
    "    p = Path(dir_path)\n",
    "    return [child.name for child in p.iterdir() if child.is_dir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6943a722-f1eb-4bfc-80af-86b0c885e122",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_loc = \"//bc_nas_storage/BCross/datasets/author_verification\"\n",
    "\n",
    "data_type = [\"training\", \"test\"]\n",
    "\n",
    "directories = list_subdir_names(f\"{base_loc}/{data_type[0]}\")\n",
    "directories = ['Enron', 'Wiki']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79d57d5-47eb-4bb4-a464-fe88074e7914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer once\n",
    "model_name = \"Qwen2.5-1.5B-Instruct\"\n",
    "model_loc = f\"C:/Users/benjc/Documents/local models/{model_name}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_loc)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_loc)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44055850-7c0c-42bd-9eb9-b3ad12c25f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_probs(text: str):\n",
    "    \"\"\"\n",
    "    Compute total log-probability of a text under a causal language model.\n",
    "    Returns the sum of log-probs for all tokens except the first.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "    logits = outputs.logits\n",
    "    log_probs = []\n",
    "    for i in range(1, input_ids.size(1)):\n",
    "        token_id = input_ids[0, i]\n",
    "        logits_prev = logits[0, i - 1]\n",
    "        log_prob = F.log_softmax(logits_prev, dim=-1)[token_id].item()\n",
    "        log_probs.append(log_prob)\n",
    "    return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e221bb33-4892-40f1-a860-3005b572b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_probs_with_median(text: str):\n",
    "    \"\"\"\n",
    "    For each token (excluding first), return:\n",
    "    - tokens: list of tokens in the text\n",
    "    - log_probs: list of chosen-token log-probs\n",
    "    - median_logprobs: list of median log-probs for each token\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    tokens = tokenizer.decode(input_ids[0]).split()  # Convert input_ids to tokens\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "    logits = outputs.logits  # [batch_size=1, seq_len, vocab_size]\n",
    "    \n",
    "    log_probs = []\n",
    "    median_logprobs = []\n",
    "    # We start from the second token, as the first one has no previous token to condition on\n",
    "    for i in range(0, input_ids.size(1)):\n",
    "        if i == 0:\n",
    "            logits_prev = logits[0, 0]\n",
    "        else:\n",
    "            logits_prev = logits[0, i - 1]\n",
    "        dist = torch.log_softmax(logits_prev, dim=-1)\n",
    "        \n",
    "        # Extract the log probabilities\n",
    "        log_prob = dist[input_ids[0, i].item()].item()\n",
    "        median_logprob = float(dist.median().item())\n",
    "        \n",
    "        # Append to lists\n",
    "        log_probs.append(log_prob)\n",
    "        median_logprobs.append(median_logprob)\n",
    "    \n",
    "    # The tokens list starts from the first token, but the log_probs and median_logprobs start from the second\n",
    "    # To align them, we need to slice the tokens list to match the lengths\n",
    "    tokens = tokens[0:]  # Match the length of log_probs and median_logprobs\n",
    "    \n",
    "    return tokens, log_probs, median_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3578db4d-8ac4-45ff-b73c-13de97cd1c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataframe(df: pd.DataFrame, text_column: str = \"text\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes a dataframe with a column of texts and computes:\n",
    "    - list of log-probs per token\n",
    "    - median log-probs per token\n",
    "    - number of tokens\n",
    "    - sum of log probs\n",
    "    - average log-prob\n",
    "    - differences between log_probs and median log-probs\n",
    "    - absolute differences between log_probs and median log-probs\n",
    "    - mean of differences\n",
    "    - mean of absolute differences\n",
    "    \"\"\"\n",
    "    tqdm.pandas(desc=\"Scoring texts\")\n",
    "    df = df.copy()\n",
    "\n",
    "    # Step 1: Extract tokens, log_probs, and median log_probs\n",
    "    df[['tokens', 'log_probs', 'med_log_prob']] = df[text_column].progress_apply(\n",
    "        lambda t: pd.Series(compute_log_probs_with_median(t))\n",
    "    )\n",
    "\n",
    "    # Step 2: Compute differences\n",
    "    df['differences'] = df.apply(\n",
    "        lambda row: [lp - mlp for lp, mlp in zip(row['log_probs'], row['med_log_prob'])],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Compute absolute differences\n",
    "    df['abs_differences'] = df.apply(\n",
    "        lambda row: [abs(lp - mlp) for lp, mlp in zip(row['log_probs'], row['med_log_prob'])],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Compute summary stats\n",
    "    df[\"num_tokens\"] = df[\"log_probs\"].apply(len)\n",
    "    df[\"sum_log_prob\"] = df[\"log_probs\"].apply(sum)\n",
    "    df[\"avg_log_prob\"] = df[\"sum_log_prob\"] / df[\"num_tokens\"]\n",
    "\n",
    "    # Compute mean of differences and absolute differences\n",
    "    df[\"mean_diff\"] = df[\"differences\"].apply(np.mean)\n",
    "    df[\"mean_abs_diff\"] = df[\"abs_differences\"].apply(np.mean)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a8616f-4065-48e0-a9ab-860f2953755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = read_jsonl(f\"{base_loc}/training/Enron/known_raw.jsonl\")\n",
    "# test_df = test_df.head(5)\n",
    "# result = score_dataframe(test_df)\n",
    "# result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9b55c9-b3b9-4bc3-be46-b9d41928847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_llr(known_lp: float, unknown_lp: float, impostor_lps: list) -> float:\n",
    "    \"\"\"\n",
    "    Log-likelihood ratio using known vs impostors as background.\n",
    "    \"\"\"\n",
    "    impostor_mean = sum(impostor_lps) / len(impostor_lps)\n",
    "    return known_lp - impostor_mean, unknown_lp - impostor_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d07dad-2376-4b8e-9802-6043e3335597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_verification(df_known, df_unknown, df_impostors, col: str = \"text\") -> dict:\n",
    "    \"\"\"\n",
    "    End-to-end LLR test using three DataFrames.\n",
    "    \"\"\"\n",
    "    scored_known = score_dataframe(df_known, col)\n",
    "    scored_unknown = score_dataframe(df_unknown, col)\n",
    "    scored_impostors = score_dataframe(df_impostors, col)\n",
    "\n",
    "    lp_known = scored_known[\"log_prob\"].mean()\n",
    "    lp_unknown = scored_unknown[\"log_prob\"].mean()\n",
    "    lp_impostors = scored_impostors[\"log_prob\"].tolist()\n",
    "\n",
    "    llr_known, llr_unknown = compute_llr(lp_known, lp_unknown, lp_impostors)\n",
    "\n",
    "    return {\n",
    "        \"known_logprob\": lp_known,\n",
    "        \"unknown_logprob\": lp_unknown,\n",
    "        \"impostor_avg_logprob\": sum(lp_impostors)/len(lp_impostors),\n",
    "        \"LLR_known\": llr_known,\n",
    "        \"LLR_unknown\": llr_unknown,\n",
    "        \"LLR_difference\": llr_known - llr_unknown\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0ee9ed-8385-4f6a-ac4d-31c74b718b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "for corpus in directories:\n",
    "    for dtype in data_type:\n",
    "        for author in ['known', 'unknown']:\n",
    "\n",
    "            data_loc = f\"{base_loc}/{dtype}/{corpus}/{author}_raw.jsonl\"\n",
    "            save_loc = f\"{base_loc}/{dtype}/{corpus}/{author}_logprobs_{model_name.lower().replace(\"-\", \"_\")}.jsonl\"\n",
    "            \n",
    "            if os.path.exists(save_loc):\n",
    "                print(f\"Skipping {dtype} – {corpus} – {author} (already exists)\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing Corpus - {dtype} - {corpus} - {author}\")\n",
    "\n",
    "            t0 = time.time()\n",
    "            \n",
    "            df = read_jsonl(data_loc)\n",
    "            df = apply_temp_doc_id(df)\n",
    "            \n",
    "            num_docs = df.shape[0]\n",
    "            print(f\"    Number of {author} docs - {num_docs}\")\n",
    "    \n",
    "            df_scored = score_dataframe(df)\n",
    "            write_jsonl(df_scored, save_loc)\n",
    "            \n",
    "            elapsed = time.time() - t0\n",
    "            \n",
    "            print(f\"    Completed Corpus - {dtype} - {corpus} - {author} - Time per Doc: {elapsed/num_docs}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
